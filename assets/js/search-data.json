{
  
    
        "post0": {
            "title": "The 80/20 Pandas Tutorial: 5 Key Methods for the Majority of Your Data Transformation Needs",
            "content": ". Ahh, pandas. In addition to being everyone&#39;s favorite vegetarian bear from south central China, it&#39;s also the python library for working with tabular data, a.k.a. dataframes. When you dive into pandas, you&#39;ll quickly find out that there is a lot going on; indeed there are hundreds of methods for operating on dataframes. But luckily for us, as with many areas of life, there is a Pareto Principle, or 80/20 rule, that will help us focus on the small set of methods that collectively solve the majority of our data transformation needs. . If you&#39;re like me, then pandas is not your first data-handling tool; maybe you&#39;ve been using SQL or R with data.table or dplyr. If so, that&#39;s great because you already have a sense for the key operations we need when working with tabular data. In their book, R for Data Science, Garrett Grolemund and Hadley Wickham describe five essential operations for manipulating dataframes. I&#39;ve found that these cover the majority of my data transformation tasks to prepare data for analysis, visualization, and modeling. . filtering rows based on data values | sorting rows based on data values | selecting columns by name | adding new columns based on the existing columns | creating grouped summaries of the dataset | I would add that we also need a way to build up more complex transformations by chaining these fundamental operations together sequentially. . Before we dive in, here&#39;s the TLDR on the pandas methods that I prefer for accomplishing these tasks, along with their equivalents from SQL and dplyr in R. . description pandas SQL dplyr . filter rows based on data values | query() | WHERE | filter() | . sort rows based on data values | sort_values() | ORDER BY | arrange() | . select columns by name | filter() | SELECT | select() | . add new columns based on the existing columns | assign() | AS | mutate() | . create grouped summaries of the dataset | groupby() apply() | GROUP BY | group_by() summarise() | . chain operations together | . | | %&gt;% | . Imports and Data . import pandas as pd import numpy as np . We&#39;ll use the nycflights13 dataset which contains data on the 336,776 flights that departed from New York City in 2013. . flights = pd.read_csv(&#39;https://www.openintro.org/book/statdata/nycflights.csv&#39;) flights.head() . year month day dep_time dep_delay arr_time arr_delay carrier tailnum flight origin dest air_time distance hour minute . 0 2013 | 6 | 30 | 940 | 15 | 1216 | -4 | VX | N626VA | 407 | JFK | LAX | 313 | 2475 | 9 | 40 | . 1 2013 | 5 | 7 | 1657 | -3 | 2104 | 10 | DL | N3760C | 329 | JFK | SJU | 216 | 1598 | 16 | 57 | . 2 2013 | 12 | 8 | 859 | -1 | 1238 | 11 | DL | N712TW | 422 | JFK | LAX | 376 | 2475 | 8 | 59 | . 3 2013 | 5 | 14 | 1841 | -4 | 2122 | -34 | DL | N914DL | 2391 | JFK | TPA | 135 | 1005 | 18 | 41 | . 4 2013 | 7 | 21 | 1102 | -3 | 1230 | -8 | 9E | N823AY | 3652 | LGA | ORF | 50 | 296 | 11 | 2 | . Select rows based on their values with query() . query() lets you retain a subset of rows based on the values of the data; it&#39;s like dplyr::filter() in R or WHERE in SQL. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using and and or, and you can even check if the column value matches any items in a list. . # compare one column to a value flights.query(&#39;month == 6&#39;) # compare two column values flights.query(&#39;arr_delay &gt; dep_delay&#39;) # using arithmetic flights.query(&#39;arr_delay &gt; 0.5 * air_time&#39;) # using &quot;and&quot; flights.query(&#39;month == 6 and day == 1&#39;) # using &quot;or&quot; flights.query(&#39;origin == &quot;JFK&quot; or dest == &quot;JFK&quot;&#39;) # column value matching any item in a list flights.query(&#39;carrier in [&quot;AA&quot;, &quot;UA&quot;]&#39;) . You may have noticed that it seems to be much more popular to filter pandas data frames using boolean indexing. Indeed when I ask my favorite search engine how to filter a pandas dataframe on its values, I find this tutorial, this blog post, various questions on Stack Overflow, and even the pandas documentation, all espousing boolean indexing. Here&#39;s what it looks like. . # canonical boolean indexing flights[(flights[&#39;carrier&#39;] == &quot;AA&quot;) &amp; (flights[&#39;origin&#39;] == &quot;JFK&quot;)] # the equivalent use of query() flights.query(&#39;carrier == &quot;AA&quot; and origin == &quot;JFK&quot;&#39;) . There are a few reasons I prefer query() over boolean indexing. . query() does not require me to type the dataframe name again, whereas boolean indexing requires me to type it every time I wish to refer to a column. | query() makes the code easier to read and understand, especially when expressions get complex. | query() is more computationally efficient than boolean indexing. | query() can safely be used in dot chains, which we&#39;ll see very soon. | Select columns by name with filter() . filter() lets you pick out a specific set of columns by name; it&#39;s analogous to dplyr::select() in R or SELECT in SQL. You can either provide exactly the column names you want, or you can grab all columns whose names contain a given substring or which match a given regular expression. This isn&#39;t a big deal when your dataframe has only a few columns, but is particularly useful when you have a dataframe with tens or hundreds of columns. . # select a list of columns flights.filter([&#39;origin&#39;, &#39;dest&#39;]) # select columns containing a particular substring flights.filter(like=&#39;time&#39;) # select columns matching a regular expression flights.filter(regex=&#39;e$&#39;) . Sort rows with sort_values() . sort_values() changes the order of the rows based on the data values; it&#39;s likedplyr::arrange() in R or ORDER BY in SQL. You can specify one or more columns on which to sort, where their order denotes the sorting priority. You can also specify whether to sort in ascending or descending order. . # sort by a single column flights.sort_values(&#39;air_time&#39;) # sort by a single column in descending order flights.sort_values(&#39;air_time&#39;, ascending=False) # sort by carrier, then within carrier, sort by descending distance flights.sort_values([&#39;carrier&#39;, &#39;distance&#39;], ascending=[True, False]) . Add new columns with assign() . assign() adds new columns which can be functions of the existing columns; it&#39;s like dplyr::mutate() from R. . # add a new column based on other columns flights.assign(speed = lambda x: x.distance / x.air_time) # another new column based on existing columns flights.assign(gain = lambda x: x.dep_delay - x.arr_delay) . If you&#39;re like me, this way of using assign() might seem a little strange at first. Let&#39;s break it down. In the call to assign() the keyword argument speed tells pandas the name of our new column. The business to the right of the = is a inline lambda function that takes the dataframe we passed to assign() and returns the column we want to add. . I like using x as the lambda argument because its easy to type and it evokes tabular data (think design matrix), which reminds me that it refers to the entire dataframe. We can then access the other columns in our dataframe using the dot like x.other_column. . It&#39;s true that you can skip the whole lambda business and refer to the dataframe to which you are assigning directly inside the assign. That might look like this. . flights.assign(speed = flights.distance / flights.air_time) . I prefer using a lambda for the following reasons. . If you gave your dataframe a good name, using the lambda will save you from typing the name every time you want to refer to a column. | The lambda makes your code more portable. Since you refer to the dataframe as a generic x, you can reuse this same assignment code on a dataframe with a different name. | Most importantly, the lambda will allow you to harness the power of dot chaining. | Chain transformations together with the dot chain . One of the awesome things about pandas is that the object.method() paradigm lets us easily build up complex dataframe transformations from a sequence of method calls. In R, this is effectively accomplished by the pipe %&gt;% operator. For example, suppose we want to look at high-speed flights from JFK to Honolulu, which would require us to query for JFK to Honolulu flights, assign a speed column, and maybe sort on that new speed column. . We can say: . # neatly chain method calls together ( flights .query(&#39;origin == &quot;JFK&quot;&#39;) .query(&#39;dest == &quot;HNL&quot;&#39;) .assign(speed = lambda x: x.distance / x.air_time) .sort_values(by=&#39;speed&#39;, ascending=False) .query(&#39;speed &gt; 8.0&#39;) ) . We compose the dot chain by wrapping the entire expression in parentheses and indenting each line within. The first line is the name of the dataframe on which we are operating. Each subsequent line has a single method call. . There are a few great things about writing the code this way: . Readability. It&#39;s easy to scan down the left margin of the code to see what&#39;s happening. The first line gives us our noun (the dataframe) and each subsequent line starts with a verb. You could read this as &quot;take flights then query the rows where origin is JFK, then query for rows where destination is HNL, then assign a new column called speed, then sort the dataframe by speed, then query only for the rows where speed is greater than 8.0. | Flexibility - It&#39;s easy to comment out individual lines and re-run the cell. It&#39;s also easy to reorder operations, since only one thing happens on each line. | Neatness - We have not polluted our workspace with any intermediate variables, nor have we wasted any mental energy thinking of names for any temporary variables. | By default, dot chains do not modify the original dataframe; they just output a temporary result that we can inspect directly in the output. If you want to store the result, or pass it along to another function (e.g. for plotting), you can simply assign the entire dot chain to a variable. . # sotre the output of the dot chain in a new dataframe flights_high_speed = ( flights .assign(speed = lambda x: x.distance / x.air_time) .query(&#39;speed &gt; 8.0&#39;) ) . Collapsing rows into grouped summaries with groupby() . groupby() combined with apply() gives us flexibility and control over our grouped summaries; it&#39;s like dplyr::group_by() and dplyr::summarise() in R. This is the primary pattern I use for SQL-style groupby operations in pandas. Specifically it unlocks the following essential functionality you&#39;re used to having in SQL. . specify the names of the aggregation columns we create | specify which aggregation function to use on which columns | compose more complex aggregations such as the proportion of rows meeting some condition | aggregate over arbitrary functions of multiple columns | Let&#39;s check out the departure delay stats for each carrier. . ( flights .groupby([&#39;carrier&#39;]) .apply(lambda d: pd.Series({ &#39;n_flights&#39;: len(d), &#39;med_delay&#39;: d.dep_delay.median(), &#39;avg_delay&#39;: d.dep_delay.mean(), })) .head() ) . n_flights med_delay avg_delay . carrier . 9E 1696.0 | -1.0 | 17.285967 | . AA 3188.0 | -2.0 | 9.142409 | . AS 66.0 | -4.5 | 5.181818 | . B6 5376.0 | -1.0 | 13.137091 | . DL 4751.0 | -2.0 | 8.529573 | . While you might be used to apply() acting over the rows or columns of a dataframe, here we&#39;re calling apply on a grouped dataframe object, so it&#39;s acting over the groups. According to the pandas documentation: . The function passed to apply must take a dataframe as its first argument and return a dataframe, a series or a scalar. apply will then take care of combining the results back together into a single dataframe or series. apply is therefore a highly flexible grouping method. . We need to supply apply() with a function that takes each chunk of the grouped dataframe and returns (in our case) a series object with one element for each new aggregation column. Notice that I use a lambda to specify the function we pass to apply()), and that I name its argument d, which reminds me that it&#39;s a dataframe. My lambda returns a pandas series whose index entries specify the new aggregation column names, and whose values constitute the results of the aggregations for each group. Pandas will then stitch everything back together into a lovely dataframe. . Notice how nice the code looks when we use this pattern. Each aggregation is specified on its own line, which makes it easy to see what aggregation columns we&#39;re creating and allows us to comment, uncomment, and reorder the aggregations without breaking anything. . Here are some more complex aggregations to illustrate some useful patterns. . ( flights .groupby([&#39;carrier&#39;]) .apply(lambda d: pd.Series({ &#39;avg_gain&#39;: np.mean(d.dep_delay - d.arr_delay), &#39;pct_delay_gt_30&#39;: np.mean(d.dep_delay &gt; 30), &#39;pct_late_dep_early_arr&#39;: np.mean((d.dep_delay &gt; 0) &amp; (d.arr_delay &lt; 0)), &#39;avg_arr_given_dep_delay_gt_0&#39;: d.query(&#39;dep_delay &gt; 0&#39;).arr_delay.mean(), &#39;cor_arr_delay_dep_delay&#39;: np.corrcoef(d.dep_delay, d.arr_delay)[0,1], })) .head() ) . avg_gain pct_delay_gt_30 pct_late_dep_early_arr avg_arr_given_dep_delay_gt_0 cor_arr_delay_dep_delay . carrier . 9E 9.247642 | 0.196934 | 0.110259 | 39.086111 | 0.932485 | . AA 7.743726 | 0.113237 | 0.105395 | 30.087165 | 0.891013 | . AS 16.515152 | 0.106061 | 0.121212 | 28.058824 | 0.864565 | . B6 3.411458 | 0.160528 | 0.084449 | 37.306866 | 0.914180 | . DL 7.622816 | 0.097874 | 0.100821 | 30.078029 | 0.899327 | . Here&#39;s what&#39;s happening. . np.mean(d.dep_delay - d.arr_delay) aggregates over the difference of two columns. | np.mean(d.dep_delay &gt; 30) computes the proportion of rows where the delay is greater than 30 minutes. Generating a boolean series based on some condition and then using mean() to find the proportion comes up all the time. | np.mean((d.dep_delay &gt; 0) &amp; (d.arr_delay &lt; 0)) shows that we can compute proportions where conditions on multiple columns are met. | d.query(&#39;dep_delay &gt; 0&#39;).arr_delay.mean() computes the average arrival delay on flights where the departure was delayed. Here we first filter each grouped dataframe down to the subset of rows where departure delay is greater than zero using query(), and then we take the mean of the remaining arrival delays. | np.corrcoef(d.dep_delay, d.arr_delay)[0,1] computes the correlation coefficient between departure and arrival delays. Remember we can use pretty much any reduction operation to collapse values down to a scalar. | . You might have noticed that the canonical pandas approach for grouped summaries is to use agg(). That works well if you need to apply the same aggregation function on each column in the dataframe, e.g. taking the mean of every column. But because of the kind of data I work with these days, it&#39;s much more common for me to use customized aggregations like those above, so the groupby() apply() idiom works best for me. . Wrapping Up . There you have it, how to pull off the five most essential data transformation tasks using pandas in a style reminiscent of my beloved dplyr. Remember that part of the beauty of pandas is that since there are so many ways to do most tasks, you can develop your own style based on the kind of data you work with, what you like about other tools, how you see others using the tools, and of course your own taste and preferences. . If you found this post helpful or if you have your own preferred style for accomplishing any of these key transformations with pandas, do let me know about it in the comments. .",
            "url": "https://blog.mattbowers.dev/8020-pandas-tutorial",
            "relUrl": "/8020-pandas-tutorial",
            "date": " • Nov 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Hello World! And Why I'm Inspired to Start a Blog",
            "content": ". Well, I’ve been thinking about getting this blog started for months now. I guess a combination of inertia, up-front investment in blogging platform selection/setup, and spending a little too much time writing and rewriting the first content post has drawn out the period from initial inspiration to making the blog a reality. Needless to say, I’m pretty excited to finally get things going. . Before we dive headlong into the weeds of ML algorithms, statistical methods, and whatever I happen to be learning and teaching at the moment, I figured it would be good to articulate why I’ve felt inspired to get started blogging in the first place. Hopefully this will serve the dual purpose of clarifying my intentions and introducing a vastly underappreciated concept in data science that I hope to weave through the posts to come. . Learning . The initial inception about blogging probably originated from some comments about learning that Jeremy Howard makes in the Practical Deep Learning course from fastai. During one of the lectures, he mentions that it’s a great idea to start blogging. To paraphrase Jeremy: . The thing I really love about blogging is that it helps you learn; by writing things down, you synthesize your ideas. . Beautiful. That definitely rings true for me. I tend to take notes and play around with code when learning new concepts anyway. One of my key hypotheses about this blogging experiment is that making the effort to transform those notes into blog posts will help me learn more effectively. . Teaching . Ah, teaching. Yes, sometimes it’s that thing that takes time away from your research, forcing you to sit alone in a windowless room squinting at hand-written math on a fat stack of homework assignments. But sometimes it actually involves interacting with students, endeavoring to explain a concept, and watching them light up when they get it. The latter manifestation of teaching was one of my favorite things about grad school and academia in general. While I certainly still get to do some teaching as an industry data scientist, I could see myself returning to a more teaching-centric gig somewhere off in the future. Thus we have our second key hypothesis about the blogging experiment, that the writing will entertain my inclination to teach. . Contributing . Working in the field of data science today is a bit like standing in front of a massive complementary all-you-can-learn buffet. There is an abundance of free material out on the interwebs for learning pretty much anything in data science from hello world python tutorials to research papers on cutting-edge deep learning techniques. I’ve personally benefited from many a blog post that helped me unpack a new concept or get started using a new tool. And let’s not forget the gigantic cyber warehouse full of freely available open source software tools that volunteer developers have straight-up donated to humanity. . I realize that up to now, I’ve simply been consuming all of this free goodness without giving anything substantive back in return. Well then, it’s time to start evening the score. Which brings us to key hypothesis number three, that through these blog posts, I might be able to create something helpful, thereby being of service to a community that has freely given so much to me. . Live Long and Prosper, Blog . Phew, there it is, the original source of inspiration for this blogging experiment, and three reasons I think it might be a good idea. The astute reader will have noticed that these three assertions have been formulated as hypotheses which are to be tested in the laboratory of experience. And thus, we also have our first glimpse of the scientific method, an underrated concept that is going to help us put the science back in data science. . With that, blog, I christen thee, Random Realizations. .",
            "url": "https://blog.mattbowers.dev/hello-world",
            "relUrl": "/hello-world",
            "date": " • Nov 22, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Well howdy! I’m Matt Bowers. I’m currently a data scientist at Uber where I specialize in using statistical modeling and machine learning to make large-scale telematics data useful. Before Uber I earned an MS in Applied Statistics and a PhD in Atmospheric Sciences at Purdue University where I studied the spatial-temporal correlation structure of tropical convection systems and developed statistical methods for climate research. I was also an Insight Data Science fellow at Silicon Valley in the summer of 2017. .",
          "url": "https://blog.mattbowers.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.mattbowers.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}