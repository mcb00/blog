{
  
    
        "post0": {
            "title": "Decision Tree From Scratch",
            "content": ". Yesterday we had a lovely discussion about the key strengths and weaknesses of decision trees and why tree ensembles are so great. But today, gentle reader, we abandon our philosophizing and get down to the business of implementing one of these decision trees from scratch. . A note before we get started. This is going to be the most involved scratch-build that we&#39;ve done at Random Realizations so far. It is not the kind of algorithm that I could just sit down and write all at once. We need to start with a basic frame and then add functionality step by step, testing all along the way to make sure things are working properly. Since I&#39;m writing this in a jupyter notebook, I&#39;ll try to give you a sense for how I actually put the algorithm together interactively in pieces, eventually landing on a fully-functional final product. . Shall we? . Binary Tree Data Structure . A decision tree takes a dataset with features and a target, partitions the feature space into chunks, and assigns a prediction value to each chunk. Since each partitioning step divides one chunk in two, and since the partitioning is done recursively, it&#39;s natural to use a binary tree data structure to represent a decision tree. . The basic idea of the binary tree is that we define a class to represent nodes in the tree. If we want to add children to a given node, we simply assign them as attributes of the parent node. The child nodes we add are themselves instances of the same class, so we can add children to them in the same way. . Let&#39;s start out with a simple class for our decision tree. It takes a single value called max_depth as input, which will dictate how many layers of child nodes should be inserted below the root. This controls the depth of the tree. As long as max_depth is positive, the parent will instantiate two new instances of the binary tree node class, passing along max_depth decremented by one and attaching the two children to itself as attributes called left and right. . import math import numpy as np import pandas as pd import matplotlib.pyplot as plt . class DecisionTree(): def __init__(self, max_depth): assert max_depth &gt;= 0, &#39;max_depth must be nonnegative&#39; self.max_depth = max_depth if max_depth &gt; 0: self.left = DecisionTree(max_depth=max_depth-1) self.right = DecisionTree(max_depth=max_depth-1) . Let&#39;s make a new instance of our decision tree class, a tree with depth 2. . t = DecisionTree(max_depth=2) . . We can access individual nodes and check their value of max_depth. . t.max_depth, t.left.max_depth, t.left.right.max_depth . (2, 1, 0) . Our full decision tree can expand on this idea where each node receives some input, modifies it, creates two child nodes, and passes the modified input along to them. Specifically, each node in our decision tree will receive a dataset, determine how best to split the dataset into two parts, create two child nodes, and pass one part of the data to the left child and the other part to the right child. . All we have to do now is add some additional functionality to our decision tree. First we&#39;ll start by capturing all the inputs we need to grow a tree, which include the feature dataframe X, the target array y, max_depth to explicitly limit tree depth, min_samples_leaf to specify the minimum number of observations that are allowed in a leaf node, and an optional idxs which specifies the indices of data that the node should use. The indices argument is useful for users of our decision tree because it will allow them to implement row subsampling in ensemble methods like random forest. It will also be handy for internal use inside the decision tree when passing data along to child nodes; instead of passing copies of the two data subsets, we&#39;ll just pass a reference to the full dataset and pass along a set of indices to identify that node&#39;s instance subset. . Once we get our input, we&#39;ll do a little bit of input validation and store things that we want to keep as object attributes. In case this is a leaf node, we&#39;ll go ahead and compute its predicted value; since this is a regression tree, the prediction is just the mean of the target y. We&#39;ll also go ahead and initialize a score metric which we&#39;ll use to help us find the best split later; since lower scores are going to be better, we&#39;ll initialize it to positive infinity. Finally, we&#39;ll push the logic to add child nodes into a method called _maybe_insert_child_nodes that we&#39;ll define next. . . Note: a leading underscore in a method name indicates the method is for internal use and not part of the user-facing API of the class. . class DecisionTree(): def __init__(self, X, y, min_samples_leaf=5, max_depth=6, idxs=None): assert max_depth &gt;= 0, &#39;max_depth must be nonnegative&#39; assert min_samples_leaf &gt; 0, &#39;min_samples_leaf must be positive&#39; self.min_samples_leaf, self.max_depth = min_samples_leaf, max_depth if isinstance(y, pd.Series): y = y.values if idxs is None: idxs = np.arange(len(y)) self.X, self.y, self.idxs = X, y, idxs self.n, self.c = len(idxs), X.shape[1] self.value = np.mean(y[idxs]) # node&#39;s prediction value self.best_score_so_far = float(&#39;inf&#39;) # initial loss before split finding if self.max_depth &gt; 0: self._maybe_insert_child_nodes() def _maybe_insert_child_nodes(self): pass . Now in order to test our class, we&#39;ll need some actual data. We can use the same scikit-learn diabetes data from the last post. . from sklearn.datasets import load_diabetes X, y = load_diabetes(as_frame=True, return_X_y=True) . t = DecisionTree(X, y, min_samples_leaf=5, max_depth=5) . So far, so good. . Inserting Child Nodes . Our node inserting function _maybe_insert_child_nodes needs to first find the best split; then if a valid split exists, it needs to insert the child nodes. To find the best valid split, we need to loop through the columns and search each one for the best valid split. Again we&#39;ll push the logic of finding the best split into a function that we&#39;ll define later. Next if no split was found, we need to bail by returning before trying to insert the child nodes. To check if this node is a leaf (i.e. it shouldn&#39;t have child nodes), we define a property called is_leaf which will just check if the best score so far is still infinity, in which case no split was found and the node is a leaf. . If a valid split was found, then we need to insert the child nodes. We&#39;ll assume that our split finding function assigned attributes called split_feature_idx and threshold to tell us the split feature&#39;s index and the split threshold value. We then use these to compute the indices of the data to be passed to the child nodes; the left child gets instances where the split feature value is less than or equal to the threshold, and the right child node gets instances where the split feature value is greater than the threshold. Then we create two new decision trees, passing the corresponding data indices to each and assigning them to the left and right attributes of the current node. . def _maybe_insert_child_nodes(self): for j in range(self.c): self._find_better_split(j) if self.is_leaf: # do not insert children return x = self.X.values[self.idxs,self.split_feature_idx] left_idx = np.nonzero(x &lt;= self.threshold)[0] right_idx = np.nonzero(x &gt; self.threshold)[0] self.left = DecisionTree(self.X, self.y, self.min_samples_leaf, self.max_depth - 1, self.idxs[left_idx]) self.right = DecisionTree(self.X, self.y, self.min_samples_leaf, self.max_depth - 1, self.idxs[right_idx]) def _find_better_split(self, feature_idx): pass @property def is_leaf(self): return self.best_score_so_far == float(&#39;inf&#39;) . To test these new methods , we can assign them to our DecisionTree class and create a new class instance to make sure things are still working. . DecisionTree._maybe_insert_child_nodes = _maybe_insert_child_nodes DecisionTree._find_better_split = _find_better_split DecisionTree.is_leaf = is_leaf t = DecisionTree(X, y, min_samples_leaf=5, max_depth=6) . Yep, we&#39;re still looking good. . Split Finding . Now we need to fill in the functionality of the split finding method. The overall strategy is to consider every possible way to split on the current feature, measuring the quality of each potential split with some scoring mechanism, and keeping track of the best split we&#39;ve seen so far. We&#39;ll come back to the issue of how to try all the possible splits in a moment, but let&#39;s start by figuring out how to score a particular potential split. . Like other machine learning models, trees are trained by attempting to minimize some loss function that measures how well the model predicts the target data. We&#39;ll be training our regression tree to minimize squared error. . $$ L = sum_{i=1}^n (y_i- hat{y}_i)^2$$ . For a given node, we can replace $ hat{y}$ with $ bar{y}$ because each node uses the sample mean of its target instances as its prediction. We can then rewrite the loss for a given node as . $$ L = sum_{i=1}^n(y_i - bar{y})^2 $$ $$ = sum_{i=1}^n(y_i^2 -2y_i bar{y} + bar{y}^2) $$ $$ = sum_{i=1}^ny_i^2 -2 bar{y} sum_{i=1}^ny_i + n bar{y}^2 $$ $$ = sum_{i=1}^ny_i^2 - frac{1}{n} left ( sum_{i=1}^ny_i right )^2 $$ . We can then evaluate potential splits by comparing the loss after splitting to the loss before splitting, where the split with the greatest loss reduction is best. Let&#39;s work out a simple expression for the loss reduction from a given split. . Let $I$ be the set of $n$ data instances in the current node, and let $I_L$ and $I_R$ be the instances that fall into the left and right child nodes of a proposed split. Let $L$ be the total loss for all instances in the node, while $L_L$ and $L_R$ are the losses for the left and right child nodes. The total loss contributed by instances in $I$ prior to any split is . $$L_{ text{before split}} = L = sum_{i in I} y_i^2 - frac{1}{n} left ( sum_{i in I} y_i right )^2 $$ . And the loss after splitting $I$ into $I_L$ and $I_R$ is . $$L_{ text{after split}} = L_L + L_R = sum_{i in I_L} y_i^2 - frac{1}{n_L} left ( sum_{i in I_L} y_i right )^2 + sum_{i in I_R} y_i^2 - frac{1}{n_R} left ( sum_{i in I_R} y_i right )^2 $$ . The reduction in loss from this split is . $$ Delta L = L_{ text{after split}} - L_{ text{before split}} = (L_L + L_R) - L $$ $$ = sum_{i in I_L} y_i^2 - frac{1}{n_L} left ( sum_{i in I_L} y_i right )^2 + sum_{i in I_R} y_i^2 - frac{1}{n_R} left ( sum_{i in I_R} y_i right )^2 - left ( sum_{i in I} y_i^2 - frac{1}{n} left ( sum_{i in I} y_i right )^2 right ) $$ . Since $I = I_L cup I_R$ the $ sum y^2$ terms cancel and we can simplify. . $$ Delta L = - frac{1}{n_L} left ( sum_{i in I_L} y_i right )^2 - frac{1}{n_R} left ( sum_{i in I_R} y_i right )^2 + frac{1}{n} left ( sum_{i in I} y_i right )^2 $$This is a really nice formulation of the split scoring metric from a computational complexity perspective. We can sort the data by the feature values then, starting with the smallest min_samples_leaf instances in the left node and the rest in the right node, we check the score. Then to check the next split, we simply move a single target value from the right node into the left node, updating the score by subtracting it from the right node&#39;s partial sum and adding it to the left node&#39;s partial sum. The third term is constant for all splits, so we only need to compute it once. If any split&#39;s score is lower than the best score so far, then we update the best score so far, the split feature, and the threshold value. When we&#39;re done we can be sure we found the best possible split. The time bottleneck is the sort, which puts us at an average time complexity of $O(n log n)$. . def _find_better_split(self, feature_idx): x = self.X.values[self.idxs,feature_idx] y = self.y[self.idxs] sort_idx = np.argsort(x) sort_y, sort_x = y[sort_idx], x[sort_idx] sum_y, n = y.sum(), len(y) sum_y_right, n_right = sum_y, n sum_y_left, n_left = 0., 0 for i in range(0, self.n - self.min_samples_leaf): y_i, x_i, x_i_next = sort_y[i], sort_x[i], sort_x[i + 1] sum_y_left += y_i; sum_y_right -= y_i n_left += 1; n_right -= 1 if n_left &lt; self.min_samples_leaf or x_i == x_i_next: continue score = - sum_y_left**2 / n_left - sum_y_right**2 / n_right + sum_y**2 / n if score &lt; self.best_score_so_far: self.best_score_so_far = score self.split_feature_idx = feature_idx self.threshold = (x_i + x_i_next) / 2 . Again, we assign the split finding method to our class and instantiate a new tree to make sure things are still working. . DecisionTree._find_better_split = _find_better_split t = DecisionTree(X, y, min_samples_leaf=5, max_depth=6) X.columns[t.split_feature_idx], t.threshold . (&#39;s5&#39;, -0.003761786142811515) . Nice! Looks like the tree started with a split on the s5 feature. . Inspecting the Tree . While we&#39;re developing something complex like a decision tree class, we need a good way to inspect the object to help with testing and debugging. Let&#39;s write a quick string representation method to make it easier to check what&#39;s going on with a particular node. . def __repr__(self): s = f&#39;n: {self.n}&#39; s += f&#39;; value:{self.value:0.2f}&#39; if not self.is_leaf: split_feature_name = self.X.columns[self.split_feature_idx] s += f&#39;; split: {split_feature_name} &lt;= {self.threshold:0.3f}&#39; return s . We can assign the string representation method to the class and print a few nodes. . DecisionTree.__repr__ = __repr__ t = DecisionTree(X, y, min_samples_leaf=5, max_depth=2) print(t) print(t.left) print(t.left.left) . n: 442; value:152.13; split: s5 &lt;= -0.004 n: 218; value:109.99; split: bmi &lt;= 0.006 n: 171; value:96.31 . Prediction . We need a public predict method that takes a feature dataframe and returns an array of predictions. We&#39;ll need to look up the predicted value for one instance at a time and stitch them together in an array. We can do that by iterating over the feature dataframe rows with a list comprehension that calls a _predict_row method to grab the prediction for each row. The row predict method needs to return the current node&#39;s predicted value if it&#39;s a leaf, or if not, it needs to identify the appropriate child node based on its split and ask it for a prediction. . def predict(self, X): return np.array([self._predict_row(row) for i, row in X.iterrows()]) def _predict_row(self, row): if self.is_leaf: return self.value child = self.left if row[self.split_feature_idx] &lt;= self.threshold else self.right return child._predict_row(row) . Let&#39;s assign the predict methods and make predictions on a few rows. . DecisionTree.predict = predict DecisionTree._predict_row = _predict_row t.predict(X.iloc[:3, :]) . array([225.87962963, 96.30994152, 225.87962963]) . The Complete Decision Tree Implementation . Here&#39;s the implementation, all in one place. . class DecisionTree(): def __init__(self, X, y, min_samples_leaf=5, max_depth=6, idxs=None): assert max_depth &gt;= 0, &#39;max_depth must be nonnegative&#39; assert min_samples_leaf &gt; 0, &#39;min_samples_leaf must be positive&#39; self.min_samples_leaf, self.max_depth = min_samples_leaf, max_depth if isinstance(y, pd.Series): y = y.values if idxs is None: idxs = np.arange(len(y)) self.X, self.y, self.idxs = X, y, idxs self.n, self.c = len(idxs), X.shape[1] self.value = np.mean(y[idxs]) # node&#39;s prediction value self.best_score_so_far = float(&#39;inf&#39;) # initial loss before split finding if self.max_depth &gt; 0: self._maybe_insert_child_nodes() def _maybe_insert_child_nodes(self): for j in range(self.c): self._find_better_split(j) if self.is_leaf: # do not insert children return x = self.X.values[self.idxs,self.split_feature_idx] left_idx = np.nonzero(x &lt;= self.threshold)[0] right_idx = np.nonzero(x &gt; self.threshold)[0] self.left = DecisionTree(self.X, self.y, self.min_samples_leaf, self.max_depth - 1, self.idxs[left_idx]) self.right = DecisionTree(self.X, self.y, self.min_samples_leaf, self.max_depth - 1, self.idxs[right_idx]) @property def is_leaf(self): return self.best_score_so_far == float(&#39;inf&#39;) def _find_better_split(self, feature_idx): x = self.X.values[self.idxs,feature_idx] y = self.y[self.idxs] sort_idx = np.argsort(x) sort_y, sort_x = y[sort_idx], x[sort_idx] sum_y, n = y.sum(), len(y) sum_y_right, n_right = sum_y, n sum_y_left, n_left = 0., 0 for i in range(0, self.n - self.min_samples_leaf): y_i, x_i, x_i_next = sort_y[i], sort_x[i], sort_x[i + 1] sum_y_left += y_i; sum_y_right -= y_i n_left += 1; n_right -= 1 if n_left &lt; self.min_samples_leaf or x_i == x_i_next: continue score = - sum_y_left**2 / n_left - sum_y_right**2 / n_right + sum_y**2 / n if score &lt; self.best_score_so_far: self.best_score_so_far = score self.split_feature_idx = feature_idx self.threshold = (x_i + x_i_next) / 2 def __repr__(self): s = f&#39;n: {self.n}&#39; s += f&#39;; value:{self.value:0.2f}&#39; if not self.is_leaf: split_feature_name = self.X.columns[self.split_feature_idx] s += f&#39;; split: {split_feature_name} &lt;= {self.threshold:0.3f}&#39; return s def predict(self, X): return np.array([self._predict_row(row) for i, row in X.iterrows()]) def _predict_row(self, row): if self.is_leaf: return self.value child = self.left if row[self.split_feature_idx] &lt;= self.threshold else self.right return child._predict_row(row) . From Scratch versus Scikit-Learn . As usual, we&#39;ll test our homegrown handiwork by comparing it to the existing implementation in scikit-learn. First let&#39;s train both models on the California Housing dataset which gives us 20k instances and 8 features to predict median house price by district. . from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split X, y = fetch_california_housing(as_frame=True, return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43) . from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error max_depth = 8 min_samples_leaf = 16 tree = DecisionTree(X_train, y_train, max_depth=max_depth, min_samples_leaf=min_samples_leaf) pred = tree.predict(X_test) sk_tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf) sk_tree.fit(X_train, y_train) sk_pred = sk_tree.predict(X_test) print(f&#39;from scratch MSE: {mean_squared_error(y_test, pred):0.4f}&#39;) print(f&#39;scikit-learn MSE: {mean_squared_error(y_test, sk_pred):0.4f}&#39;) . from scratch MSE: 0.3988 scikit-learn MSE: 0.3988 . We get similar accuracy on a held-out test dataset. . Let&#39;s benchmark the two implementations on training time. . %%time sk_tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf) sk_tree.fit(X_train, y_train); . CPU times: user 69.9 ms, sys: 3.05 ms, total: 73 ms Wall time: 73.3 ms . DecisionTreeRegressor(max_depth=8, min_samples_leaf=16) . %%time tree = DecisionTree(X_train, y_train, max_depth=max_depth, min_samples_leaf=min_samples_leaf) . CPU times: user 1.81 s, sys: 10.8 ms, total: 1.82 s Wall time: 1.84 s . Wow, the scikit-learn implementation absolutely smoked us, training an order of magnitude faster. This is to be expected, since they implement split finding in cython, which generates compiled C code that can run much faster than our native python code. Maybe we can take a look at how to optimize python code with cython here on the blog one of these days. . Wrapping Up . Holy cow, we just implemented a decision tree using nothing but numpy. I hope you enjoyed the scratch build as much as I did, and I hope you got a little bit better at coding (I certainly did). That was actually way harder than I expected, but looking back at the finished product, it doesn&#39;t seem so bad right? I almost thought we were going to get away with not implementing our own decision tree, but it turns out that this will be super helpful for us when it comes time to implement XGBoost from scratch. . References . This implementation is inspired and partially adapted from Jeremy Howard&#39;s live coding of a Random Forest as part of the fastai ML course. .",
            "url": "https://blog.mattbowers.dev/decision-tree-from-scratch",
            "relUrl": "/decision-tree-from-scratch",
            "date": " • Dec 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Consider the Decision Tree",
            "content": ". Ah, the decision tree. It&#39;s an underrated and often overlooked hero of modern statistical learning. Trees aren&#39;t particularly powerful learning algorithms on their own, but when utilized as building blocks in larger ensemble models like random forest and gradient boosted trees, they can achieve state of the art performance in many practical applications. Since we&#39;ve been focusing on gradient boosting ensembles lately, let&#39;s take a moment to consider the humble decision tree itself. This post gives a high-level intuition for how trees work, an opinionated list of their key strengths and weaknesses, and some perspective on why ensembling makes them truly shine. . Onward! . Classification and Regression Trees . A Decision tree is a type of statistical model that takes features or covariates as input and yields a prediction as output. The idea of the decision tree as a statistical learning tool traces back to a monograph published in 1984 by Breiman, Freidman, Olshen, and Stone called &quot;Classification and Regression Trees&quot; (a.k.a. CART). As the name suggests, trees come in two main varieties: classification trees which predict discrete class labels (e.g. DecisionTreeClassifier) and regression trees which predict numeric values (e.g. DecisionTreeRegressor). . As I mentioned earlier, tree models are not very powerful learners on their own. You might find that an individual tree model is useful for creating a simple and highly interpretable model in specific situations, but in general, trees tend to shine most as building blocks in more complex algorithms. These composite models are called ensembles, and the most important tree ensembles are random forest and gradient boosted trees. While random forest uses either regression or classification trees depending on the type of target, gradient boosting can use regression trees to solve both classification and regression tasks. . Regression Tree in Action . Let&#39;s have a closer look at regression trees by training one on the diabetes dataset from scikit learn. According to the documentation: . Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline. . First we load the data. To make our lives easier, we&#39;ll just use two features:average blood pressure (bp) and the first blood serum measurement (s1) to predict the target. I&#39;ll rescale the features to make the values easier for me to read, but it won&#39;t affect our tree--more on that later. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns color_palette = &quot;viridis&quot; . from sklearn.datasets import load_diabetes X, y = load_diabetes(as_frame=True, return_X_y=True) X = 100 * X[[&#39;bp&#39;, &#39;s1&#39;]] . Let&#39;s grow a tree to predict the target given values of blood pressure and blood serum. . from sklearn.tree import DecisionTreeRegressor tree = DecisionTreeRegressor(max_depth=2) tree.fit(X,y); . To make predictions using our fitted tree, we start at the root node (which is at the top), and we work our way down moving left if our feature is less than the split threshold and to the right if it&#39;s greater than the split threshold. For example let&#39;s predict the target for a new case with bp= 1 and s1 = 5. Since our blood pressure of 1 is less than 2.359, we move to the left child node. Here, since our serum of 5 is greater than the threshold at 0.875, we move to the right child node. This node has no further children, and thus we return its predicted value of 155.343. . tree.predict(pd.DataFrame({&#39;bp&#39;: 1, &#39;s1&#39;: 5}, index=[0])) . array([155.34313725]) . Let&#39;s overlay these splits on our feature scatterplot to see how the tree has partitioned the feature space. . The tree has managed to carve out regions of feature space where the target values tend to be similar within each region, e.g. we have low target values in the bottom left partition and high target values in the far right region. . Let&#39;s take a look at the regression surface predicted by our tree. Since the tree predicts the exact same value for all instances in a given partition, the surface has only four distinct values. . Fabulous, now that we&#39;ve seen a tree in action, let&#39;s talk about trees&#39; key strengths and weaknesses. . Why trees are awesome . Trees are awesome because they are easy to use, and trees are easy to use because they are robust, require minimal data preprocessing, and can learn complex relationships without user intervention. . Feature Scaling . Trees owe their minimal data preprocessing requirements and their robustness to the fact that split finding is controlled by the sort order of the input feature values, rather than the values themselves. This means that trees are invariant to the scaling of input features, which in turn means that we don&#39;t need to fuss around with carefully rescaling all the numeric features before fitting a tree. It also means that trees tend to work well even if features are highly skewed or contain outliers. . Categoricals . Since trees just split data based on numeric feature values, we can easily handle most categorical features by using integer encoding. For example we might encode a size feature with small = 1, medium = 2, and large = 3. This works particularly well with ordered categories, because partitioning is consistent with the category semantics. It can also work well even if the categories have no order, because with enough splits a tree can carve each category into its own partition. . Missing Values . It&#39;s worth calling out that different implementations of the decision tree handle missing feature values in different ways. Notably, scikit-learn handles them by throwing an error and telling you not to pull such shenanigans. . ValueError: Input contains NaN, infinity or a value too large for dtype(&#39;float32&#39;). . On the other hand, XGBoost supports an elegant way to make use of missing values, which we will discuss more in a later post. . Interactions . Feature interactions can also be learned automatically. An interaction means that the effect of one feature on the target differs depending on the value of another feature. For example, the effect of some drug may depend on whether or not the patient exercises. After a tree splits on exercise, it can naturally learn the correct drug effects for both exercisers and non-exercisers. This intuition extends to higher-order interactions as well, as long as the tree has enough splits to parse the relationships. . Feature Selection . Because trees choose the best feature and threshold value at each split, they essentially perform automatic feature selection. This is great because even if we throw a lot of irrelevant features at a decision tree, it will simply tend not to use them for splits. Similarly, if two or more features are highly correlated or even redundant, the tree will simply choose one or the other when making each split; having both in the model will not cause catastrophic instability as it could in a linear model. . Feature-Target Relationship . Finally, it is possible for trees to discover complex nonlinear feature-target relationships without the need for user-specification of the relationships. This is because trees use local piecewise constant approximations without making any parametric assumptions. With enough splits, the tree can approximate arbitrary feature-target relationships. . Why trees are not so awesome . The main weakness of the decision tree is that, on its own, it tends to have poor predictive performance compared to other algorithms. The main reasons for this are the tendency to overfit and prediction quantization issues. . Overfitting . If we grow a decision tree until each leaf has exactly one instance in it, we will have simply memorized the training data, and our model will not generalize well. Basically the only defense against overfitting is to reduce the number of leaf nodes in the tree, either by using hyperparameters to stop splitting earlier or by removing certain leaf nodes after growing a deep tree. The problem here is that some of the benefits of trees, like ability to approximate arbitrary target patterns and ability to learn interaction effects, depend on having enough splits for the task. We can sometimes find ourselves in a situation where we cannot learn these complex relationships without overfitting the tree. . Quantization . Because regression trees use piecewise constant functions to approximate the target, prediction accuracy can deteriorate near split boundaries. For example, if the target is increasing with the feature, a tree might tend to overpredict the target on the left side of split boundaries and overpredict on the right side of split boundaries. . Extrapolation . Because they are trained by partitioning the feature space in a training dataset, trees cannot intelligently extrapolate beyond the data on which they are trained. For example if we query a tree for predictions beyond the greatest feature value encountered in training, it will just return the prediction corresponding to the largest in-sample feature values. . The Dark Side of Convenience . Finally, there is always a price to pay for convenience. While trees can work well even with a messy dataset containing outliers, redundant features, and thoughtlessly encoded categoricals, we will rarely achieve the best performance under these conditions. Taking the time to deal with outliers, removing redundant information, purposefully choosing appropriate categorical encodings, and building an understanding of the data will often lead to much better results. . How ensembling makes trees shine . We can go a long way toward addressing the issues of overfitting and prediction quantization by using trees as building blocks in larger algorithms called tree ensembles, the most popular examples being random forest and gradient boosted trees. A tree ensemble is a collection of different individual tree models whose predictions are averaged to generate an overall prediction. . Ensembling helps address overfitting because even if each individual tree is overfitted, the average of their individual noisy predictions will tend to be more stable. Think of it in terms of the bias variance tradeoff, where variance refers to how different a model prediction would be if it were trained on a different sample of training data. Since the ensemble is averaging over the predictions of all the individual models, training it on a different sample of training data would change the individual models predictions, but not their overall average. Thus, ensembling helps reduce the effects of overfitting by reducing model variance. . Ensembling also helps address prediction quantization issues. While each individual tree&#39;s predictions might express large jumps in the regression surface, averaging many different trees&#39; predictions together effectively generates a surface with more partitions and smaller jumps between them. This provides a smoother approximation of the feature-target relationship. . Wrapping Up . Well, there you go, that&#39;s my take on the high-level overview of the decision tree and its main strengths and weaknesses. As we&#39;ve seen, ensembling allows us to keep the conveniences of the decision tree while mitigating its core weakness of relatively weak predictive power. This is why tree ensembles are so popular in practical applications. We glossed over pretty much all details of how trees actually do their magic, but fear not, next time we&#39;re going to get rowdy and build one of these things from scratch. .",
            "url": "https://blog.mattbowers.dev/consider-the-decision-tree",
            "relUrl": "/consider-the-decision-tree",
            "date": " • Dec 12, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
            "content": ". Friends, this is going to be an epic post! Today, we bring together all the ideas we&#39;ve built up over the past few posts to nail down our understanding of the key ideas in Jerome Friedman&#39;s seminal 2001 paper: &quot;Greedy Function Approximation: A Gradient Boosting Machine.&quot; In particular, we&#39;ll summarize the highlights from the paper, and we&#39;ll build an in-house python implementation of his generic gradient boosting algorithm which can train with any differentiable loss function. What&#39;s more, we&#39;ll go ahead and take our generic gradient boosting machine for a spin by training it with several of the most popular loss functions used in practice. . Are you freaking stoked or what? . Sweet. Let&#39;s do this. . Friedman 2001: TL;DR . I&#39;ve mentioned this paper a couple of times before, but as far as I can tell, this is the origin of gradient boosting; it is therefore, a seminal work worth reading. You know what, I think you might like to pick up the paper and read it yourself. Like many papers, there is a lot of scary looking math in the first few pages, but if you&#39;ve been following along on this blog, you&#39;ll find that it&#39;s actually totally approachable. This is the kind of thing that cures imposter syndrome, so give it a shot. That said, here&#39;s the TL;DR as I see it. . The first part of the paper introduces the idea of fitting models by doing gradient descent in function space, an ingenious idea we spent an entire post demystifying earlier. Friedman goes on to introduce the generic gradient boost algorithm, which works with any differentiable loss function, as well as specific variants for minimizing absolute error, Huber loss, and binary deviance. In terms of hyperparameters, he points out that the learning rate can be used to reduce overfitting, while increased tree depth can help capture more complex interactions among features. He even discusses feature importance and partial dependence methods for interpreting fitted gradient boosting models. . Friedman concludes by musing about the advantages of gradient boosting with trees. He notes some key advantages afforded by the use of decision trees including no need to rescale input data, robustness against irrelevant input features, and elegant handling of missing feature values. He points out that gradient boosting manages to capitalize on the benefits of decision trees while minimizing their key weakness (crappy accuracy). I think this offers a great insight into why gradient boosting models have become so widespread and successful in practical ML applications. . Friedman&#39;s Generic Gradient Boosting Algorithm . Let&#39;s take a closer look at Friedman&#39;s original gradient boost algorithm, Alg. 1 in Section 3 of the paper (translated into the notation we&#39;ve been using so far). . Like last time, we have training data $( mathbf{y}, mathbf{X})$ where $ mathbf{y}$ is a length-$n$ vector of target values, and $ mathbf{X}$ is an $n times p$ matrix with $n$ observations of $p$ features. We also have a differentiable loss function $L( mathbf{y}, mathbf{ hat{y}}) = sum_{i=1}^n l(y_i, hat{y}_i)$, a &quot;learning rate&quot; hyperparameter $ eta$, and a fixed number of model iterations $M$. . Algorithm: gradient_boost$( mathbf{X}, mathbf{y},L, eta, M)$ returns: model $F_M$ . Let base model $F_0( mathbf{x}) = c$, where $c = text{argmin}_{c} sum_{i=1}^n l(y_i, c)$ . | for $m$ = $0$ to $M-1$: . | &nbsp;&nbsp;&nbsp;&nbsp; Let &quot;pseudo-residual&quot; vector $ mathbf{r}_m = - nabla_{ mathbf{ hat{y}}_m} L( mathbf{y}, mathbf{ hat{y}}_m)$ . | &nbsp;&nbsp;&nbsp;&nbsp; Train decision tree regressor $h_m( mathbf{X})$ to predict $ mathbf{r}_m$ (minimizing squared error) . | &nbsp;&nbsp;&nbsp;&nbsp; foreach terminal leaf node $t in h_m$: . | &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Let $v = text{argmin}_v sum_{i in t} l(y_i, F_m( mathbf{x}_i) + v)$ . | &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Set terminal leaf node $t$ to predict value $v$ . | &nbsp;&nbsp;&nbsp;&nbsp; $F_{m+1}( mathbf{X}) = F_{m}( mathbf{X}) + eta h_m( mathbf{X})$ . | Return composite model $F_M$ . | By now, most of this is already familiar to us. We begin by setting the base model $F_0$ equal to the constant prediction value that minimizes the loss over all examples in the training dataset (line 1). Then we begin the boosting iterations (line 2), each time computing the negative gradients of the loss with respect to the current model predictions (known as the pseudo residuals) (line 3). We then fit our next decision tree regressor to predict the pseudo residuals (line 4). . Then we encounter something new on lines 5-7. When we fit a vanilla decision tree regressor to predict pseudo residuals, we&#39;re using mean squared error as the loss function to train the tree. As you might imagine, this works well when the global loss function is also squared error. But if we want to use a global loss other than squared error, there is an additional trick we can use to further increase the composite model&#39;s accuracy. The idea is to continue using squared error to train each decision tree, keeping its structure and split conditions but altering the predicted value in each leaf to help minimize the global loss function. Instead of using the mean target value as the prediction for each node (as we would do when minimizing squared error), we use a numerical optimization method like line search to choose the constant value for that leaf that leads to the best overall loss. This is the same thing we did in line 1 of the algorithm to set the base prediction, but here we choose the optimal prediction for each terminal node of the newly trained decision tree. . Implementation . I did some (half-assed) searching on the interweb for an implementation of GBM that allows the user to provide a custom loss function, and you know what? I couldn&#39;t find anything. If you find another implementation, post in the comments so we can learn from it too. . Since we need to modify the values predicted by our decision trees&#39; terminal nodes, we&#39;ll want to brush up on the scikit-learn decision tree structure before we get going. You can see explanations of all the necessary decision tree hacks in this notebook. . import numpy as np from sklearn.tree import DecisionTreeRegressor from scipy.optimize import minimize class GradientBoostingMachine(): &#39;&#39;&#39;Gradient Boosting Machine supporting any user-supplied loss function. Parameters - n_trees : int number of boosting rounds learning_rate : float learning rate hyperparameter max_depth : int maximum tree depth &#39;&#39;&#39; def __init__(self, n_trees, learning_rate=0.1, max_depth=1): self.n_trees=n_trees; self.learning_rate=learning_rate self.max_depth=max_depth; def fit(self, X, y, objective): &#39;&#39;&#39;Fit the GBM using the specified loss function. Parameters - X : ndarray of size (number observations, number features) design matrix y : ndarray of size (number observations,) target values objective : loss function class instance Class specifying the loss function for training. Should implement two methods: loss(labels: ndarray, predictions: ndarray) -&gt; float negative_gradient(labels: ndarray, predictions: ndarray) -&gt; ndarray &#39;&#39;&#39; self.trees = [] self.base_prediction = self._get_optimal_base_value(y, objective.loss) current_predictions = self.base_prediction * np.ones(shape=y.shape) for _ in range(self.n_trees): pseudo_residuals = objective.negative_gradient(y, current_predictions) tree = DecisionTreeRegressor(max_depth=self.max_depth) tree.fit(X, pseudo_residuals) self._update_terminal_nodes(tree, X, y, current_predictions, objective.loss) current_predictions += self.learning_rate * tree.predict(X) self.trees.append(tree) def _get_optimal_base_value(self, y, loss): &#39;&#39;&#39;Find the optimal initial prediction for the base model.&#39;&#39;&#39; fun = lambda c: loss(y, c) c0 = y.mean() return minimize(fun=fun, x0=c0).x[0] def _update_terminal_nodes(self, tree, X, y, current_predictions, loss): &#39;&#39;&#39;Update the tree&#39;s predictions according to the loss function.&#39;&#39;&#39; # terminal node id&#39;s leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0] # compute leaf for each sample in ``X``. leaf_node_for_each_sample = tree.apply(X) for leaf in leaf_nodes: samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0] y_in_leaf = y.take(samples_in_this_leaf, axis=0) preds_in_leaf = current_predictions.take(samples_in_this_leaf, axis=0) val = self._get_optimal_leaf_value(y_in_leaf, preds_in_leaf, loss) tree.tree_.value[leaf, 0, 0] = val def _get_optimal_leaf_value(self, y, current_predictions, loss): &#39;&#39;&#39;Find the optimal prediction value for a given leaf.&#39;&#39;&#39; fun = lambda c: loss(y, current_predictions + c) c0 = y.mean() return minimize(fun=fun, x0=c0).x[0] def predict(self, X): &#39;&#39;&#39;Generate predictions for the given input data.&#39;&#39;&#39; return (self.base_prediction + self.learning_rate * np.sum([tree.predict(X) for tree in self.trees], axis=0)) . In terms of design, we implement a class for the GBM with scikit-like fit and predict methods. Notice in the below implementation that the fit method is only 10 lines long, and corresponds very closely to Friedman&#39;s gradient boost algorithm from above. Most of the complexity comes from the helper methods for updating the leaf values according to the specified loss function. . When the user wants to call the fit method, they&#39;ll need to supply the loss function they want to use for boosting. We&#39;ll make the user implement their loss (a.k.a. objective) function as a class with two methods: (1) a loss method taking the labels and the predictions and returning the loss score and (2) a negative_gradient method taking the labels and the predictions and returning an array of negative gradients. . Testing our Model . Let&#39;s test drive our custom-loss-ready GBM with a few different loss functions! We&#39;ll compare it to the scikit-learn GBM to sanity check our implementation. . from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier # test data def make_test_data(n, noise_scale): x = np.linspace(0, 10, 500).reshape(-1,1) y = (np.where(x &lt; 5, x, 5) + rng.normal(0, noise_scale, size=x.shape)).ravel() return x, y # print model loss scores def print_model_loss_scores(obj, y, preds, sk_preds): print(f&#39;From Scratch Loss = {obj.loss(y, pred):0.4}&#39;) print(f&#39;Scikit-Learn Loss = {obj.loss(y, sk_pred):0.4}&#39;) . Mean Squared Error . Mean Squared Error (a.k.a. Least Squares) loss produces estimates of the mean target value conditioned on the feature values. Here&#39;s the implementation. . x, y = make_test_data(500, 0.4) . # from scratch GBM class SquaredErrorLoss(): &#39;&#39;&#39;User-Defined Squared Error Loss&#39;&#39;&#39; def loss(self, y, preds): return np.mean((y - preds)**2) def negative_gradient(self, y, preds): return y - preds gbm = GradientBoostingMachine(n_trees=10, learning_rate=0.5, max_depth=1) gbm.fit(x, y, SquaredErrorLoss()) pred = gbm.predict(x) . # scikit-learn GBM sk_gbm = GradientBoostingRegressor(n_estimators=10, learning_rate=0.5, max_depth=1, loss=&#39;ls&#39;) sk_gbm.fit(x, y) sk_pred = sk_gbm.predict(x) . print_model_loss_scores(SquaredErrorLoss(), y, pred, sk_pred) . From Scratch Loss = 0.1787 Scikit-Learn Loss = 0.1787 . Mean Absolute Error . Mean Absolute Error (a.k.a.Least Absolute Deviations) loss produces estimates of the median target value conditioned on the feature values. Here&#39;s the implementation. . x, y = make_test_data(500, 0.4) . # from scratch GBM class AbsoluteErrorLoss(): &#39;&#39;&#39;User-Defined Absolute Error Loss&#39;&#39;&#39; def loss(self, y, preds): return np.mean(np.abs(y - preds)) def negative_gradient(self, y, preds): return np.sign(y - preds) gbm = GradientBoostingMachine(n_trees=10, learning_rate=0.5, max_depth=1) gbm.fit(x, y, AbsoluteErrorLoss()) pred = gbm.predict(x) . # scikit-learn GBM sk_gbm = GradientBoostingRegressor(n_estimators=10, learning_rate=0.5, max_depth=1, loss=&#39;lad&#39;) sk_gbm.fit(x, y) sk_pred = sk_gbm.predict(x) . print_model_loss_scores(AbsoluteErrorLoss(), y, pred, sk_pred) . From Scratch Loss = 0.3161 Scikit-Learn Loss = 0.3157 . Quantile Loss . Quantile loss yields estimates of a given quantile of the target variable conditioned on the features. Here&#39;s my implementation. . x, y = make_test_data(500, 1) . # from scratch GBM class QuantileLoss(): &#39;&#39;&#39;Quantile Loss Parameters - alpha : float quantile to be estimated, 0 &lt; alpha &lt; 1 &#39;&#39;&#39; def __init__(self, alpha): if alpha &lt; 0 or alpha &gt;1: raise ValueError(&#39;alpha must be between 0 and 1&#39;) self.alpha = alpha def loss(self, y, preds): e = y - preds return np.mean(np.where(e &gt; 0, self.alpha * e, (self.alpha - 1) * e)) def negative_gradient(self, y, preds): e = y - preds return np.where(e &gt; 0, self.alpha, self.alpha - 1) gbm = GradientBoostingMachine(n_trees=10, learning_rate=0.5, max_depth=1) gbm.fit(x, y, QuantileLoss(alpha=0.9)) pred = gbm.predict(x) . # scikit-learn GBM sk_gbm = GradientBoostingRegressor(n_estimators=10, learning_rate=0.5, max_depth=1, loss=&#39;quantile&#39;, alpha=0.9) sk_gbm.fit(x, y) sk_pred = sk_gbm.predict(x) . print_model_loss_scores(QuantileLoss(alpha=0.9), y, pred, sk_pred) . From Scratch Loss = 0.1837 Scikit-Learn Loss = 0.1836 . Binary Cross Entropy Loss . The previous losses are useful for regression problems, where the target is numeric. But we can also solve classification problems, simply by swapping in an appropriate loss function. Here we&#39;ll implement binary cross entropy, a.k.a. binary deviance, a.k.a. negative binomial log likelihood (sometimes abusively called log loss). One thing to remember is that, as with logistic regression, our model is actually predicting the log odds ratio, not the probability of the positive class. Thus we use expit transformations (the inverse of logit) whenever probabilities are needed, e.g., when predicting the probability that an observation belongs to the positive class. . # make categorical test data def expit(t): return np.exp(t) / (1 + np.exp(t)) x = np.linspace(-3, 3, 500) p = expit(x) y = rng.binomial(1, p, size=p.shape) x = x.reshape(-1,1) . # from scratch GBM class BinaryCrossEntropyLoss(): &#39;&#39;&#39;Binary Cross Entropy Loss Note that the predictions should be log odds ratios. &#39;&#39;&#39; def __init__(self): self.expit = lambda t: np.exp(t) / (1 + np.exp(t)) def loss(self, y, preds): p = self.expit(preds) return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p)) def negative_gradient(self, y, preds): p = self.expit(preds) return y / p - (1 - y) / (1 - p) gbm = GradientBoostingMachine(n_trees=10, learning_rate=0.5, max_depth=1) gbm.fit(x, y, BinaryCrossEntropyLoss()) pred = expit(gbm.predict(x)) . # scikit-learn GBM sk_gbm = GradientBoostingClassifier(n_estimators=10, learning_rate=0.5, max_depth=1, loss=&#39;deviance&#39;) sk_gbm.fit(x, y) sk_pred = sk_gbm.predict_proba(x)[:, 1] . print_model_loss_scores(BinaryCrossEntropyLoss(), y, pred, sk_pred) . From Scratch Loss = 0.6246 Scikit-Learn Loss = 0.6263 . Wrapping Up . Woohoo! We did it! We finally made it through Friedman&#39;s paper in its entirety, and we implemented the generic gradient boosting algorithm which works with any differentiable loss function. If you made it this far, great job, gold star! By now you hopefully have a pretty solid grasp on gradient boosting, which is good, because next we&#39;re going to dive into the modern Newton descent gradient boosting frameworks like XGBoost. Onward! . References . Friedman&#39;s 2001 paper: Greedy Function Approximation: A Gradient Boosting Machine .",
            "url": "https://blog.mattbowers.dev/gradient-boosting-machine-with-any-loss-function",
            "relUrl": "/gradient-boosting-machine-with-any-loss-function",
            "date": " • Oct 23, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Hello PySpark!",
            "content": ". Well, you guessed it: it&#39;s time for us to learn PySpark! . I know, I know, I can hear you screaming into your pillow. Indeed we just spent all that time converting from R and learning python and why the hell do we need yet another API for working with dataframes? . That&#39;s a totally fair question. . So what happens when we&#39;re working on something in the real world, where datasets get large in a hurry, and we suddenly have a dataframe that no longer fits into memory? We need a way for our computations and datasets to scale across multiple nodes in a distributed system without having to get too fussy about all the distributed compute details. . Enter PySpark. . I think it&#39;s fair to think of PySpark as a python package for working with arbitrarily large dataframes, i.e., it&#39;s like pandas but scalable. It&#39;s built on top of Apache Spark, a unified analytics engine for large-scale data processing. PySpark is essentially a way to access the functionality of spark via python code. While there are other high-level interfaces to Spark (such as Java, Scala, and R), for data scientists who are already working extensively with python, PySpark will be the natural interface of choice. PySpark also has great integration with SQL, and it has a companion machine learning library called MLlib that&#39;s more or less a scalable scikit-learn (maybe we can cover it in a future post). . So, here&#39;s the plan. First we&#39;re going to get set up to run PySpark locally in a jupyter notebook on our laptop. This is my preferred environment for interactively playing with PySpark and learning the ropes. Then we&#39;re going to get up and running in PySpark as quickly as possible by reviewing the most essential functionality for working with dataframes and comparing it to how we would do things in pandas. Once we&#39;re comfortable running PySpark on the laptop, it&#39;s going to be much easier to jump onto a distributed cluster and run PySpark at scale. . Let&#39;s do this. . How to Run PySpark in a Jupyter Notebook on Your Laptop . Ok, I&#39;m going to walk us through how to get things installed on a Mac or Linux machine where we&#39;re using homebrew and conda to manage virtual environments. If you have a different setup, your favorite search engine will help you get PySpark set up locally. . Install Spark . Most of the Spark sourcecode is written in Scala, so first we install Scala. . $ brew install scala . Install Spark. . $ brew install apache-spark . Check where Spark is installed. . $ brew info apache-spark apache-spark: stable 3.1.1, HEAD Engine for large-scale data processing https://spark.apache.org/ /usr/local/Cellar/apache-spark/3.1.2 (1,361 files, 242.6MB) * ... . Set the Spark home environment variable to the path returned by brew info with /libexec appended to the end. Don&#39;t forget to add the export to your .zshrc file too. . $ export SPARK_HOME=/usr/local/Cellar/apache-spark/3.1.2/libexec . Test the installation by starting the Spark shell. . $ spark-shell ... Welcome to ____ __ / __/__ ___ _____/ /__ _ / _ / _ `/ __/ &#39;_/ /___/ .__/ _,_/_/ /_/ _ version 3.1.1 /_/ Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 14.0.1) Type in expressions to have them evaluated. Type :help for more information. scala&gt; . If you get the scala&gt; prompt, then you&#39;ve successfully installed Spark on your laptop! . Install PySpark . Use conda to install the PySpark python package. As usual, it&#39;s advisable to do this in a new virtual environment. . $ conda install pyspark . You should be able to launch an interactive PySpark REPL by saying pyspark. . $ pyspark ... Welcome to ____ __ / __/__ ___ _____/ /__ _ / _ / _ `/ __/ &#39;_/ /__ / .__/ _,_/_/ /_/ _ version 3.1.2 /_/ Using Python version 3.8.3 (default, Jul 2 2020 11:26:31) Spark context Web UI available at http://192.168.100.47:4041 Spark context available as &#39;sc&#39; (master = local[*], app id = local-1624127229929). SparkSession available as &#39;spark&#39;. &gt;&gt;&gt; . This time we get a familiar python &gt;&gt;&gt; prompt. This is an interactive shell where we can easily experiment with PySpark. Feel free to run the example code in this post here in the PySpark shell, or, if you prefer a notebook, read on and we&#39;ll get set up to run PySpark in a jupyter notebook. . The Spark Session Object . You may have noticed that when we launched that PySpark interactive shell, it told us that something called SparkSession was available as &#39;spark&#39;. So basically, what&#39;s happening here is that when we launch the pyspark shell, it instantiates an object called spark which is an instance of class pyspark.sql.session.SparkSession. The spark session object is going to be our entry point for all kinds of PySpark functionality, i.e., we&#39;re going to be saying things like spark.this() and spark.that() to make stuff happen. . The PySpark interactive shell is kind enough to instantiate one of these spark session objects for us automatically. However, when we&#39;re using another interface to PySpark (like say a jupyter notebook running a python kernal), we&#39;ll have to make a spark session object for ourselves. . Create a PySpark Session in a Jupyter Notebook . There are a few ways to run PySpark in jupyter which you can read about here. . For derping around with PySpark on your laptop, I think the best way is to instantiate a spark session from a jupyter notebook running on a regular python kernel. The method we&#39;ll use involves running a standard jupyter notebook session with a python kernal and using the findspark package to initialize the spark session. So, first install the findspark package. . $ conda install findspark . Launch jupyter as usual. . $ jupyter notebook . Go ahead and fire up a new notebook using a regular python 3 kernal. Once you land inside the notebook, there are a couple things we need to do to get a spark session instantiated. You can think of this as boilerplate code that we need to run in the first cell of a notebook where we&#39;re going to use PySpark. . import pyspark import findspark from pyspark.sql import SparkSession findspark.init() spark = SparkSession.builder.appName(&#39;My Spark App&#39;).getOrCreate() . First we&#39;re running findspark&#39;s init() method to find our Spark installation. If you run into errors here, make sure you got the SPARK_HOME environment variable correctly set in the install instructions above. Then we instantiate a spark session as spark. Once you run this, you&#39;re ready to rock and roll with PySpark in your jupyter notebook. . . Note: Spark provides a handy web UI that you can use for monitoring and debugging. Once you instantiate the spark session You can open the UI in your web browser at http://localhost:4040/jobs/. . PySpark Concepts . PySpark provides two main abstractions for data: the RDD and the dataframe. RDD&#39;s are just a distributed list of objects; we won&#39;t go into details about them in this post. For us, the key object in PySpark is the dataframe. . While PySpark dataframes expose much of the functionality you would expect from a library for tabular data manipulation, they behave a little differently from pandas dataframes, both syntactically and under-the-hood. There are a couple of key concepts that will help explain these idiosyncracies. . Immutability - Pyspark RDD&#39;s and dataframes are immutable. This means that if you change an object, e.g. by adding a column to a dataframe, PySpark returns a reference to a new dataframe; it does not modify the existing dataframe. This is kind of nice, because we don&#39;t have to worry about that whole view versus copy nonsense that happens in pandas. . Lazy Evaluation - Lazy evaluation means that when we start manipulating a dataframe, PySpark won&#39;t actually perform any of the computations until we explicitly ask for the result. This is nice because it potentially allows PySpark to do fancy optimizations before executing a sequence of operations. It&#39;s also confusing at first, because PySpark will seem to blaze through complex operations and then take forever to print a few rows of the dataframe. . PySpark Dataframe Essentials . Creating a PySpark dataframe with createDataFrame() . The first thing we&#39;ll need is a way to make dataframes. createDataFrame() allows us to create PySpark dataframes from python objects like nested lists or pandas dataframes. Notice that createDataFrame() is a method of the spark session class, so we&#39;ll call it from our spark session sparkby saying spark.createDataFrame(). . # create pyspark dataframe from nested lists my_df = spark.createDataFrame( data=[ [2022, &quot;tiger&quot;], [2023, &quot;rabbit&quot;], [2024, &quot;dragon&quot;] ], schema=[&#39;year&#39;, &#39;animal&#39;] ) . Let&#39;s read the seaborn tips dataset into a pandas dataframe and then use it to create a PySpark dataframe. . import pandas as pd # load tips dataset into a pandas dataframe pandas_df = pd.read_csv(&#39;https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv&#39;) # create pyspark dataframe from a pandas dataframe pyspark_df = spark.createDataFrame(pandas_df) . . Note: In real life when we&#8217;re running PySpark on a large-scale distributed system, we would not generally want to use python lists or pandas dataframes to load data into PySpark. Ideally we would want to read data directly from where it is stored on HDFS, e.g. by reading parquet files, or by querying directly from a hive database using spark sql. . Peeking at a dataframe&#39;s contents . The default print method for the PySpark dataframe will just give you the schema. . pyspark_df . DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint] . If we want to peek at some of the data, we&#39;ll need to use the show() method, which is analogous to the pandas head(). Remember that show() will cause PySpark to execute any operations that it&#39;s been lazily waiting to evaluate, so sometimes it can take a while to run. . # show the first few rows of the dataframe pyspark_df.show(5) . +-+-+++++-+ |total_bill| tip| sex|smoker|day| time|size| +-+-+++++-+ | 16.99|1.01|Female| No|Sun|Dinner| 2| | 10.34|1.66| Male| No|Sun|Dinner| 3| | 21.01| 3.5| Male| No|Sun|Dinner| 3| | 23.68|3.31| Male| No|Sun|Dinner| 2| | 24.59|3.61|Female| No|Sun|Dinner| 4| +-+-+++++-+ only showing top 5 rows . We thus encounter our first rude awakening. PySpark&#39;s default representation of dataframes in the notebook isn&#39;t as pretty as that of pandas. But no one ever said it would be pretty, they just said it would be scalable. . You can also use the printSchema() method for a nice vertical representation of the schema. . # show the dataframe schema pyspark_df.printSchema() . root |-- total_bill: double (nullable = true) |-- tip: double (nullable = true) |-- sex: string (nullable = true) |-- smoker: string (nullable = true) |-- day: string (nullable = true) |-- time: string (nullable = true) |-- size: long (nullable = true) . Select columns by name . You can select specific columns from a dataframe using the select() method. You can pass either a list of names, or pass names as arguments. . # select some of the columns pyspark_df.select(&#39;total_bill&#39;, &#39;tip&#39;) # select columns in a list pyspark_df.select([&#39;day&#39;, &#39;time&#39;, &#39;total_bill&#39;]) . Filter rows based on column values . Analogous to the WHERE clause in SQL, and the query() method in pandas, PySpark provides a filter() method which returns only the rows that meet the specified conditions. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using and and or, and you can even do a SQL-like in to check if the column value matches any items in a list. . ## compare a column to a value pyspark_df.filter(&#39;total_bill &gt; 20&#39;) # compare two columns with arithmetic pyspark_df.filter(&#39;tip &gt; 0.15 * total_bill&#39;) # check equality with a string value pyspark_df.filter(&#39;sex == &quot;Male&quot;&#39;) # check equality with any of several possible values pyspark_df.filter(&#39;day in (&quot;Sat&quot;, &quot;Sun&quot;)&#39;) # use &quot;and&quot; pyspark_df.filter(&#39;day == &quot;Fri&quot; and time == &quot;Lunch&quot;&#39;) . If you&#39;re into boolean indexing with the brackets, PySpark does support that too, but I encourage you to use filter() instead. Check out my rant about why you shouldn&#39;t use boolean indexing) for the details. The TLDR is that filter() requires less typing, makes your code more readable and portable, and it allows you to chain method calls together using dot chains. . Here&#39;s the boolean indexing equivalent of the last example from above. . # using boolean indexing pyspark_df[(pyspark_df.day == &#39;Fri&#39;) &amp; (pyspark_df.time == &#39;Lunch&#39;)] . I know, it looks horrendous, but not as horrendous as the error message you&#39;ll get if you forget the parentheses. :smiley: . Add new columns to a dataframe . You can add new columns which are functions of the existing columns with the withColumn() method. . import pyspark.sql.functions as f # add a new column using col() to reference other columns pyspark_df.withColumn(&#39;tip_percent&#39;, f.col(&#39;tip&#39;) / f.col(&#39;total_bill&#39;)) . Notice that we&#39;ve imported the pyspark.sql.functions) module. This module contains lots of useful functions that we&#39;ll be using all over the place, so it&#39;s probably a good idea to go ahead and import it whenever you&#39;re using PySpark. BTW, it seems like folks usually import this module as f or F. In this example we&#39;re using the col() function, which allows us to refer to columns in our dataframe using string representations of the column names. . You could also achieve the same result using the dot to reference the other columns, but this requires us to type the dataframe name over and over again, which makes it harder to reuse this code on different dataframes or in dot chains. . # add a new column using the dot to reference other columns (less recommended) pyspark_df.withColumn(&#39;tip_percent&#39;, pyspark_df.tip / pyspark_df.total_bill) . If you want to apply numerical transformations like exponents or logs, use the built-in functions in the pyspark.sql.functions module. . # log pyspark_df.withColumn(&#39;log_bill&#39;, f.log(f.col(&#39;total_bill&#39;))) # exponent pyspark_df.withColumn(&#39;bill_squared&#39;, f.pow(f.col(&#39;total_bill&#39;), 2)) . You can implement conditional assignment like SQL&#39;s CASE WHEN construct using the when() function and the otherwise() method. . # conditional assignment (like CASE WHEN) pyspark_df.withColumn(&#39;is_male&#39;, f.when(f.col(&#39;sex&#39;) == &#39;Male&#39;, True).otherwise(False)) # using multiple when conditions and values pyspark_df.withColumn(&#39;bill_size&#39;, f.when(f.col(&#39;total_bill&#39;) &lt; 10, &#39;small&#39;) .when(f.col(&#39;total_bill&#39;) &lt; 20, &#39;medium&#39;) .otherwise(&#39;large&#39;) ) . Remember that since PySpark dataframes are immutable, calling withColumns() on a dataframe returns a new dataframe. If you want to persist the result, you&#39;ll need to make an assignment. . pyspark_df = pyspark_df.withColumns(...) . Group by and aggregate . PySpark provides a groupBy() method similar to the pandas groupby(). Just like in pandas, we can call methods like count() and mean() on our grouped dataframe, and we also have a more flexible agg() method that allows us to specify column-aggregation mappings. . # group by and count pyspark_df.groupBy(&#39;time&#39;).count().show() . ++--+ | time|count| ++--+ | Lunch| 68| |Dinner| 176| ++--+ . # group by and specify column-aggregation mappings with agg() pyspark_df.groupBy(&#39;time&#39;).agg({&#39;total_bill&#39;: &#39;mean&#39;, &#39;tip&#39;: &#39;max&#39;}).show() . ++--++ | time|max(tip)| avg(total_bill)| ++--++ | Lunch| 6.7|17.168676470588235| |Dinner| 10.0| 20.79715909090909| ++--++ . If you want to get fancier with your aggregations, it might just be easier to express them using hive syntax. Read on to find out how. . Run Hive SQL on dataframes . One of the mind-blowing features of PySpark is that it allows you to write hive SQL queries on your dataframes. To take a PySpark dataframe into the SQL world, use the createOrReplaceTempView() method. This method takes one string argument which will be the dataframes name in the SQL world. Then you can use spark.sql() to run a query. The result is returned as a PySpark dataframe. . # put pyspark dataframe in SQL world and query it pyspark_df.createOrReplaceTempView(&#39;tips&#39;) spark.sql(&#39;select * from tips&#39;).show(5) . This is awesome for a couple of reasons. First, it allows us to easily express any transformations in hive syntax. If you&#39;re like me and you&#39;ve already been using hive, this will dramatically reduce the PySpark learning curve, because when in doubt, you can always bump a dataframe into the SQL world and simply use hive to do what you need. Second, if you have a hive deployment, PySpark&#39;s SQL world also has access to all of your hive tables. This means you can write queries involving both hive tables and your PySpark dataframes. It also means you can run hive commands, like inserting into a table, directly from PySpark. . Let&#39;s do some aggregations that might be a little trickier to do using the PySpark built-in functions. . # run hive query and save result to dataframe tip_stats_by_time = spark.sql(&quot;&quot;&quot; select time , count(*) as n , avg(tip) as avg_tip , percentile_approx(tip, 0.5) as med_tip , avg(case when tip &gt; 3 then 1 else 0 end) as pct_tip_gt_3 from tips group by 1 &quot;&quot;&quot;) tip_stats_by_time.show() . Visualization with PySpark . There aren&#39;t any tools for visualization included in PySpark. But that&#39;s no problem, because we can just use the toPandas() method on a PySpark dataframe to pull data back into pandas. Once we have a pandas dataframe, we can happily build visualizations as usual. Of course, if your PySpark dataframe is huge, you wouldn&#39;t want to use toPandas() directly, because PySpark will attempt to read the entire contents of its huge dataframe into memory. Instead, it&#39;s best to use PySpark to generate aggregations of your data for plotting or to pull only a sample of your full data into pandas. . # read aggregated pyspark dataframe into pandas for plotting plot_pdf = tip_stats_by_time.toPandas() plot_pdf.plot.bar(x=&#39;time&#39;, y=[&#39;avg_tip&#39;, &#39;med_tip&#39;]); . Wrapping Up . So that&#39;s a wrap on our crash course in working with PySpark. You now have a good idea of what pyspark is and how to get started manipulating dataframes with it. Stay tuned for a future post on PySpark&#39;s companion ML library MLlib. In the meantime, may no dataframe be too large for you ever again. .",
            "url": "https://blog.mattbowers.dev/hello-pyspark",
            "relUrl": "/hello-pyspark",
            "date": " • Jun 22, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "How Gradient Boosting Does Gradient Descent",
            "content": ". In the last two posts, we learned the basics of gradient boosting machines and the gradient descent algorithm. But we still haven&#39;t explicitly addressed what puts the &quot;gradient&quot; in gradient boosting. It turns out that gradient boosting models are using a sort of gradient descent to minimize their loss function; according to Friedman&#39;s classic paper, they&#39;re doing gradient descent in &quot;function space&quot;. If you&#39;re like me, and this is your first encounter with this idea, then the phrase &quot;gradient descent in function space&quot; is going to sound a little, ahem, mysterious. No worries, friends; we&#39;re about to make sense of it all. . Understanding the underlying mechanics of gradient boosting as a form of gradient descent will empower us to train our models with custom loss functions. This opens up many interesting possibilities including doing not only regression and classification, but also predicting quantiles, prediction intervals, and even the conditional probability distribution of the response variable. . Generalized intuition for gradient boosting . In my earlier post on building a gradient boosting model from scratch, we established the intuition for how gradient boosting works in a regression problem. In this post we&#39;re going to generalize the ideas we encountered in the regression context, so check out the earlier post if you&#39;re not already familiar with gradient boosting for regression. In the following sections we&#39;ll build up the intuition for gradient boosting in general terms, and then we&#39;ll be able to state the gradient boosting algorithm in a form that can fit models to customized loss functions. . The loss function . You recall that we measure how well a model fits data by using a loss function that yields small values when a model fits well. &quot;Training&quot; essentially means finding the model that minimizes our loss function. A loss function takes the correct target values and the predicted target values, and it returns a scalar loss score. For example, in the last post on gradient descent we used a mean squared error (MSE) loss . $$L( mathbf{y}, hat{ mathbf{y}}) = frac{1}{n} sum_{i=1}^{n} (y_i - hat{y}_i)^2 $$ . where we express the correct targets and predicted values as the vector arguments $ mathbf{y}=[y_1,y_2, dots,y_n]$ and $ hat{ mathbf{y}}=[ hat{y}_1, hat{y}_2, dots, hat{y}_n]$ respectively. . Which way to nudge a prediction to get a better model . Now, let&#39;s say we have a model $F( mathbf{X})= mathbf{ hat{y}}$ that we want to improve. One approach is that we could figure out whether each prediction $ hat{y}_i$ needed to be higher or lower to get a better loss score. We could then nudge each prediction in the right direction, thereby decreasing our model&#39;s loss score. . To figure out whether we should increase or decrease a particular prediction $ hat{y}_i$ (and by how much), we can compute the partial derivative of the loss function with respect to that prediction. Recall the partial derivative just tells us the rate of change in a function when we change one of its arguments. Since we want to make the loss $L( mathbf{y}, mathbf{ hat{y}})$ decrease, we can use the negative partial derivative of the loss function with respect to a given prediction to help us choose the right nudge for that prediction. . $$ text{nudge for } hat{y}_i = - frac{ partial L( mathbf{y}, mathbf{ hat{y}})}{ partial hat{y}_i}$$ . Sometimes it can get a little intense when there are partial derivatives flying around, but it doesn&#39;t have to be that way. Remember that in practice $- frac{ partial L( mathbf{y}, mathbf{ hat{y}})}{ partial hat{y}_i}$ is just an expression that evaluates to a number like 2.7 or -0.5, and here it&#39;s telling us how to nudge $ hat{y}_i$ to decrease our loss score. . The intuition is that if $ frac{ partial L( mathbf{y}, mathbf{ hat{y}})}{ partial hat{y}_i}$ is negative, then increasing the prediction $ hat{y}_i$ will make the loss decrease. We then notice that the negative of the partial derivative tells us whether to increase or decrease $ hat{y}_i$. For example, if $- frac{ partial L( mathbf{y}, mathbf{ hat{y}})}{ partial hat{y}_i}$ is positive, then increasing the prediction $ hat{y}_i$ will make the loss decrease; whereas if $- frac{ partial L( mathbf{y}, mathbf{ hat{y}})}{ partial hat{y}_i}$ is negative, then decreasing the prediction $ hat{y}_i$ will make the loss decrease. . Since we&#39;ll want to find the right nudge for each of the $ hat{y}_i$&#39;s, we can use the negative gradient of the loss function $L( mathbf{y}, mathbf{ hat{y}})$ with respect to the vector argument $ hat{ mathbf{y}}$ to get the vector of all the partial derivatives. Let&#39;s call this vector of desired nudge values $ mathbf{r}$. . $$ mathbf{r} = - nabla_{ hat{ mathbf{y}}} L( mathbf{y}, hat{ mathbf{y}}) = left [ - frac{ partial L( mathbf{y}, mathbf{ hat{y}})}{ partial hat{y}_1}, - frac{ partial L( mathbf{y}, mathbf{ hat{y}})}{ partial hat{y}_2}, cdots, - frac{ partial L( mathbf{y}, mathbf{ hat{y}})}{ partial hat{y}_n} right ]$$ . Nudging predictions in the right direction . Great, now that we know we should nudge each prediction in the direction of the negative partial derivative of the loss with respect to that prediction, we need to figure out how to do the actual nudging. Remember that we already have an initial model $F( mathbf{X})= mathbf{ hat{y}}$. . At this point we might be tempted to simply add the vector of nudge values to our predictions to get better predictions. . $$ text{we might be tempted to try } mathbf{ hat{y}}_{ text{new}} = mathbf{ hat{y}} + mathbf{r}$$ . Sure, based on our reasoning in the previous section, plugging the vector of nudged predictions into the loss function would yield a lower loss score. . $$ L( mathbf{y}, mathbf{ hat{y}} + mathbf{r}) le L( mathbf{y}, mathbf{ hat{y}})$$ . The problem is that this will only work for in-sample data, because we only know the nudge values for the cases which are present in the training dataset. In order for our model to generalize to unseen test data, we need a way to get the nudge values for new observations of the independent variables. So how can we do that? . Well what if we fit another model $h( mathbf{X})$ that used our same features $ mathbf{X}$ to predict our desired nudge values $ mathbf{r}$, and then we added that new model to our original model $F( mathbf{X})$. For a given prediction the nudge model $h( mathbf{X})$ would essentially return an approximation of the desired nudge, so adding it would push the prediction in the right direction to decrease the loss function. Furthermore, the nudge model can return predictions of the nudges for out-of-sample cases which are not present in the training dataset. Since both the initial model $F( mathbf{X})$ and the nudge model $h( mathbf{X})$ are functions of our features $ mathbf{X}$, we can add the two functions to get an updated model that can generalize beyond the training data. . $$F_{ text{new}} ( mathbf{X}) = F( mathbf{X}) + h( mathbf{X})$$ . A generalized gradient boosting algorithm . Ok, let&#39;s put these pieces of intuition together to create a more general gradient boosting algorithm recipe. . We begin with training data $( mathbf{y}, mathbf{X})$ where $ mathbf{y}$ is a length-$n$ vector of target values, and $ mathbf{X}$ is an $n times p$ matrix with $n$ observations of $p$ features. We also have a differentiable loss function $L( mathbf{y}, mathbf{ hat{y}})$, a &quot;learning rate&quot; hyperparameter $ eta$, and a fixed number of model iterations $M$. . We create an initial model $F_0( mathbf{X})$ that predicts a constant value. We choose the constant value that would give the best loss score. . $$F_0( mathbf{X}) = underset{c}{ operatorname{argmin}} L( mathbf{y}, c)$$ . Then we iteratively update the initial model with $M$ nudge models. . For $m$ in 0 to $M-1$: . Compute current composite model predictions $ mathbf{ hat{y}}_{m} = F_{m}( mathbf{X})$. | Compute the desired nudge values given by the negative gradient of the loss function with respect to each prediction $ mathbf{r}_m = - nabla_{ mathbf{ hat{y}}_m} L ( mathbf{y}, mathbf{ hat{y}}_m)$. | Fit a weak model (e.g. shallow decision tree) $h_{m}( mathbf{X})$ that predicts the nudge values $ mathbf{r}_{m}$ using features $ mathbf{X}$. | Update the composite model. | . $$F_{m+1}( mathbf{X}) = F_{m}( mathbf{X}) + eta h_{m}( mathbf{X})$$ . After $M$ iterations, we are left with the final composite model $F_M( mathbf{X})$. . Wait, in what sense is this doing gradient descent? . In my previous post, we learned how to use gradient descent to iteratively update model parameters to find a model that minimizes the loss function. We could write the update rule as . $$ mathbf{ theta}_{t+1} = mathbf{ theta}_{t} + eta ( - nabla_{ mathbf{ theta}} L( mathbf{y}, mathbf{ hat{y}}_{ mathbf{ theta}_{t}}) ) $$ . where the predictions $ mathbf{ hat{y}}$ depend on the model parameters $ mathbf{ theta}$, and we&#39;re trying to find the value of the parameter vector $ mathbf{ theta}$ that minimizes the loss function $L( mathbf{y}, mathbf{ hat{y}}_{ mathbf{ theta}_{t}})$, so we nudge the vector $ mathbf{ theta}_t$ by the negative gradient of $L( mathbf{y}, mathbf{ hat{y}}_{ mathbf{ theta}_{t}})$ with respect to $ mathbf{ theta}_t$. Compare that with the boosting model update rule we obtained in the previous section. . $$F_{m+1}( mathbf{X}) = F_{m}( mathbf{X}) + eta h_{m}( mathbf{X})$$ . where $h_{m}( mathbf{X}) approx - nabla_{ mathbf{ hat{y}}_m} L ( mathbf{y}, mathbf{ hat{y}}_m)$. . If we replace $F( mathbf{X})$ with its prediction vector $ mathbf{ hat{y}}$, and we replace the nudge model $h( mathbf{X})$ with the negative gradient of the loss function (which it approximates), the likeness to the parameter gradient descent update rule becomes more obvious. . $$ mathbf{ hat{y}}_{m+1} approx mathbf{ hat{y}}_m + eta (- nabla_{ mathbf{ hat{y}}_m} L ( mathbf{y}, mathbf{ hat{y}}_m))$$ . Indeed, gradient boosting is performing gradient descent to obtain a good model by minimizing a loss function. But there are a couple of key differences between gradient boosting and the parameter gradient descent that we discussed in the previous post. . Gradient boosting versus parameter gradient descent . The generic gradient boosting algorithm outlined above implies two key differences from parameter gradient descent. . Instead of nudging parameters, we nudge each individual prediction, thus instead of taking the gradient of loss with respect to the parameters, we take the gradient with respect to the predictions. | Instead of directly adding the negative gradient to our current parameter values, we create a functional approximation of the negative gradient and add that to our model. Our functional approximation is just a crappy model that tries to use the model features to predict the negative gradient of the loss with respect to our current model predictions. | The true genius of the gradient boosting algorithm is in chasing the negative gradient of the loss with crappy models, rather than using it to directly update our predictions. If we just directly added the negative gradient of the loss to our predictions, and plugged them into the loss function we could get a lower loss score, but our updated model would be useless since it couldn&#39;t make predictions on new out-of-sample data. Instead we train a crappy model to predict the negative gradient of the loss with respect to the current model predictions, thus we can iteratively update our composite model by adding these crappy models to it. . Gradient boosting is gradient descent in function space, a.k.a. prediction space . Let&#39;s address the statement in Friedman&#39;s classic paper that gradient boosting is doing gradient descent in function space. Again we&#39;ll use parameter gradient descent as a basis for comparison. . In parameter gradient descent, we have a vector of parameter values which, when plugged into the loss function, return some loss score. At each step of gradient descent, we compute the negative gradient of the loss function with respect to each parameter; that tells us which way to nudge each parameter value to achieve a lower loss score. We then add this vector of parameter nudge values to our previous parameter vector to get the new parameter vector. We could view this sequence of successive parameter vectors as a trajectory passing through parameter space, the space spanned by all possible parameter values. Therefore parameter gradient descent operates in parameter space. . In contrast, when we do gradient boosting, at each step we have a model, a.k.a. a function, that maps feature values to predictions. Given our training dataset, this model yields predictions which can be plugged into our loss function to get a loss score. At each boosting iteration, we compute the negative gradient of the loss with respect to each of the predictions; that tells us which way to nudge each prediction to achieve a lower loss score. We then create a function (a crappy model) that takes feature values and returns an approximation of the corresponding prediction&#39;s nudge value. We then add this crappy model (a function) to our current composite model (also a function) to get the new composite model (you guessed it; also a function). And so by analogy with parameter vectors in parameter space, we can view this sequence of successive model functions as a trajectory passing through function space, the space spanned by all possible functions that map feature values to predictions. Therefore, gradient boosting does gradient descent in function space. . If this talk about function space still feels a little abstract, you could just use the same substitution trick we used above and swap the model $F( mathbf{X})$ for its predictions $ mathbf{ hat{y}}$ which is just a vector of numbers. The target values for our nudge models are given by the negative gradient of the loss with respect to this prediction vector. From here, we can see that each time we add a new nudge model to our composite model, we get a new prediction vector. We can view this sequence of successive prediction vectors as a trajectory passing through prediction space, the space spanned by all possible prediction vector values. Therefore we can also say that gradient boosting does gradient descent in prediction space. . So why did we fit the crappy models to the residuals in our regression GBM? . In my first post on gradient boosting machines, in the interest of simplicity I left one key aspect of the problem unaddressed, that is, what loss function were we using to train that GBM? It turns out that because of the way we built our GBM, without knowing it we were actually using a mean squared error (MSE) loss function. . $$L( mathbf{y}, hat{ mathbf{y}}) = frac{1}{n} sum_{i=1}^{n} (y_i - hat{y}_i)^2 $$ . If the GBM was using gradient descent to find a $ hat{ mathbf{y}}$ vector that minimized this loss function, then at each iteration it would have to nudge the current $ hat{ mathbf{y}}$ by the negative gradient of the loss function with respect to $ hat{ mathbf{y}}$, i.e. $- nabla_{ hat{ mathbf{y}}} L( mathbf{y}, hat{ mathbf{y}})$. Since our loss function takes a length $n$ vector of predictions $ hat{ mathbf{y}}$ as input, the gradient will be a length-$n$ vector of partial derivatives with respect to each of the predictions $ hat{y}_i$. Let&#39;s start by taking the negative partial derivative with respect to a particular prediction $ hat{y}_j$. . $$ begin{array}{rcl} - frac{ partial}{ partial hat{y}_j} L( mathbf{y}, mathbf{ hat{y}}) &amp; = &amp; - frac{ partial}{ partial hat{y}_j} left ( frac{1}{n} sum_{i=1}^{n} (y_i - hat{y}_i)^2 right ) &amp; = &amp; - frac{ partial}{ partial hat{y}_j} left ( frac{1}{n} (y_j - hat{y}_j)^2 right ) &amp; = &amp; - frac{1}{n} (2)(y_j - hat{y}_j) frac{ partial}{ partial hat{y}_j} (y_j - hat{y}_j) &amp; = &amp; frac{2}{n} (y_j - hat{y}_j) end{array} $$It turns out that the negative partial derivative of the MSE loss function with respect to a particular prediction $ hat{y}_i$ is proportional to the residual $y_i - hat{y}_i$! This is a pretty intuitive result, because if we nudge a prediction by it&#39;s residual, we&#39;ll end up with the correct target value. . We can go ahead and write the nudge vector as . $$ mathbf{r} = - nabla_{ hat{ mathbf{y}}} L( mathbf{y}, hat{ mathbf{y}}) = frac{2}{n}( mathbf{y} - hat{ mathbf{y}})$$ . which is proportional to the residual vector $ mathbf{y} - hat{ mathbf{y}}$. This means that when we use the mean squared error loss function, our nudge values are given by the current model residuals, and therefore each new crappy model targets the previous model&#39;s residuals. . And this result brings us full circle, back to our original intuition from the first GBM post about chasing residuals with crappy models. Now we see that intuitive idea is just a special case of the more general and, dare I say, even more beautiful idea of chasing the negative gradient of the loss function with crappy models. . Key Takeaways . We covered a lot of conceptual ground in this post, so let&#39;s recap the key ideas. . Gradient boosting can use gradient descent to minimize any differentiable loss function in service of creating a good final model. | There are two key differences between gradient boosting and parameter gradient descent: In gradient boosting, we nudge prediction values rather than parameter values, so to find the desired nudge values, we take the negative gradient of the loss function with respect to the predictions. | In gradient boosting, we nudge our predictions by adding a crappy model that approximates the nudge values, rather than adding the nudge values directly to the predictions. | . | Gradient boosting does gradient descent in function space. But since the model predictions are just numeric vectors, and since we take the gradient of the loss function with respect to the prediction vector, it&#39;s also valid and probably easier to think of gradient boosting as gradient descent in prediction space. | We saw that iteratively fitting crappy models to the previous model residuals, as we did in the regression GBM from scratch post, is just a special case of fitting crappy models to the negative gradient of the loss function (in this case the mean squared error loss). | Wrapping Up . Phew, there it is, how gradient boosting models do gradient descent in function space. Understanding how the general form of gradient boosting works opens up the possibility for us to use any differentiable loss function for model training. That is pretty exciting because it means that we can get a lot of mileage out of this one class of learning algorithms. Stay tuned for more on some of the awesome things we can do with these ideas in future posts! . There are a couple of resources I found to be super helpful while researching the content in this post. Definitely check them out if you want to read more about gradient boosting and gradient descent. . How to explain gradient boosting by Terence Parr and Jeremy Howard . Understanding Gradient Boosting as Gradient Descent by Nicolas Hug .",
            "url": "https://blog.mattbowers.dev/how-gradient-boosting-does-gradient-descent",
            "relUrl": "/how-gradient-boosting-does-gradient-descent",
            "date": " • Apr 27, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Get Down with Gradient Descent",
            "content": ". Ahh, gradient descent. It&#39;s probably one of the most ubiquitous algorithms used in data science, but you&#39;re unlikely to see it being celebrated in the limelight of the Kaggle podium. Rather than taking center stage, gradient descent operates under the hood, powering the training for a wide range of models including deep neural networks, gradient boosting trees, generalized linear models, and mixed effects models. Getting an intuition for the algorithm will reveal how model fitting actually works and help us to see the common thread connecting a wide range of seemingly unrelated models. In this post we&#39;ll get the intuition for gradient descent with a fresh analogy, develop the mathematical formulation, and ground our understanding by using it to train ourselves a linear regression model. . Intuition . Before we dive into the intuition for gradient descent itself, let’s get a high-level view of why it’s useful in training or fiting a model. Training a model basically means finding the model parameter values that make the model fit a given dataset well. We measure how well a model fits data using a special function variously called a loss or cost or objective function. A loss function takes the dataset and the model as arguments and returns a number that tells us how well our model fits the data. Therefore training is an optimization problem in which we search for the model parameter values that result in the minimum value of the loss function. Enter gradient descent. . Gradient descent is a numerical optimization technique that helps us find the inputs that yield the minimum value of a function. Since most explanations of the gradient descent algorithm seem to use a story about hikers being lost in some foggy mountains, we&#39;re going to try out a new analogy. . Let&#39;s say you&#39;re at a concert. Remember those? They&#39;re these things that used to happen where people played music and everyone danced and had a great time. Now suppose at this concert there&#39;s a dance floor which has become a bit sweltering from copious amounts of &quot;getting down&quot;. But the temperature isn&#39;t quite uniform; maybe there&#39;s a cool spot from a ceiling fan somewhere. . . Let&#39;s get ourselves to that cool spot using the following procedure. . From our current location, figure out which direction feels coolest. | Take a step (or simply shimmy) in that direction. | Repeat steps 1 and 2 until we reach the coolest spot on the dance floor. | The crux of this procedure is figuring out, at each step, which direction yields the greatest temperature reduction. Our skin is pretty sensitive to temperature, so we can just use awareness of body sensation to sense which direction feels coolest. Luckily, we have a mathematical equivalent to our skin&#39;s ability to sense local variation in temperature. . Determine which way to go . Let $f(x,y)$ be the temperature on the dance floor at position $(x,y)$. The direction of fastest decrease in temperature is going to be given by some vector in our $(x,y)$ space, e.g., . [vector component in $x$ direction, vector component in $y$ direction] . Turns out that the gradient of a function evaluated at a particular location yields a vector that points in the direction of fastest increase in the function, pretty similar to what we&#39;re looking for. The gradient of $f(x,y)$ is given by . $$ nabla f(x,y) = left [ frac{ partial f(x,y)}{ partial x}, frac{ partial f(x,y)}{ partial y} right ] $$ . The components of the gradient vector are the partial derivatives of our function $f(x,y)$, evaluated at the point $(x,y)$. These partial derivatives just tell us the slope of $f(x,y)$ in the $x$ and $y$ directions respectively. The intuition is that if $ frac{ partial f(x,y)}{ partial x}$ is a large positive number, then moving in the positive $x$ direction will make $f(x,y)$ increase a lot, whereas if $ frac{ partial f(x,y)}{ partial x}$ is a large negative number, then moving in the negative $x$ direction will make $f(x,y)$ increase a lot. . It&#39;s not too hard to see that the direction of fastest decrease is actually just the exact opposite direction from that of fastest increase. Since we can point a vector in the opposite direction by negating its component values, our direction of fastest temperature decrease will be given by the negative gradient of the temperature field $- nabla f(x,y)$. . . Take a step in the right direction . Now that we have our direction vector, we&#39;re ready to take a step toward the cool part of the dance floor. To do this, we&#39;ll just add our direction vector to our current position. The update rule would look like this. . $$ [x_ text{next}, y_ text{next}] = [x_ text{prev}, y_ text{prev}] - nabla f (x_ text{prev}, y_ text{prev}) = [x_ text{prev}, y_ text{prev}] - left [ frac{ partial f (x_ text{prev}, y_ text{prev})}{ partial x}, frac{ partial f (x_ text{prev}, y_ text{prev})}{ partial y} right ] $$ . If we iteratively apply this update rule, we&#39;ll end up tracing a trajectory through the $(x,y)$ space on the dance floor and we&#39;ll eventually end up at the coolest spot! . . Great success! . General Formulation . Let&#39;s generalize a bit to get to the form of gradient descent you&#39;ll see in references like the wikipedia article. . First we modify our update equation above to handle functions with more than two arguments. We&#39;ll use a bold $ mathbf{x}$ to indicate a vector of inputs $ mathbf{x} = [x_1,x_2, dots,x_p]$. Our function $f( mathbf{x}): mathbb{R}^p mapsto mathbb{R}$ maps a $p$ dimensional input to a scalar output. . Second, instead of displacing our current location with the negative gradient vector itself, we&#39;ll first rescale it with a learning rate parameter. This helps address any issues with units on inputs versus outputs. Imagine the input could range between 0 and 1, but the output ranged from 0 to 1,000. We would need to rescale the partial derivatives so the update step doesn&#39;t send us way too far off in input space. . Finally, we&#39;ll index our updates with $t=0,1, dots$. We&#39;ll run for some prespecified number of iterations or we&#39;ll stop the procedure once the change in $f( mathbf{x})$ is sufficiently small from one iteration to the next. Our update equation will look like this. . $$ mathbf{x}_{t+1} = mathbf{x}_t - eta nabla f ( mathbf{x}_t) $$ . In pseudocode we could write it like this. . # gradient descent x = initial_value_of_x for t in range(n_iterations): # or some other convergence condition x -= learning_rate * gradient_of_f(x) . Now let&#39;s see how this algorithm gets used to train models. . Training a Linear Regression Model with Gradient Descent . To get the intuition for how we use gradient descent to train models, let’s use it to train a linear regression model. Note that we wouldn&#39;t actually use gradient descent to train a linear model in real life since there is an exact analytical solution for the best-fit parameter values. . Anyway, in the simple linear regression problem we have numerical feature $x$ and numerical target $y$, and we want to find a model of the form . $$F(x) = alpha + beta x$$ . This model has two parameters, $ alpha$ and $ beta$. Here &quot;training&quot; means finding the parameter values that make $F(x)$ fit our $y$ data best. We measure how well, or really how poorly, our model fits the data by using a loss function that yields a small value when a model fits well. Ordinary least squares is so named because it uses mean squared error as its loss function. . $$L(y, F(x)) = frac{1}{n} sum_{i=1}^{n} (y_i - F(x_i))^2 = frac{1}{n} sum_{i=1}^{n} (y_i - ( alpha + beta x_i))^2 $$ . The loss function $L$ takes four arguments: $x$, $y$, $ alpha$, and $ beta$. But since $x$ and $y$ are fixed given our dataset, we could write the loss as $L( alpha, beta | x, y)$ to emphasize that $ alpha$ and $ beta$ are the only free parameters. So we&#39;re looking for the following. . $$ underset{ alpha, beta}{ operatorname{argmin}} ~ L( alpha, beta|x,y) $$ . That&#39;s right, we&#39;re looking for the values of $ alpha$ and $ beta$ that minimize scalar-valued function $L( alpha, beta)$. Sounds familiar huh? . To solve this minimization problem with gradient descent, we can use the following update rule. . $$[ alpha_{t+1}, beta_{t+1}] = [ alpha_{t}, beta_{t}] - eta nabla L( alpha_t, beta_t | x, y) $$ . To get the gradient $ nabla L( alpha, beta|x,y)$, we need the partial derivatives of $L$ with respect to $ alpha$ and $ beta$. Since $L$ is just a big sum, it&#39;s easy to calculate the derivatives. . $$ frac{ partial L( alpha, beta)}{ partial alpha} = frac{1}{n} sum_{i=1}^{n} -2 (y_i - ( alpha + beta x_i)) $$ $$ frac{ partial L( alpha, beta)}{ partial beta} = frac{1}{n} sum_{i=1}^{n} -2x_i (y_i - ( alpha + beta x_i)) $$ . Great! We&#39;ve got everything we need to implement gradient descent to train an ordinary least squares model. Everything except data that is. . Toy Data . Let&#39;s make a friendly little linear dataset where $ alpha=-10$ and $ beta=2$, i.e. . $$ y = -10 + 2x + text{noise}$$ . import numpy as np alpha_true = -10 beta_true = 2 rng = np.random.default_rng(42) x = np.linspace(0, 10, 50) y = alpha_true + beta_true*x + rng.normal(0, 1, size=x.shape) . Implementation . Our implementation will use a function to compute the gradient of the loss function. Since we have two parameters, we&#39;ll use length-2 arrays to hold their values and their partial derivatives. At each iteration, we update the parameter values by subtracting the rescaled partial derivatives. . # linear regression using gradient descent def gradient_of_loss(parameters, x, y): alpha = parameters[0] beta = parameters[1] partial_alpha = np.mean(-2*(y - (alpha + beta*x))) partial_beta = np.mean(-2*x*(y - (alpha + beta*x))) return np.array([partial_alpha, partial_beta]) learning_rate = 0.02 parameters = np.array([0.0, 0.0]) # initial values of alpha and beta for _ in range(500): partial_derivatives = gradient_of_loss(parameters, x, y) parameters -= learning_rate * partial_derivatives parameters . array([-10.07049616, 2.03559051]) . We can see the loss function decreasing throughout the 500 iterations. . And we can visualize the loss function as a contour plot over $( alpha, beta)$ space. The blue points show the trajectory our gradient descent followed as it shimmied from the initial position to the coolest spot in $( alpha, beta)$ space where the loss function is nice and small. . Our gradient descent settles in a spot pretty close to $(-10, 2)$ in $( alpha, beta)$ space, which gives us the final fitted model below. . Wrapping Up . There you have it, gradient descent explained with a fresh new analogy having nothing whatsoever to do with foggy mountains, plus an implemented example fitting a linear model. While we often see gradient descent used to train models by performing an optimization in parameter space, as in generalized linear models and neural networks, there are other ways to use this powerful technique to train models. In particular, we&#39;ll soon see how our beloved gradient boosting tree models use gradient descent in prediction space, rather than parameter space. Stay tuned for that mind bender in a future post. .",
            "url": "https://blog.mattbowers.dev/get-down-with-gradient-descent",
            "relUrl": "/get-down-with-gradient-descent",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "How to Build a Gradient Boosting Machine from Scratch",
            "content": ". Ahh, gradient boosting. In addition to having a totally kickass name, this family of machine learning algorithms is currently among the best known approaches for prediction problems on structured data. Like its cousin random forest, gradient boosting is an ensemble technique that generates a single strong model by combining many simple models, usually decision trees. These tree ensemble methods perform very well on tabular data prediction problems and are therefore widely used in industrial applications and machine learning competitions. . There are several noteworthy variants of gradient boosting out there in the wild including XGBoost, NGBoost, LightGBM, and of course the classic gradient boosting machine (GBM). While XGBoost and LightGBM tend to have a marginal performance edge on the classic GBM, they are all based on a similar, very clever, idea about how to ensemble decision trees. Let&#39;s avail ourselves of the intuition behind that clever idea, and then we&#39;ll be able to build our very own GBM from scratch. . Toy Data . We begin our boosting adventure with a deceptively simple toy dataset having one feature $x$ and target $y$. . Notice that $y$ increases with $x$ for a while, then flattens out. This is a pattern that happens all the time in real data, and it&#39;s one that linear models epically fail to capture. Let&#39;s build a gradient boosting machine to model it. . Intuition . Suppose we have a crappy model $F_0(x)$ that uses features $x$ to predict target $y$. A crappy but reasonable choice of $F_0(x)$ would be a model that always predicts the mean of $y$. . $$F_0(x) = bar{y}$$ . That would look like this. . $F_0(x)$ by itself is not a great model, so its residuals $y - F_0(x)$ are still pretty big and they still exhibit meaningful structure that we should try to capture with our model. . Well what if I had another crappy model $h_1(x)$ that could predict the residuals $y - F_0(x)$? . $$ begin{array}{rcl} text{Model: }&amp; &amp; h_1(x) text{Features:}&amp; &amp; x text{Target:}&amp; &amp; y - F_0(x) end{array} $$ It&#39;s worth noting that the crappiness of this new model is essential; in fact in this boosting context, it&#39;s usually called a weak learner. To get a model that&#39;s only slightly better than nothing, let&#39;s use a very simple decision tree with a single split, a.k.a. a stump. This model basically divides our feature $x$ into two regions and predicts the mean value of $y$ for all of the $x$&#39;s in each region. It might look like this. . We could make a composite model by adding the predictions of the base model $F_0(x)$ to the predictions of the supplemental model $h_1(x)$ (which will pick up some of the slack left by $F_0(x)$). We&#39;d get a new model $F_1(x)$: . $$F_1(x) = F_0(x) + h_1(x)$$ . which iss better at predicting $y$ than the original model $F_0(x)$ alone. . Why stop there? Our composite model $F_1(x)$ might still be kind of crappy, and so its residuals $y - F_1(x)$ might still be pretty big and structurey. Let&#39;s add another model $h_2(x)$ to predict those residuals. . $$ begin{array}{rcl} text{Model: }&amp; &amp; h_2(x) text{Features:}&amp; &amp; x text{Target:}&amp; &amp; y - F_1(x) end{array} $$ The new composite model is . $$F_2(x) = F_1(x) + h_2(x).$$ . If we keep doing this, at each stage we&#39;ll train a new model $h_m(x)$ on the previous composite model&#39;s residuals $y-F_{m-1}(x)$, and we&#39;ll get a new composite model . $$F_m(x) = F_{m-1}(x) + h_m(x).$$ . If we add $M$ crappy models constructed in this way to our original crappy model $F_0(x)$, we might actually end up with a pretty good model $F_M(x)$ that looks like . $$ F_M(x) = F_0(x) + sum_{m = 1}^{M} h_m(x) $$Here&#39;s how our model would evolve up to $M=6$. . Voila! That, friends, is boosting! . Learning Rate . Let&#39;s talk about overfitting. In real life, if we just add our new weak learner $h_m(x)$ directly to our existing composite model $F_{m-1}(x)$, then we&#39;re likely to end up overfitting on our training data. That&#39;s because if we add enough of these weak learners, they&#39;re going to chase down y so closely that all the remaining residuals are pretty much zero, and we will have successfully memorized the training data. To prevent that, we&#39;ll scale them down a bit by a parameter $ eta$ called the learning rate. . With the learning rate $ eta$, the update step will then look like . $$F_{m}(x) = F_{m-1}(x) + eta h_m(x),$$ . and our composite model will look like . $$F_M(x) = F_0(x) + eta sum_{m = 1}^{M} h_m(x)$$ . Note that since the learning rate can be factored out of the sum, it looks kinda like we could just build our models without it and slap it on at the end when we sum up the weak learners to make the final composite model. But that won&#39;t work, since at each stage we train the next weak learner on the residuals from the current composite model, and the current composite model depends on the learning rate. . Implementation . Ok, we&#39;re ready to implement this thing from &quot;scratch&quot;. Well, sort of. To quote Carl Sagan, . If you wish to make an apple pie from scratch, you must first invent the universe. . We will not be inventing a universe that contains the Earth, apple trees, computers, python, numpy, and sklearn. To keep the &quot;scratch&quot; implementation clean, we&#39;ll allow ourselves the luxury of numpy and an off-the-shelf sklearn decision tree which we&#39;ll use as our weak learner. . from sklearn.tree import DecisionTreeRegressor # model hyperparameters learning_rate = 0.3 n_trees = 10 max_depth = 1 # Training F0 = y.mean() Fm = F0 trees = [] for _ in range(n_trees): tree = DecisionTreeRegressor(max_depth=max_depth) tree.fit(x, y - Fm) Fm += learning_rate * tree.predict(x) trees.append(tree) # Prediction y_hat = F0 + learning_rate * np.sum([t.predict(x) for t in trees], axis=0) . We first define our hyperparameters: . learning_rate is ($ eta$) | n_trees is the number of weak learner trees to add ($M$) | max_depth controls the depth of the trees; here we set to 1 for stumps | . We define our base model predictions F0 to simply predict the mean value of y. Fm corresponds to the current composite model $F_m(x)$ as we iteratively add weak learners, so we&#39;ll initialize it with F0. trees is an empty list that we&#39;ll use to hold our weak learners. . Next we iteratively add n_trees weak learners to our composite model. At each iteration, we create a new decision tree and train it on x to predict the current residuals y - Fm. We update Fm with the newly trained learner&#39;s predictions scaled by the learning rate, and we append the new weak learner $h_m(x)$ in the trees list. We generate final predictions y_hat on the training data by summing up the predictions from each weak learner, scaling by the learning rate, and adding to the base model (a.k.a. the mean of y). . Nice! Our GBM fits that nonlinear data pretty well. . Now that we have a working implementation, let&#39;s go ahead and implement it as a class with fit and predict methods like we&#39;re useed to having in sklearn. . class GradientBoostingFromScratch(): def __init__(self, n_trees, learning_rate, max_depth=1): self.n_trees=n_trees; self.learning_rate=learning_rate; self.max_depth=max_depth; def fit(self, x, y): self.trees = [] self.F0 = y.mean() Fm = self.F0 for _ in range(self.n_trees): tree = DecisionTreeRegressor(max_depth=self.max_depth) tree.fit(x, y - Fm) Fm += self.learning_rate * tree.predict(x) self.trees.append(tree) def predict(self, x): return self.F0 + self.learning_rate * np.sum([tree.predict(x) for tree in self.trees], axis=0) . Let&#39;s compare the performance of our implementation with the sklearn GradientBoostingRegressor. . from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error sklearn_gbm = GradientBoostingRegressor(n_estimators =25, learning_rate=0.3, max_depth=1) sklearn_gbm.fit(x,y) scratch_gbm = GradientBoostingFromScratch(n_trees=25, learning_rate=0.3, max_depth=1) scratch_gbm.fit(x,y) mean_squared_error(y, sklearn_gbm.predict(x)), mean_squared_error(y, scratch_gbm.predict(x)) . (0.10740933643265559, 0.10740933643265561) . Heck yeah! Our homemade GBM is consistent with the sklearn implementation! . Wrapping Up . Alright, there you have it, the intuition behind basic gradient boosting and a from scratch implementation of the gradient boosting machine. I tried to keep this explanation as simple as possible while giving a complete intuition for the basic GBM. But it turns out that the rabbit hole goes pretty deep on these gradient boosting algorithms. We can actually wave our magic generalization wand over some custom loss functions and end up with algorithms that can do gradient descent in function space (whatever that means). We&#39;ll get into what that means and why it&#39;s so baller in future posts. For now, go forth and boost! .",
            "url": "https://blog.mattbowers.dev/gradient-boosting-machine-from-scratch",
            "relUrl": "/gradient-boosting-machine-from-scratch",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "The 80/20 Pandas Tutorial",
            "content": ". Ahh, pandas. In addition to being everyone&#39;s favorite mostly vegetarian bear from south central China, it&#39;s also the python library for working with tabular data, a.k.a. dataframes. When you dive into pandas, you&#39;ll quickly find out that there is a lot going on; indeed there are hundreds of methods for operating on dataframes. But luckily for us, as with many areas of life, there is a Pareto Principle, or 80/20 rule, that will help us focus on the small set of methods that collectively solve the majority of our data transformation needs. . If you&#39;re like me, then pandas is not your first data-handling tool; maybe you&#39;ve been using SQL or R with data.table or dplyr. If so, that&#39;s great because you already have a sense for the key operations we need when working with tabular data. In their book, R for Data Science, Garrett Grolemund and Hadley Wickham describe five essential operations for manipulating dataframes. I&#39;ve found that these cover the majority of my data transformation tasks to prepare data for analysis, visualization, and modeling. . filtering rows based on data values | sorting rows based on data values | selecting columns by name | adding new columns based on the existing columns | creating grouped summaries of the dataset | I would add that we also need a way to build up more complex transformations by chaining these fundamental operations together sequentially. . Before we dive in, here&#39;s the TLDR on the pandas methods that I prefer for accomplishing these tasks, along with their equivalents from SQL and dplyr in R. . description pandas SQL dplyr . filter rows based on data values | query() | WHERE | filter() | . sort rows based on data values | sort_values() | ORDER BY | arrange() | . select columns by name | filter() | SELECT | select() | . add new columns based on the existing columns | assign() | AS | mutate() | . create grouped summaries of the dataset | groupby() apply() | GROUP BY | group_by() summarise() | . chain operations together | . | | %&gt;% | . Imports and Data . import pandas as pd import numpy as np . We&#39;ll use the nycflights13 dataset which contains data on the 336,776 flights that departed from New York City in 2013. . flights = pd.read_csv(&#39;https://www.openintro.org/book/statdata/nycflights.csv&#39;) flights.head() . year month day dep_time dep_delay arr_time arr_delay carrier tailnum flight origin dest air_time distance hour minute . 0 2013 | 6 | 30 | 940 | 15 | 1216 | -4 | VX | N626VA | 407 | JFK | LAX | 313 | 2475 | 9 | 40 | . 1 2013 | 5 | 7 | 1657 | -3 | 2104 | 10 | DL | N3760C | 329 | JFK | SJU | 216 | 1598 | 16 | 57 | . 2 2013 | 12 | 8 | 859 | -1 | 1238 | 11 | DL | N712TW | 422 | JFK | LAX | 376 | 2475 | 8 | 59 | . 3 2013 | 5 | 14 | 1841 | -4 | 2122 | -34 | DL | N914DL | 2391 | JFK | TPA | 135 | 1005 | 18 | 41 | . 4 2013 | 7 | 21 | 1102 | -3 | 1230 | -8 | 9E | N823AY | 3652 | LGA | ORF | 50 | 296 | 11 | 2 | . Select rows based on their values with query() . query() lets you retain a subset of rows based on the values of the data; it&#39;s like dplyr::filter() in R or WHERE in SQL. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using and and or, and you can even check if the column value matches any items in a list. . # compare one column to a value flights.query(&#39;month == 6&#39;) # compare two column values flights.query(&#39;arr_delay &gt; dep_delay&#39;) # using arithmetic flights.query(&#39;arr_delay &gt; 0.5 * air_time&#39;) # using &quot;and&quot; flights.query(&#39;month == 6 and day == 1&#39;) # using &quot;or&quot; flights.query(&#39;origin == &quot;JFK&quot; or dest == &quot;JFK&quot;&#39;) # column value matching any item in a list flights.query(&#39;carrier in [&quot;AA&quot;, &quot;UA&quot;]&#39;) . You may have noticed that it seems to be much more popular to filter pandas data frames using boolean indexing. Indeed when I ask my favorite search engine how to filter a pandas dataframe on its values, I find this tutorial, this blog post, various questions on Stack Overflow, and even the pandas documentation, all espousing boolean indexing. Here&#39;s what it looks like. . # canonical boolean indexing flights[(flights[&#39;carrier&#39;] == &quot;AA&quot;) &amp; (flights[&#39;origin&#39;] == &quot;JFK&quot;)] # the equivalent use of query() flights.query(&#39;carrier == &quot;AA&quot; and origin == &quot;JFK&quot;&#39;) . There are a few reasons I prefer query() over boolean indexing. . query() does not require me to type the dataframe name again, whereas boolean indexing requires me to type it every time I wish to refer to a column. | query() makes the code easier to read and understand, especially when expressions get complex. | query() is more computationally efficient than boolean indexing. | query() can safely be used in dot chains, which we&#39;ll see very soon. | Select columns by name with filter() . filter() lets you pick out a specific set of columns by name; it&#39;s analogous to dplyr::select() in R or SELECT in SQL. You can either provide exactly the column names you want, or you can grab all columns whose names contain a given substring or which match a given regular expression. This isn&#39;t a big deal when your dataframe has only a few columns, but is particularly useful when you have a dataframe with tens or hundreds of columns. . # select a list of columns flights.filter([&#39;origin&#39;, &#39;dest&#39;]) # select columns containing a particular substring flights.filter(like=&#39;time&#39;) # select columns matching a regular expression flights.filter(regex=&#39;e$&#39;) . Sort rows with sort_values() . sort_values() changes the order of the rows based on the data values; it&#39;s likedplyr::arrange() in R or ORDER BY in SQL. You can specify one or more columns on which to sort, where their order denotes the sorting priority. You can also specify whether to sort in ascending or descending order. . # sort by a single column flights.sort_values(&#39;air_time&#39;) # sort by a single column in descending order flights.sort_values(&#39;air_time&#39;, ascending=False) # sort by carrier, then within carrier, sort by descending distance flights.sort_values([&#39;carrier&#39;, &#39;distance&#39;], ascending=[True, False]) . Add new columns with assign() . assign() adds new columns which can be functions of the existing columns; it&#39;s like dplyr::mutate() from R. . # add a new column based on other columns flights.assign(speed = lambda x: x.distance / x.air_time) # another new column based on existing columns flights.assign(gain = lambda x: x.dep_delay - x.arr_delay) . If you&#39;re like me, this way of using assign() might seem a little strange at first. Let&#39;s break it down. In the call to assign() the keyword argument speed tells pandas the name of our new column. The business to the right of the = is a inline lambda function that takes the dataframe we passed to assign() and returns the column we want to add. . I like using x as the lambda argument because its easy to type and it evokes tabular data (think design matrix), which reminds me that it refers to the entire dataframe. We can then access the other columns in our dataframe using the dot like x.other_column. . It&#39;s true that you can skip the whole lambda business and refer to the dataframe to which you are assigning directly inside the assign. That might look like this. . flights.assign(speed = flights.distance / flights.air_time) . I prefer using a lambda for the following reasons. . If you gave your dataframe a good name, using the lambda will save you from typing the name every time you want to refer to a column. | The lambda makes your code more portable. Since you refer to the dataframe as a generic x, you can reuse this same assignment code on a dataframe with a different name. | Most importantly, the lambda will allow you to harness the power of dot chaining. | Chain transformations together with the dot chain . One of the awesome things about pandas is that the object.method() paradigm lets us easily build up complex dataframe transformations from a sequence of method calls. In R, this is effectively accomplished by the pipe %&gt;% operator. For example, suppose we want to look at high-speed flights from JFK to Honolulu, which would require us to query for JFK to Honolulu flights, assign a speed column, and maybe sort on that new speed column. . We can say: . # neatly chain method calls together ( flights .query(&#39;origin == &quot;JFK&quot;&#39;) .query(&#39;dest == &quot;HNL&quot;&#39;) .assign(speed = lambda x: x.distance / x.air_time) .sort_values(by=&#39;speed&#39;, ascending=False) .query(&#39;speed &gt; 8.0&#39;) ) . We compose the dot chain by wrapping the entire expression in parentheses and indenting each line within. The first line is the name of the dataframe on which we are operating. Each subsequent line has a single method call. . There are a few great things about writing the code this way: . Readability. It&#39;s easy to scan down the left margin of the code to see what&#39;s happening. The first line gives us our noun (the dataframe) and each subsequent line starts with a verb. You could read this as &quot;take flights then query the rows where origin is JFK, then query for rows where destination is HNL, then assign a new column called speed, then sort the dataframe by speed, then query only for the rows where speed is greater than 8.0. | Flexibility - It&#39;s easy to comment out individual lines and re-run the cell. It&#39;s also easy to reorder operations, since only one thing happens on each line. | Neatness - We have not polluted our workspace with any intermediate variables, nor have we wasted any mental energy thinking of names for any temporary variables. | By default, dot chains do not modify the original dataframe; they just output a temporary result that we can inspect directly in the output. If you want to store the result, or pass it along to another function (e.g. for plotting), you can simply assign the entire dot chain to a variable. . # sotre the output of the dot chain in a new dataframe flights_high_speed = ( flights .assign(speed = lambda x: x.distance / x.air_time) .query(&#39;speed &gt; 8.0&#39;) ) . Collapsing rows into grouped summaries with groupby() . groupby() combined with apply() gives us flexibility and control over our grouped summaries; it&#39;s like dplyr::group_by() and dplyr::summarise() in R. This is the primary pattern I use for SQL-style groupby operations in pandas. Specifically it unlocks the following essential functionality you&#39;re used to having in SQL. . specify the names of the aggregation columns we create | specify which aggregation function to use on which columns | compose more complex aggregations such as the proportion of rows meeting some condition | aggregate over arbitrary functions of multiple columns | Let&#39;s check out the departure delay stats for each carrier. . ( flights .groupby([&#39;carrier&#39;]) .apply(lambda d: pd.Series({ &#39;n_flights&#39;: len(d), &#39;med_delay&#39;: d.dep_delay.median(), &#39;avg_delay&#39;: d.dep_delay.mean(), })) .head() ) . n_flights med_delay avg_delay . carrier . 9E 1696.0 | -1.0 | 17.285967 | . AA 3188.0 | -2.0 | 9.142409 | . AS 66.0 | -4.5 | 5.181818 | . B6 5376.0 | -1.0 | 13.137091 | . DL 4751.0 | -2.0 | 8.529573 | . While you might be used to apply() acting over the rows or columns of a dataframe, here we&#39;re calling apply on a grouped dataframe object, so it&#39;s acting over the groups. According to the pandas documentation: . The function passed to apply must take a dataframe as its first argument and return a dataframe, a series or a scalar. apply will then take care of combining the results back together into a single dataframe or series. apply is therefore a highly flexible grouping method. . We need to supply apply() with a function that takes each chunk of the grouped dataframe and returns (in our case) a series object with one element for each new aggregation column. Notice that I use a lambda to specify the function we pass to apply()), and that I name its argument d, which reminds me that it&#39;s a dataframe. My lambda returns a pandas series whose index entries specify the new aggregation column names, and whose values constitute the results of the aggregations for each group. Pandas will then stitch everything back together into a lovely dataframe. . Notice how nice the code looks when we use this pattern. Each aggregation is specified on its own line, which makes it easy to see what aggregation columns we&#39;re creating and allows us to comment, uncomment, and reorder the aggregations without breaking anything. . Here are some more complex aggregations to illustrate some useful patterns. . ( flights .groupby([&#39;carrier&#39;]) .apply(lambda d: pd.Series({ &#39;avg_gain&#39;: np.mean(d.dep_delay - d.arr_delay), &#39;pct_delay_gt_30&#39;: np.mean(d.dep_delay &gt; 30), &#39;pct_late_dep_early_arr&#39;: np.mean((d.dep_delay &gt; 0) &amp; (d.arr_delay &lt; 0)), &#39;avg_arr_given_dep_delay_gt_0&#39;: d.query(&#39;dep_delay &gt; 0&#39;).arr_delay.mean(), &#39;cor_arr_delay_dep_delay&#39;: np.corrcoef(d.dep_delay, d.arr_delay)[0,1], })) .head() ) . avg_gain pct_delay_gt_30 pct_late_dep_early_arr avg_arr_given_dep_delay_gt_0 cor_arr_delay_dep_delay . carrier . 9E 9.247642 | 0.196934 | 0.110259 | 39.086111 | 0.932485 | . AA 7.743726 | 0.113237 | 0.105395 | 30.087165 | 0.891013 | . AS 16.515152 | 0.106061 | 0.121212 | 28.058824 | 0.864565 | . B6 3.411458 | 0.160528 | 0.084449 | 37.306866 | 0.914180 | . DL 7.622816 | 0.097874 | 0.100821 | 30.078029 | 0.899327 | . Here&#39;s what&#39;s happening. . np.mean(d.dep_delay - d.arr_delay) aggregates over the difference of two columns. | np.mean(d.dep_delay &gt; 30) computes the proportion of rows where the delay is greater than 30 minutes. Generating a boolean series based on some condition and then using mean() to find the proportion comes up all the time. | np.mean((d.dep_delay &gt; 0) &amp; (d.arr_delay &lt; 0)) shows that we can compute proportions where conditions on multiple columns are met. | d.query(&#39;dep_delay &gt; 0&#39;).arr_delay.mean() computes the average arrival delay on flights where the departure was delayed. Here we first filter each grouped dataframe down to the subset of rows where departure delay is greater than zero using query(), and then we take the mean of the remaining arrival delays. | np.corrcoef(d.dep_delay, d.arr_delay)[0,1] computes the correlation coefficient between departure and arrival delays. Remember we can use pretty much any reduction operation to collapse values down to a scalar. | . You might have noticed that the canonical pandas approach for grouped summaries is to use agg(). That works well if you need to apply the same aggregation function on each column in the dataframe, e.g. taking the mean of every column. But because of the kind of data I work with these days, it&#39;s much more common for me to use customized aggregations like those above, so the groupby() apply() idiom works best for me. . Wrapping Up . There you have it, how to pull off the five most essential data transformation tasks using pandas in a style reminiscent of my beloved dplyr. Remember that part of the beauty of pandas is that since there are so many ways to do most tasks, you can develop your own style based on the kind of data you work with, what you like about other tools, how you see others using the tools, and of course your own taste and preferences. . If you found this post helpful or if you have your own preferred style for accomplishing any of these key transformations with pandas, do let me know about it in the comments. .",
            "url": "https://blog.mattbowers.dev/8020-pandas-tutorial",
            "relUrl": "/8020-pandas-tutorial",
            "date": " • Nov 25, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Hello World! And Why I'm Inspired to Start a Blog",
            "content": ". Well, I’ve been thinking about getting this blog started for months now. I guess a combination of inertia, up-front investment in blogging platform selection/setup, and spending a little too much time writing and rewriting the first content post has drawn out the period from initial inspiration to making the blog a reality. Needless to say, I’m pretty excited to finally get things going. . Before we dive headlong into the weeds of ML algorithms, statistical methods, and whatever I happen to be learning and teaching at the moment, I figured it would be good to articulate why I’ve felt inspired to get started blogging in the first place. Hopefully this will serve the dual purpose of clarifying my intentions and introducing a vastly underappreciated concept in data science that I hope to weave through the posts to come. . Learning . The initial inception about blogging probably originated from some comments about learning that Jeremy Howard makes in the Practical Deep Learning course from fastai. During one of the lectures, he mentions that it’s a great idea to start blogging. To paraphrase Jeremy: . The thing I really love about blogging is that it helps you learn; by writing things down, you synthesize your ideas. . Beautiful. That definitely rings true for me. I tend to take notes and play around with code when learning new concepts anyway. One of my key hypotheses about this blogging experiment is that making the effort to transform those notes into blog posts will help me learn more effectively. . Teaching . Ah, teaching. Yes, sometimes it’s that thing that takes time away from your research, forcing you to sit alone in a windowless room squinting at hand-written math on a fat stack of homework assignments. But sometimes it actually involves interacting with students, endeavoring to explain a concept, and watching them light up when they get it. The latter manifestation of teaching was one of my favorite things about grad school and academia in general. While I certainly still get to do some teaching as an industry data scientist, I could see myself returning to a more teaching-centric gig somewhere off in the future. Thus we have our second key hypothesis about the blogging experiment, that the writing will entertain my inclination to teach. . Contributing . Working in the field of data science today is a bit like standing in front of a massive complimentary all-you-can-learn buffet. There is an abundance of free material out on the interwebs for learning pretty much anything in data science from hello world python tutorials to research papers on cutting-edge deep learning techniques. I’ve personally benefited from many a blog post that helped me unpack a new concept or get started using a new tool. And let’s not forget the gigantic cyber warehouse full of freely available open source software tools that volunteer developers have straight-up donated to humanity. . I realize that up to now, I’ve simply been consuming all of this free goodness without giving anything substantive back in return. Well then, it’s time to start evening the score. Which brings us to key hypothesis number three, that through these blog posts, I might be able to create something helpful, thereby being of service to a community that has freely given so much to me. . Live Long and Prosper, Blog . Phew, there it is, the original source of inspiration for this blogging experiment, and three reasons I think it might be a good idea. The astute reader will have noticed that these three assertions have been formulated as hypotheses which are to be tested in the laboratory of experience. And thus, we also have our first glimpse of the scientific method, an underrated concept that is going to help us put the science back in data science. . With that, blog, I christen thee, Random Realizations. .",
            "url": "https://blog.mattbowers.dev/hello-world",
            "relUrl": "/hello-world",
            "date": " • Nov 22, 2020"
        }
        
    
  

  
  
      ,"page0": {
          "title": "About",
          "content": ". Welcome to Random Realizations! This blog is a celebration of the fascinating world of data science. I hope you find the content useful, and I hope you enjoy reading along and learning with me! . And me? Well, I’m Matt Bowers. I’m a data scientist at Uber where I solve problems using statistical modeling and machine learning. Over the years I’ve driven product decisions through controlled experimentation and user analytics, built ML products like ETA prediction on large-scale telematics data, and cracked long-standing ML problems in marketplace pricing. Before Uber I earned an MS in Applied Statistics and a PhD in Atmospheric Sciences at Purdue University where I studied the spatial-temporal correlation structure of tropical convection systems and developed statistical methods for climate research. . But enough about me. Let’s get down with some data science! .",
          "url": "https://blog.mattbowers.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "Subscribe",
          "content": "Sign up below to receive an email whenever I publish a new post. If you’re hard core and have an RSS reader, you can subscribe to the feed down in the footer. Or you can follow me on Linkedin or Twitter where I also share new posts. . * indicates required Email Address * First Name Last Name .",
          "url": "https://blog.mattbowers.dev/_pages/1-subscribe",
          "relUrl": "/_pages/1-subscribe",
          "date": ""
      }
      
  

  

  

  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.mattbowers.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}