{
  
    
        "post0": {
            "title": "Get Down with Gradient Descent",
            "content": ". Ahh, gradient descent. It&#39;s probably one of the most ubiquitous algorithms used in data science, but you&#39;re unlikely to see it being celebrated in the limelight of the Kaggle podium. Rather than taking center stage, gradient descent operates under the hood, powering the training for a wide range of models including deep neural networks, gradient boosting trees, generalized linear models, and mixed effects models. Getting an intuition for the algorithm will reveal how model fitting actually works and help us to see the common thread connecting a wide range of seemingly unrelated models. In this post we&#39;ll get the intuition for gradient descent with a fresh analogy, develop the mathematical formulation, and ground our understanding by using it to train ourselves a linear regression model. . Intuition . Before we dive into the intuition for gradient descent itself, let’s get a high-level view of why it’s useful in training or fiting a model. Training a model basically means finding the model parameter values that make the model fit a given dataset well. We measure how well a model fits data using a special function variously called a loss or cost or objective function. A loss function takes the dataset and the model as arguments and returns a number that tells us how well our model fits the data. Therefore training is an optimization problem in which we search for the model parameter values that result in the minimum value of the loss function. Enter gradient descent. . Gradient descent is a numerical optimization technique that helps us find the inputs that yield the minimum value of a function. Since most explanations of the gradient descent algorithm seem to use a story about hikers being lost in some foggy mountains, we&#39;re going to try out a new analogy. . Let&#39;s say you&#39;re at a concert. Remember those? They&#39;re these things that used to happen where people played music and everyone danced and had a great time. Now suppose at this concert there&#39;s a dance floor which has become a bit sweltering from copious amounts of &quot;getting down&quot;. But the temperature isn&#39;t quite uniform; maybe there&#39;s a cool spot from a ceiling fan somewhere. . . Let&#39;s get ourselves to that cool spot using the following procedure. . From our current location, figure out which direction feels coolest. | Take a step (or simply shimmy) in that direction. | Repeat steps 1 and 2 until we reach the coolest spot on the dance floor. | The crux of this procedure is figuring out, at each step, which direction yields the greatest temperature reduction. Our skin is pretty sensitive to temperature, so we can just use awareness of body sensation to sense which direction feels coolest. Luckily, we have a mathematical equivalent to our skin&#39;s ability to sense local variation in temperature. . Determine which way to go . Let $f(x,y)$ be the temperature on the dance floor at position $(x,y)$. The direction of fastest decrease in temperature is going to be given by some vector in our $(x,y)$ space, e.g., . [vector component in $x$ direction, vector component in $y$ direction] . Turns out that the gradient of a function evaluated at a particular location yields a vector that points in the direction of fastest increase in the function, pretty similar to what we&#39;re looking for. The gradient of $f(x,y)$ is given by . $$ nabla f(x,y) = left [ frac{ partial f(x,y)}{ partial x}, frac{ partial f(x,y)}{ partial y} right ] $$ . The components of the gradient vector are the partial derivatives of our function $f(x,y)$, evaluated at the point $(x,y)$. These partial derivatives just tell us the slope of $f(x,y)$ in the $x$ and $y$ directions respectively. The intuition is that if $ frac{ partial f(x,y)}{ partial x}$ is a large positive number, then moving in the positive $x$ direction will make $f(x,y)$ increase a lot, whereas if $ frac{ partial f(x,y)}{ partial x}$ is a large negative number, then moving in the negative $x$ direction will make $f(x,y)$ increase a lot. . It&#39;s not too hard to see that the direction of fastest decrease is actually just the exact opposite direction from that of fastest increase. Since we can point a vector in the opposite direction by negating its component values, our direction of fastest temperature decrease will be given by the negative gradient of the temperature field $- nabla f(x,y)$. . . Take a step in the right direction . Now that we have our direction vector, we&#39;re ready to take a step toward the cool part of the dance floor. To do this, we&#39;ll just add our direction vector to our current position. The update rule would look like this. . $$ [x_ text{next}, y_ text{next}] = [x_ text{prev}, y_ text{prev}] - nabla f (x_ text{prev}, y_ text{prev}) = [x_ text{prev}, y_ text{prev}] - left [ frac{ partial f (x_ text{prev}, y_ text{prev})}{ partial x}, frac{ partial f (x_ text{prev}, y_ text{prev})}{ partial y} right ] $$ . If we iteratively apply this update rule, we&#39;ll end up tracing a trajectory through the $(x,y)$ space on the dance floor and we&#39;ll eventually end up at the coolest spot! . . Great success! . General Formulation . Let&#39;s generalize a bit to get to the form of gradient descent you&#39;ll see in references like the wikipedia article. . First we modify our update equation above to handle functions with more than two arguments. We&#39;ll use a bold $ mathbf{x}$ to indicate a vector of inputs $ mathbf{x} = [x_1,x_2, dots,x_p]$. Our function $f( mathbf{x}): mathbb{R}^p mapsto mathbb{R}$ maps a $p$ dimensional input to a scalar output. . Second, instead of displacing our current location with the negative gradient vector itself, we&#39;ll first rescale it with a learning rate parameter. This helps address any issues with units on inputs versus outputs. Imagine the input could range between 0 and 1, but the output ranged from 0 to 1,000. We would need to rescale the partial derivatives so the update step doesn&#39;t send us way too far off in input space. . Finally, we&#39;ll index our updates with $t=0,1, dots$. We&#39;ll run for some prespecified number of iterations or we&#39;ll stop the procedure once the change in $f( mathbf{x})$ is sufficiently small from one iteration to the next. Our update equation will look like this. . $$ mathbf{x}_{t+1} = mathbf{x}_t - eta nabla f ( mathbf{x}_t) $$ . In pseudocode we could write it like this. . # gradient descent x = initial_value_of_x for t in range(n_iterations): # or some other convergence condition x -= learning_rate * gradient_of_f(x) . Now let&#39;s see how this algorithm gets used to train models. . Training a Linear Regression Model with Gradient Descent . To get the intuition for how we use gradient descent to train models, let’s use it to train a linear regression model. Note that we wouldn&#39;t actually use gradient descent to train a linear model in real life since there is an exact analytical solution for the best-fit parameter values. . Anyway, in the simple linear regression problem we have numerical feature $x$ and numerical target $y$, and we want to find a model of the form . $$F(x) = alpha + beta x$$ . This model has two parameters, $ alpha$ and $ beta$. Here &quot;training&quot; means finding the parameter values that make $F(x)$ fit our $y$ data best. We measure how well, or really how poorly, our model fits the data by using a loss function that yields a small value when a model fits well. Ordinary least squares is so named because it uses mean squared error as its loss function. . $$L(y, F(x)) = frac{1}{n} sum_{i=1}^{n} (y_i - F(x_i))^2 = frac{1}{n} sum_{i=1}^{n} (y_i - ( alpha + beta x_i))^2 $$ . The loss function $L$ takes four arguments: $x$, $y$, $ alpha$, and $ beta$. But since $x$ and $y$ are fixed given our dataset, we could write the loss as $L( alpha, beta | x, y)$ to emphasize that $ alpha$ and $ beta$ are the only free parameters. So we&#39;re looking for the following. . $$ underset{ alpha, beta}{ operatorname{argmin}} ~ L( alpha, beta|x,y) $$ . That&#39;s right, we&#39;re looking for the values of $ alpha$ and $ beta$ that minimize scalar-valued function $L( alpha, beta)$. Sounds familiar huh? . To solve this minimization problem with gradient descent, we can use the following update rule. . $$[ alpha_{t+1}, beta_{t+1}] = [ alpha_{t}, beta_{t}] - eta nabla L( alpha_t, beta_t | x, y) $$ . To get the gradient $ nabla L( alpha, beta|x,y)$, we need the partial derivatives of $L$ with respect to $ alpha$ and $ beta$. Since $L$ is just a big sum, it&#39;s easy to calculate the derivatives. . $$ frac{ partial L( alpha, beta)}{ partial alpha} = frac{1}{n} sum_{i=1}^{n} -2 (y_i - ( alpha + beta x_i)) $$ $$ frac{ partial L( alpha, beta)}{ partial beta} = frac{1}{n} sum_{i=1}^{n} -2x_i (y_i - ( alpha + beta x_i)) $$ . Great! We&#39;ve got everything we need to implement gradient descent to train an ordinary least squares model. Everything except data that is. . Toy Data . Let&#39;s make a friendly little linear dataset where $ alpha=-10$ and $ beta=2$, i.e. . $$ y = -10 + 2x + text{noise}$$ . import numpy as np alpha_true = -10 beta_true = 2 rng = np.random.default_rng(42) x = np.linspace(0, 10, 50) y = alpha_true + beta_true*x + rng.normal(0, 1, size=x.shape) . Implementation . Our implementation will use a function to compute the gradient of the loss function. Since we have two parameters, we&#39;ll use length-2 arrays to hold their values and their partial derivatives. At each iteration, we update the parameter values by subtracting the rescaled partial derivatives. . # linear regression using gradient descent def gradient_of_loss(parameters, x, y): alpha = parameters[0] beta = parameters[1] partial_alpha = np.mean(-2*(y - (alpha + beta*x))) partial_beta = np.mean(-2*x*(y - (alpha + beta*x))) return np.array([partial_alpha, partial_beta]) learning_rate = 0.02 parameters = np.array([0.0, 0.0]) # initial values of alpha and beta for _ in range(500): partial_derivatives = gradient_of_loss(parameters, x, y) parameters -= learning_rate * partial_derivatives parameters . array([-10.07049616, 2.03559051]) . We can see the loss function decreasing throughout the 500 iterations. . And we can visualize the loss function as a contour plot over $( alpha, beta)$ space. The blue points show the trajectory our gradient descent followed as it shimmied from the initial position to the coolest spot in $( alpha, beta)$ space where the loss function is nice and small. . Our gradient descent settles in a spot pretty close to $(-10, 2)$ in $( alpha, beta)$ space, which gives us the final fitted model below. . Wrapping Up . There you have it, gradient descent explained with a fresh new analogy having nothing whatsoever to do with foggy mountains, plus an implemented example fitting a linear model. While we often see gradient descent used to train models by performing an optimization in parameter space, as in generalized linear models and neural networks, there are other ways to use this powerful technique to train models. In particular, we&#39;ll soon see how our beloved gradient boosting tree models use gradient descent in prediction space, rather than parameter space. Stay tuned for that mind bender in a future post. .",
            "url": "https://blog.mattbowers.dev/get-down-with-gradient-descent",
            "relUrl": "/get-down-with-gradient-descent",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How to Build a Gradient Boosting Machine from Scratch",
            "content": ". Ahh, gradient boosting. In addition to having a totally kickass name, this family of machine learning algorithms is currently among the best known approaches for prediction problems on structured data. Like its cousin random forest, gradient boosting is an ensemble technique that generates a single strong model by combining many simple models, usually decision trees. These tree ensemble methods perform very well on tabular data prediction problems and are therefore widely used in industrial applications and machine learning competitions. . There are several noteworthy variants of gradient boosting out there in the wild including XGBoost, NGBoost, LightGBM, and of course the classic gradient boosting machine (GBM). While XGBoost and LightGBM tend to have a marginal performance edge on the classic GBM, they are all based on a similar, very clever, idea about how to ensemble decision trees. Let&#39;s avail ourselves of the intuition behind that clever idea, and then we&#39;ll be able to build our very own GBM from scratch. . Toy Data . We begin our boosting adventure with a deceptively simple toy dataset having one feature $x$ and target $y$. . Notice that $y$ increases with $x$ for a while, then flattens out. This is a pattern that happens all the time in real data, and it&#39;s one that linear models epically fail to capture. Let&#39;s build a gradient boosting machine to model it. . Intuition . Suppose we have a crappy model $F_0(x)$ that uses features $x$ to predict target $y$. A crappy but reasonable choice of $F_0(x)$ would be a model that always predicts the mean of $y$. . $$F_0(x) = bar{y}$$ . That would look like this. . $F_0(x)$ by itself is not a great model, so its residuals $y - F_0(x)$ are still pretty big and they still exhibit meaningful structure that we should try to capture with our model. . Well what if I had another crappy model $h_1(x)$ that could predict the residuals $y - F_0(x)$? . $$ begin{array}{rcl} text{Model: }&amp; &amp; h_1(x) text{Features:}&amp; &amp; x text{Target:}&amp; &amp; y - F_0(x) end{array} $$ It&#39;s worth noting that the crappiness of this new model is essential; in fact in this boosting context, it&#39;s usually called a weak learner. To get a model that&#39;s only slightly better than nothing, let&#39;s use a very simple decision tree with a single split, a.k.a. a stump. This model basically divides our feature $x$ into two regions and predicts the mean value of $y$ for all of the $x$&#39;s in each region. It might look like this. . We could make a composite model by adding the predictions of the base model $F_0(x)$ to the predictions of the supplemental model $h_1(x)$ (which will pick up some of the slack left by $F_0(x)$). We&#39;d get a new model $F_1(x)$: . $$F_1(x) = F_0(x) + h_1(x)$$ . which iss better at predicting $y$ than the original model $F_0(x)$ alone. . Why stop there? Our composite model $F_1(x)$ might still be kind of crappy, and so its residuals $y - F_1(x)$ might still be pretty big and structurey. Let&#39;s add another model $h_2(x)$ to predict those residuals. . $$ begin{array}{rcl} text{Model: }&amp; &amp; h_2(x) text{Features:}&amp; &amp; x text{Target:}&amp; &amp; y - F_1(x) end{array} $$ The new composite model is . $$F_2(x) = F_1(x) + h_2(x).$$ . If we keep doing this, at each stage we&#39;ll train a new model $h_m(x)$ on the previous composite model&#39;s residuals $y-F_{m-1}(x)$, and we&#39;ll get a new composite model . $$F_m(x) = F_{m-1}(x) + h_m(x).$$ . If we add $M$ crappy models constructed in this way to our original crappy model $F_0(x)$, we might actually end up with a pretty good model $F_M(x)$ that looks like . $$ F_M(x) = F_0(x) + sum_{m = 1}^{M} h_m(x) $$Here&#39;s how our model would evolve up to $M=6$. . Voila! That, friends, is boosting! . Learning Rate . Let&#39;s talk about overfitting. In real life, if we just add our new weak learner $h_m(x)$ directly to our existing composite model $F_{m-1}(x)$, then we&#39;re likely to end up overfitting on our training data. That&#39;s because if we add enough of these weak learners, they&#39;re going to chase down y so closely that all the remaining residuals are pretty much zero, and we will have successfully memorized the training data. To prevent that, we&#39;ll scale them down a bit by a parameter $ eta$ called the learning rate. . With the learning rate $ eta$, the update step will then look like . $$F_{m}(x) = F_{m-1}(x) + eta h_m(x),$$ . and our composite model will look like . $$F_M(x) = F_0(x) + eta sum_{m = 1}^{M} h_m(x)$$ . Note that since the learning rate can be factored out of the sum, it looks kinda like we could just build our models without it and slap it on at the end when we sum up the weak learners to make the final composite model. But that won&#39;t work, since at each stage we train the next weak learner on the residuals from the current composite model, and the current composite model depends on the learning rate. . Implementation . Ok, we&#39;re ready to implement this thing from &quot;scratch&quot;. Well, sort of. To quote Carl Sagan, . If you wish to make an apple pie from scratch, you must first invent the universe. . We will not be inventing a universe that contains the Earth, apple trees, computers, python, numpy, and sklearn. To keep the &quot;scratch&quot; implementation clean, we&#39;ll allow ourselves the luxury of numpy and an off-the-shelf sklearn decision tree which we&#39;ll use as our weak learner. . from sklearn.tree import DecisionTreeRegressor # model hyperparameters learning_rate = 0.3 n_trees = 10 max_depth = 1 # Training F0 = y.mean() Fm = F0 trees = [] for _ in range(n_trees): tree = DecisionTreeRegressor(max_depth=max_depth) tree.fit(x, y - Fm) Fm += learning_rate * tree.predict(x) trees.append(tree) # Prediction y_hat = F0 + learning_rate * np.sum([t.predict(x) for t in trees], axis=0) . We first define our hyperparameters: . learning_rate is ($ eta$) | n_trees is the number of weak learner trees to add ($M$) | max_depth controls the depth of the trees; here we set to 1 for stumps | . We define our base model predictions F0 to simply predict the mean value of y. Fm corresponds to the current composite model $F_m(x)$ as we iteratively add weak learners, so we&#39;ll initialize it with F0. trees is an empty list that we&#39;ll use to hold our weak learners. . Next we iteratively add n_trees weak learners to our composite model. At each iteration, we create a new decision tree and train it on x to predict the current residuals y - Fm. We update Fm with the newly trained learner&#39;s predictions scaled by the learning rate, and we append the new weak learner $h_m(x)$ in the trees list. We generate final predictions y_hat on the training data by summing up the predictions from each weak learner, scaling by the learning rate, and adding to the base model (a.k.a. the mean of y). . Nice! Our GBM fits that nonlinear data pretty well. . Now that we have a working implementation, let&#39;s go ahead and implement it as a class with fit and predict methods like we&#39;re useed to having in sklearn. . class GradientBoostingFromScratch(): def __init__(self, n_trees, learning_rate, max_depth=1): self.n_trees=n_trees; self.learning_rate=learning_rate; self.max_depth=max_depth; def fit(self, x, y): self.trees = [] self.F0 = y.mean() Fm = self.F0 for _ in range(self.n_trees): tree = DecisionTreeRegressor(max_depth=self.max_depth) tree.fit(x, y - Fm) Fm += self.learning_rate * tree.predict(x) self.trees.append(tree) def predict(self, x): return self.F0 + self.learning_rate * np.sum([tree.predict(x) for tree in self.trees], axis=0) . Let&#39;s compare the performance of our implementation with the sklearn GradientBoostingRegressor. . from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import mean_squared_error sklearn_gbm = GradientBoostingRegressor(n_estimators =25, learning_rate=0.3, max_depth=1) sklearn_gbm.fit(x,y) scratch_gbm = GradientBoostingFromScratch(n_trees=25, learning_rate=0.3, max_depth=1) scratch_gbm.fit(x,y) mean_squared_error(y, sklearn_gbm.predict(x)), mean_squared_error(y, scratch_gbm.predict(x)) . (0.10740933643265559, 0.10740933643265561) . Heck yeah! Our homemade GBM is consistent with the sklearn implementation! . Wrapping Up . Alright, there you have it, the intuition behind basic gradient boosting and a from scratch implementation of the gradient boosting machine. I tried to keep this explanation as simple as possible while giving a complete intuition for the basic GBM. But it turns out that the rabbit hole goes pretty deep on these gradient boosting algorithms. We can actually wave our magic generalization wand over some custom loss functions and end up with algorithms that can do gradient descent in function space (whatever that means). We&#39;ll get into what that means and why it&#39;s so baller in future posts. For now, go forth and boost! .",
            "url": "https://blog.mattbowers.dev/gradient-boosting-machine-from-scratch",
            "relUrl": "/gradient-boosting-machine-from-scratch",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "The 80/20 Pandas Tutorial",
            "content": ". Ahh, pandas. In addition to being everyone&#39;s favorite mostly vegetarian bear from south central China, it&#39;s also the python library for working with tabular data, a.k.a. dataframes. When you dive into pandas, you&#39;ll quickly find out that there is a lot going on; indeed there are hundreds of methods for operating on dataframes. But luckily for us, as with many areas of life, there is a Pareto Principle, or 80/20 rule, that will help us focus on the small set of methods that collectively solve the majority of our data transformation needs. . If you&#39;re like me, then pandas is not your first data-handling tool; maybe you&#39;ve been using SQL or R with data.table or dplyr. If so, that&#39;s great because you already have a sense for the key operations we need when working with tabular data. In their book, R for Data Science, Garrett Grolemund and Hadley Wickham describe five essential operations for manipulating dataframes. I&#39;ve found that these cover the majority of my data transformation tasks to prepare data for analysis, visualization, and modeling. . filtering rows based on data values | sorting rows based on data values | selecting columns by name | adding new columns based on the existing columns | creating grouped summaries of the dataset | I would add that we also need a way to build up more complex transformations by chaining these fundamental operations together sequentially. . Before we dive in, here&#39;s the TLDR on the pandas methods that I prefer for accomplishing these tasks, along with their equivalents from SQL and dplyr in R. . description pandas SQL dplyr . filter rows based on data values | query() | WHERE | filter() | . sort rows based on data values | sort_values() | ORDER BY | arrange() | . select columns by name | filter() | SELECT | select() | . add new columns based on the existing columns | assign() | AS | mutate() | . create grouped summaries of the dataset | groupby() apply() | GROUP BY | group_by() summarise() | . chain operations together | . | | %&gt;% | . Imports and Data . import pandas as pd import numpy as np . We&#39;ll use the nycflights13 dataset which contains data on the 336,776 flights that departed from New York City in 2013. . flights = pd.read_csv(&#39;https://www.openintro.org/book/statdata/nycflights.csv&#39;) flights.head() . year month day dep_time dep_delay arr_time arr_delay carrier tailnum flight origin dest air_time distance hour minute . 0 2013 | 6 | 30 | 940 | 15 | 1216 | -4 | VX | N626VA | 407 | JFK | LAX | 313 | 2475 | 9 | 40 | . 1 2013 | 5 | 7 | 1657 | -3 | 2104 | 10 | DL | N3760C | 329 | JFK | SJU | 216 | 1598 | 16 | 57 | . 2 2013 | 12 | 8 | 859 | -1 | 1238 | 11 | DL | N712TW | 422 | JFK | LAX | 376 | 2475 | 8 | 59 | . 3 2013 | 5 | 14 | 1841 | -4 | 2122 | -34 | DL | N914DL | 2391 | JFK | TPA | 135 | 1005 | 18 | 41 | . 4 2013 | 7 | 21 | 1102 | -3 | 1230 | -8 | 9E | N823AY | 3652 | LGA | ORF | 50 | 296 | 11 | 2 | . Select rows based on their values with query() . query() lets you retain a subset of rows based on the values of the data; it&#39;s like dplyr::filter() in R or WHERE in SQL. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using and and or, and you can even check if the column value matches any items in a list. . # compare one column to a value flights.query(&#39;month == 6&#39;) # compare two column values flights.query(&#39;arr_delay &gt; dep_delay&#39;) # using arithmetic flights.query(&#39;arr_delay &gt; 0.5 * air_time&#39;) # using &quot;and&quot; flights.query(&#39;month == 6 and day == 1&#39;) # using &quot;or&quot; flights.query(&#39;origin == &quot;JFK&quot; or dest == &quot;JFK&quot;&#39;) # column value matching any item in a list flights.query(&#39;carrier in [&quot;AA&quot;, &quot;UA&quot;]&#39;) . You may have noticed that it seems to be much more popular to filter pandas data frames using boolean indexing. Indeed when I ask my favorite search engine how to filter a pandas dataframe on its values, I find this tutorial, this blog post, various questions on Stack Overflow, and even the pandas documentation, all espousing boolean indexing. Here&#39;s what it looks like. . # canonical boolean indexing flights[(flights[&#39;carrier&#39;] == &quot;AA&quot;) &amp; (flights[&#39;origin&#39;] == &quot;JFK&quot;)] # the equivalent use of query() flights.query(&#39;carrier == &quot;AA&quot; and origin == &quot;JFK&quot;&#39;) . There are a few reasons I prefer query() over boolean indexing. . query() does not require me to type the dataframe name again, whereas boolean indexing requires me to type it every time I wish to refer to a column. | query() makes the code easier to read and understand, especially when expressions get complex. | query() is more computationally efficient than boolean indexing. | query() can safely be used in dot chains, which we&#39;ll see very soon. | Select columns by name with filter() . filter() lets you pick out a specific set of columns by name; it&#39;s analogous to dplyr::select() in R or SELECT in SQL. You can either provide exactly the column names you want, or you can grab all columns whose names contain a given substring or which match a given regular expression. This isn&#39;t a big deal when your dataframe has only a few columns, but is particularly useful when you have a dataframe with tens or hundreds of columns. . # select a list of columns flights.filter([&#39;origin&#39;, &#39;dest&#39;]) # select columns containing a particular substring flights.filter(like=&#39;time&#39;) # select columns matching a regular expression flights.filter(regex=&#39;e$&#39;) . Sort rows with sort_values() . sort_values() changes the order of the rows based on the data values; it&#39;s likedplyr::arrange() in R or ORDER BY in SQL. You can specify one or more columns on which to sort, where their order denotes the sorting priority. You can also specify whether to sort in ascending or descending order. . # sort by a single column flights.sort_values(&#39;air_time&#39;) # sort by a single column in descending order flights.sort_values(&#39;air_time&#39;, ascending=False) # sort by carrier, then within carrier, sort by descending distance flights.sort_values([&#39;carrier&#39;, &#39;distance&#39;], ascending=[True, False]) . Add new columns with assign() . assign() adds new columns which can be functions of the existing columns; it&#39;s like dplyr::mutate() from R. . # add a new column based on other columns flights.assign(speed = lambda x: x.distance / x.air_time) # another new column based on existing columns flights.assign(gain = lambda x: x.dep_delay - x.arr_delay) . If you&#39;re like me, this way of using assign() might seem a little strange at first. Let&#39;s break it down. In the call to assign() the keyword argument speed tells pandas the name of our new column. The business to the right of the = is a inline lambda function that takes the dataframe we passed to assign() and returns the column we want to add. . I like using x as the lambda argument because its easy to type and it evokes tabular data (think design matrix), which reminds me that it refers to the entire dataframe. We can then access the other columns in our dataframe using the dot like x.other_column. . It&#39;s true that you can skip the whole lambda business and refer to the dataframe to which you are assigning directly inside the assign. That might look like this. . flights.assign(speed = flights.distance / flights.air_time) . I prefer using a lambda for the following reasons. . If you gave your dataframe a good name, using the lambda will save you from typing the name every time you want to refer to a column. | The lambda makes your code more portable. Since you refer to the dataframe as a generic x, you can reuse this same assignment code on a dataframe with a different name. | Most importantly, the lambda will allow you to harness the power of dot chaining. | Chain transformations together with the dot chain . One of the awesome things about pandas is that the object.method() paradigm lets us easily build up complex dataframe transformations from a sequence of method calls. In R, this is effectively accomplished by the pipe %&gt;% operator. For example, suppose we want to look at high-speed flights from JFK to Honolulu, which would require us to query for JFK to Honolulu flights, assign a speed column, and maybe sort on that new speed column. . We can say: . # neatly chain method calls together ( flights .query(&#39;origin == &quot;JFK&quot;&#39;) .query(&#39;dest == &quot;HNL&quot;&#39;) .assign(speed = lambda x: x.distance / x.air_time) .sort_values(by=&#39;speed&#39;, ascending=False) .query(&#39;speed &gt; 8.0&#39;) ) . We compose the dot chain by wrapping the entire expression in parentheses and indenting each line within. The first line is the name of the dataframe on which we are operating. Each subsequent line has a single method call. . There are a few great things about writing the code this way: . Readability. It&#39;s easy to scan down the left margin of the code to see what&#39;s happening. The first line gives us our noun (the dataframe) and each subsequent line starts with a verb. You could read this as &quot;take flights then query the rows where origin is JFK, then query for rows where destination is HNL, then assign a new column called speed, then sort the dataframe by speed, then query only for the rows where speed is greater than 8.0. | Flexibility - It&#39;s easy to comment out individual lines and re-run the cell. It&#39;s also easy to reorder operations, since only one thing happens on each line. | Neatness - We have not polluted our workspace with any intermediate variables, nor have we wasted any mental energy thinking of names for any temporary variables. | By default, dot chains do not modify the original dataframe; they just output a temporary result that we can inspect directly in the output. If you want to store the result, or pass it along to another function (e.g. for plotting), you can simply assign the entire dot chain to a variable. . # sotre the output of the dot chain in a new dataframe flights_high_speed = ( flights .assign(speed = lambda x: x.distance / x.air_time) .query(&#39;speed &gt; 8.0&#39;) ) . Collapsing rows into grouped summaries with groupby() . groupby() combined with apply() gives us flexibility and control over our grouped summaries; it&#39;s like dplyr::group_by() and dplyr::summarise() in R. This is the primary pattern I use for SQL-style groupby operations in pandas. Specifically it unlocks the following essential functionality you&#39;re used to having in SQL. . specify the names of the aggregation columns we create | specify which aggregation function to use on which columns | compose more complex aggregations such as the proportion of rows meeting some condition | aggregate over arbitrary functions of multiple columns | Let&#39;s check out the departure delay stats for each carrier. . ( flights .groupby([&#39;carrier&#39;]) .apply(lambda d: pd.Series({ &#39;n_flights&#39;: len(d), &#39;med_delay&#39;: d.dep_delay.median(), &#39;avg_delay&#39;: d.dep_delay.mean(), })) .head() ) . n_flights med_delay avg_delay . carrier . 9E 1696.0 | -1.0 | 17.285967 | . AA 3188.0 | -2.0 | 9.142409 | . AS 66.0 | -4.5 | 5.181818 | . B6 5376.0 | -1.0 | 13.137091 | . DL 4751.0 | -2.0 | 8.529573 | . While you might be used to apply() acting over the rows or columns of a dataframe, here we&#39;re calling apply on a grouped dataframe object, so it&#39;s acting over the groups. According to the pandas documentation: . The function passed to apply must take a dataframe as its first argument and return a dataframe, a series or a scalar. apply will then take care of combining the results back together into a single dataframe or series. apply is therefore a highly flexible grouping method. . We need to supply apply() with a function that takes each chunk of the grouped dataframe and returns (in our case) a series object with one element for each new aggregation column. Notice that I use a lambda to specify the function we pass to apply()), and that I name its argument d, which reminds me that it&#39;s a dataframe. My lambda returns a pandas series whose index entries specify the new aggregation column names, and whose values constitute the results of the aggregations for each group. Pandas will then stitch everything back together into a lovely dataframe. . Notice how nice the code looks when we use this pattern. Each aggregation is specified on its own line, which makes it easy to see what aggregation columns we&#39;re creating and allows us to comment, uncomment, and reorder the aggregations without breaking anything. . Here are some more complex aggregations to illustrate some useful patterns. . ( flights .groupby([&#39;carrier&#39;]) .apply(lambda d: pd.Series({ &#39;avg_gain&#39;: np.mean(d.dep_delay - d.arr_delay), &#39;pct_delay_gt_30&#39;: np.mean(d.dep_delay &gt; 30), &#39;pct_late_dep_early_arr&#39;: np.mean((d.dep_delay &gt; 0) &amp; (d.arr_delay &lt; 0)), &#39;avg_arr_given_dep_delay_gt_0&#39;: d.query(&#39;dep_delay &gt; 0&#39;).arr_delay.mean(), &#39;cor_arr_delay_dep_delay&#39;: np.corrcoef(d.dep_delay, d.arr_delay)[0,1], })) .head() ) . avg_gain pct_delay_gt_30 pct_late_dep_early_arr avg_arr_given_dep_delay_gt_0 cor_arr_delay_dep_delay . carrier . 9E 9.247642 | 0.196934 | 0.110259 | 39.086111 | 0.932485 | . AA 7.743726 | 0.113237 | 0.105395 | 30.087165 | 0.891013 | . AS 16.515152 | 0.106061 | 0.121212 | 28.058824 | 0.864565 | . B6 3.411458 | 0.160528 | 0.084449 | 37.306866 | 0.914180 | . DL 7.622816 | 0.097874 | 0.100821 | 30.078029 | 0.899327 | . Here&#39;s what&#39;s happening. . np.mean(d.dep_delay - d.arr_delay) aggregates over the difference of two columns. | np.mean(d.dep_delay &gt; 30) computes the proportion of rows where the delay is greater than 30 minutes. Generating a boolean series based on some condition and then using mean() to find the proportion comes up all the time. | np.mean((d.dep_delay &gt; 0) &amp; (d.arr_delay &lt; 0)) shows that we can compute proportions where conditions on multiple columns are met. | d.query(&#39;dep_delay &gt; 0&#39;).arr_delay.mean() computes the average arrival delay on flights where the departure was delayed. Here we first filter each grouped dataframe down to the subset of rows where departure delay is greater than zero using query(), and then we take the mean of the remaining arrival delays. | np.corrcoef(d.dep_delay, d.arr_delay)[0,1] computes the correlation coefficient between departure and arrival delays. Remember we can use pretty much any reduction operation to collapse values down to a scalar. | . You might have noticed that the canonical pandas approach for grouped summaries is to use agg(). That works well if you need to apply the same aggregation function on each column in the dataframe, e.g. taking the mean of every column. But because of the kind of data I work with these days, it&#39;s much more common for me to use customized aggregations like those above, so the groupby() apply() idiom works best for me. . Wrapping Up . There you have it, how to pull off the five most essential data transformation tasks using pandas in a style reminiscent of my beloved dplyr. Remember that part of the beauty of pandas is that since there are so many ways to do most tasks, you can develop your own style based on the kind of data you work with, what you like about other tools, how you see others using the tools, and of course your own taste and preferences. . If you found this post helpful or if you have your own preferred style for accomplishing any of these key transformations with pandas, do let me know about it in the comments. .",
            "url": "https://blog.mattbowers.dev/8020-pandas-tutorial",
            "relUrl": "/8020-pandas-tutorial",
            "date": " • Nov 25, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Hello World! And Why I'm Inspired to Start a Blog",
            "content": ". Well, I’ve been thinking about getting this blog started for months now. I guess a combination of inertia, up-front investment in blogging platform selection/setup, and spending a little too much time writing and rewriting the first content post has drawn out the period from initial inspiration to making the blog a reality. Needless to say, I’m pretty excited to finally get things going. . Before we dive headlong into the weeds of ML algorithms, statistical methods, and whatever I happen to be learning and teaching at the moment, I figured it would be good to articulate why I’ve felt inspired to get started blogging in the first place. Hopefully this will serve the dual purpose of clarifying my intentions and introducing a vastly underappreciated concept in data science that I hope to weave through the posts to come. . Learning . The initial inception about blogging probably originated from some comments about learning that Jeremy Howard makes in the Practical Deep Learning course from fastai. During one of the lectures, he mentions that it’s a great idea to start blogging. To paraphrase Jeremy: . The thing I really love about blogging is that it helps you learn; by writing things down, you synthesize your ideas. . Beautiful. That definitely rings true for me. I tend to take notes and play around with code when learning new concepts anyway. One of my key hypotheses about this blogging experiment is that making the effort to transform those notes into blog posts will help me learn more effectively. . Teaching . Ah, teaching. Yes, sometimes it’s that thing that takes time away from your research, forcing you to sit alone in a windowless room squinting at hand-written math on a fat stack of homework assignments. But sometimes it actually involves interacting with students, endeavoring to explain a concept, and watching them light up when they get it. The latter manifestation of teaching was one of my favorite things about grad school and academia in general. While I certainly still get to do some teaching as an industry data scientist, I could see myself returning to a more teaching-centric gig somewhere off in the future. Thus we have our second key hypothesis about the blogging experiment, that the writing will entertain my inclination to teach. . Contributing . Working in the field of data science today is a bit like standing in front of a massive complimentary all-you-can-learn buffet. There is an abundance of free material out on the interwebs for learning pretty much anything in data science from hello world python tutorials to research papers on cutting-edge deep learning techniques. I’ve personally benefited from many a blog post that helped me unpack a new concept or get started using a new tool. And let’s not forget the gigantic cyber warehouse full of freely available open source software tools that volunteer developers have straight-up donated to humanity. . I realize that up to now, I’ve simply been consuming all of this free goodness without giving anything substantive back in return. Well then, it’s time to start evening the score. Which brings us to key hypothesis number three, that through these blog posts, I might be able to create something helpful, thereby being of service to a community that has freely given so much to me. . Live Long and Prosper, Blog . Phew, there it is, the original source of inspiration for this blogging experiment, and three reasons I think it might be a good idea. The astute reader will have noticed that these three assertions have been formulated as hypotheses which are to be tested in the laboratory of experience. And thus, we also have our first glimpse of the scientific method, an underrated concept that is going to help us put the science back in data science. . With that, blog, I christen thee, Random Realizations. .",
            "url": "https://blog.mattbowers.dev/hello-world",
            "relUrl": "/hello-world",
            "date": " • Nov 22, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Well howdy! I’m Matt Bowers. I’m a data scientist at Uber where I specialize in using statistical modeling and machine learning to make large-scale telematics data useful. Before Uber I earned an MS in Applied Statistics and a PhD in Atmospheric Sciences at Purdue University where I studied the spatial-temporal correlation structure of tropical convection systems and developed statistical methods for climate research. I was also an Insight Data Science fellow at Silicon Valley in the summer of 2017. .",
          "url": "https://blog.mattbowers.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.mattbowers.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}