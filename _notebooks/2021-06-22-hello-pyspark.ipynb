{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Hello PySpark!\"\n",
    "\n",
    "> Get up and running fast with a local pyspark installation, and learn the essentials of working with dataframes at scale.\n",
    "\n",
    "- hide: false\n",
    "- toc: flase\n",
    "- comments: true\n",
    "- categories: [PySpark]\n",
    "- image: images/guiones_wave.jpeg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](nb_images/guiones_wave.jpeg \"A big day at Playa Guiones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you guessed it: it's time for us to learn PySpark!\n",
    "\n",
    "I know, I know, I can hear you screaming into your pillow. Indeed we just spent all that time converting from R and learning python and why the hell do we need yet another API for working with dataframes?\n",
    "\n",
    "That's a totally fair question.\n",
    "\n",
    "So what happens when we're working on something in the real world, where datasets get large in a hurry, and we suddenly have a dataframe that no longer fits into memory?\n",
    "We need a way for our computations and datasets to scale across multiple nodes in a distributed system without having to get too fussy about all the distributed compute details.\n",
    "\n",
    "Enter PySpark.\n",
    "\n",
    "I think it's fair to think of PySpark as a python package for working with arbitrarily large dataframes, i.e., it's like pandas but scalable.\n",
    "It's built on top of [Apache Spark](https://spark.apache.org/), a unified analytics engine for large-scale data processing. \n",
    "[PySpark](https://spark.apache.org/docs/latest/api/python/)  is essentially a way to access the functionality of spark via python code. \n",
    "While there are other high-level interfaces to Spark (such as Java, Scala, and R), for data scientists who are already working extensively with python, PySpark will be the natural interface of choice.\n",
    "PySpark also has great integration with [SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html), and it has a companion machine learning library called [MLlib](https://spark.apache.org/mllib/) that's more or less a scalable scikit-learn (maybe we can cover it in a future post).\n",
    "\n",
    "So, here's the plan. \n",
    "First we're going to get set up to run PySpark locally in a jupyter notebook on our laptop.\n",
    "This is my preferred environment for interactively playing with PySpark and learning the ropes.\n",
    "Then we're going to get up and running in PySpark as quickly as possible by reviewing the most essential functionality for working with dataframes and comparing it to how we would do things in pandas.\n",
    "Once we're comfortable running PySpark on the laptop, it's going to be much easier to jump onto a distributed cluster and run PySpark at scale.\n",
    "\n",
    "Let's do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Run PySpark in a Jupyter Notebook on Your Laptop\n",
    "\n",
    "Ok, I'm going to walk us through how to get things installed on a Mac or Linux machine where we're using homebrew and conda to manage virtual environments.\n",
    "If you have a different setup, your favorite search engine will help you get PySpark set up locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Spark\n",
    "\n",
    "Most of the Spark sourcecode is written in Scala, so first we install Scala. \n",
    "\n",
    "```\n",
    "$ brew install scala\n",
    "```\n",
    "\n",
    "Install Spark.\n",
    "\n",
    "```\n",
    "$ brew install apache-spark\n",
    "```\n",
    "\n",
    "Check where Spark is installed.\n",
    "```\n",
    "$ brew info apache-spark\n",
    "apache-spark: stable 3.1.1, HEAD\n",
    "Engine for large-scale data processing\n",
    "https://spark.apache.org/\n",
    "/usr/local/Cellar/apache-spark/3.1.2 (1,361 files, 242.6MB) *\n",
    "...\n",
    "```\n",
    "\n",
    "Set the Spark home environment variable to the path returned by `brew info` with `/libexec` appended to the end.\n",
    "Don't forget to add the export to your `.zshrc` file too.\n",
    "\n",
    "```\n",
    "$ export SPARK_HOME=/usr/local/Cellar/apache-spark/3.1.2/libexec\n",
    "```\n",
    "\n",
    "Test the installation by starting the Spark shell.\n",
    "\n",
    "```\n",
    "$ spark-shell\n",
    "...\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n",
    "      /_/\n",
    "         \n",
    "Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 14.0.1)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala> \n",
    "```\n",
    "\n",
    "If you get the `scala>` prompt, then you've successfully installed Spark on your laptop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install PySpark\n",
    "\n",
    "Use conda to install the PySpark python package.\n",
    "As usual, it's advisable to do this in a new virtual environment.\n",
    "\n",
    "\n",
    "```\n",
    "$ conda install pyspark\n",
    "```\n",
    "\n",
    "You should be able to launch an interactive PySpark REPL by saying pyspark.\n",
    "\n",
    "```\n",
    "$ pyspark\n",
    "...\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.8.3 (default, Jul  2 2020 11:26:31)\n",
    "Spark context Web UI available at http://192.168.100.47:4041\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1624127229929).\n",
    "SparkSession available as 'spark'.\n",
    ">>> \n",
    "```\n",
    "\n",
    "This time we get a familiar python `>>>` prompt.\n",
    "This is an interactive shell where we can easily experiment with PySpark.\n",
    "Feel free to run the example code in this post here in the PySpark shell, or, if you prefer a notebook, read on and we'll get set up to run PySpark in a jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spark Session Object\n",
    "\n",
    "You may have noticed that when we launched that PySpark interactive shell, it told us that something called `SparkSession` was available as `'spark'`.\n",
    "So basically, what's happening here is that when we launch the pyspark shell, it instantiates an object called `spark` which is an instance of class `pyspark.sql.session.SparkSession`.\n",
    "The spark session object is going to be our entry point for all kinds of PySpark functionality, i.e., we're going to be saying things like `spark.this()` and `spark.that()` to make stuff happen. \n",
    "\n",
    "The PySpark interactive shell is kind enough to instantiate one of these spark session objects for us automatically.\n",
    "However, when we're using another interface to PySpark (like say a jupyter notebook running a python kernal), we'll have to make a spark session object for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PySpark Session in a Jupyter Notebook\n",
    "\n",
    "There are a few ways to run PySpark in jupyter which you can read about [here](https://www.datacamp.com/community/tutorials/apache-spark-python).\n",
    "\n",
    "For derping around with PySpark on your laptop, I think the best way is to instantiate a spark session from a  jupyter notebook running on a regular python kernel. \n",
    "The method we'll use involves running a standard jupyter notebook session with a python kernal and using the findspark package to initialize the spark session.\n",
    "So, first install the findspark package.\n",
    "\n",
    "```\n",
    "$ conda install findspark\n",
    "```\n",
    "\n",
    "Launch jupyter as usual.\n",
    "\n",
    "``` \n",
    "$ jupyter notebook\n",
    "```\n",
    "\n",
    "\n",
    "Go ahead and fire up a new notebook using a regular python 3 kernal.\n",
    "Once you land inside the notebook, there are a couple things we need to do to get a spark session instantiated.\n",
    "You can think of this as boilerplate code that we need to run in the first cell of a notebook where we're going to use PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName('My Spark App').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we're running findspark's `init()` method to find our Spark installation. If you run into errors here, \n",
    "make sure you got the `SPARK_HOME` environment variable correctly set in the install instructions above.\n",
    "Then we instantiate a spark session as `spark`.\n",
    "Once you run this, you're ready to rock and roll with PySpark in your jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Spark provides a handy web UI that you can use for monitoring and debugging. Once you instantiate the spark session You can open the UI in your web browser at [http://localhost:4040/jobs/](http://localhost:4040/jobs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Concepts\n",
    "\n",
    "PySpark provides two main abstractions for data: the RDD and the dataframe.\n",
    "**RDD**'s are just a distributed list of objects; we won't go into details about them in this post.\n",
    "For us, the key object in PySpark is the **dataframe**. \n",
    "\n",
    "While PySpark dataframes expose much of the functionality you would expect from a library for tabular data manipulation, they behave a little differently from pandas dataframes, both syntactically and under-the-hood.\n",
    "There are a couple of key concepts that will help explain these idiosyncracies.\n",
    "\n",
    "**Immutability** - Pyspark RDD's and dataframes are immutable. This means that if you change an object, e.g. by adding a column to a dataframe, PySpark returns a reference to a new dataframe; it does not modify the existing dataframe. This is kind of nice, because we don't have to worry about that whole [view versus copy](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy) nonsense that happens in pandas.\n",
    "\n",
    "**Lazy Evaluation** - Lazy evaluation means that when we start manipulating a dataframe, PySpark won't actually perform any of the computations until we explicitly ask for the result. This is nice because it potentially allows PySpark to do fancy optimizations before executing a sequence of operations. It's also confusing at first, because PySpark will seem to blaze through complex operations and then take forever to print a few rows of the dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Dataframe Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a PySpark dataframe with `createDataFrame()`\n",
    "\n",
    "The first thing we'll need is a way to make dataframes.\n",
    "[`createDataFrame()`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html) allows us to create PySpark dataframes from python objects like nested lists or pandas dataframes.\n",
    "Notice that `createDataFrame()` is a method of the spark session class, so we'll call it from our spark session `spark`by saying `spark.createDataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create pyspark dataframe from nested  lists\n",
    "my_df = spark.createDataFrame(\n",
    "    data=[\n",
    "        [2022, \"tiger\"],\n",
    "        [2023, \"rabbit\"],\n",
    "        [2024, \"dragon\"]\n",
    "    ],\n",
    "    schema=['year', 'animal']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the seaborn tips dataset into a pandas dataframe and then use it to create a PySpark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load tips dataset into a pandas dataframe\n",
    "pandas_df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv')\n",
    "\n",
    "# create pyspark dataframe from a pandas dataframe\n",
    "pyspark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: In real life when we're running PySpark on a large-scale distributed system, we would not generally want to use python lists or pandas dataframes to load data into PySpark.  Ideally we would want to read data directly from where it is stored on HDFS, e.g. by reading [parquet files](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html), or by querying directly from a hive database using [spark sql](https://spark.apache.org/docs/latest/sql-programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peeking at a dataframe's contents\n",
    "\n",
    "The default print method for the PySpark dataframe will just give you the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to peek at some of the data, we'll need to use the `show()` method, which is analogous to the pandas `head()`.\n",
    "Remember that `show()` will cause PySpark to execute any operations that it's been lazily waiting to evaluate, so sometimes it can take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# show the first few rows of the dataframe\n",
    "pyspark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus encounter our first rude awakening. \n",
    "PySpark's default representation of dataframes in the notebook isn't as pretty as that of pandas. \n",
    "But no one ever said it would be pretty, they just said it would be scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the `printSchema()`  method for a nice vertical representation of the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- total_bill: double (nullable = true)\n",
      " |-- tip: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# show the dataframe schema\n",
    "pyspark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select columns by name\n",
    "\n",
    "You can select specific columns from a dataframe using the [`select()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.select.html) method.\n",
    "You can pass either a list of names, or pass names as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[day: string, time: string, total_bill: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "# select some of the columns\n",
    "pyspark_df.select('total_bill', 'tip')\n",
    "\n",
    "# select columns in a list\n",
    "pyspark_df.select(['day', 'time', 'total_bill'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter rows based on column values\n",
    "\n",
    "Analogous to the `WHERE` clause in SQL, and the `query()` method in pandas, PySpark provides a [`filter()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.filter.html) method which returns only the rows that meet the specified conditions.\n",
    "Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like <, >, <=, >=, == (equal), and ~= (not equal). You can specify compound expressions using `and` and `or`, and you can even do a SQL-like `in` to check if the column value matches any items in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "## compare a column to a value\n",
    "pyspark_df.filter('total_bill > 20')\n",
    "\n",
    "# compare two columns with arithmetic\n",
    "pyspark_df.filter('tip > 0.15 * total_bill')\n",
    "\n",
    "# check equality with a string value\n",
    "pyspark_df.filter('sex == \"Male\"')\n",
    "\n",
    "# check equality with any of several possible values\n",
    "pyspark_df.filter('day in (\"Sat\", \"Sun\")')\n",
    "\n",
    "# use \"and\" \n",
    "pyspark_df.filter('day == \"Fri\" and time == \"Lunch\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're into boolean indexing with the brackets, PySpark does support that too, but I encourage you to use `filter()` instead. \n",
    "Check out my rant about [why you shouldn't use boolean indexing](https://blog.mattbowers.dev/8020-pandas-tutorial#Select--rows-based-on-their-values-with-query()) for the details.\n",
    "The TLDR is that `filter()` requires less typing, makes your code more readable and portable, and it allows you to chain method calls together using dot chains.\n",
    "\n",
    "Here's the boolean indexing equivalent of the last example from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "# using boolean indexing\n",
    "pyspark_df[(pyspark_df.day == 'Fri') & (pyspark_df.time == 'Lunch')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know, it looks horrendous, but not as horrendous as the error message you'll get if you forget the parentheses. :smiley:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new columns to a dataframe\n",
    "\n",
    "You can add new columns which are functions of the existing columns with the [`withColumn()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html) method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint, tip_percent: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# add a new column using col() to reference other columns\n",
    "pyspark_df.withColumn('tip_percent', f.col('tip') / f.col('total_bill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we've imported the [`pyspark.sql.functions`]([pyspark.sql.functions](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)) module. This module contains lots of useful functions that we'll be using all over the place, so it's probably a good idea to go ahead and import it whenever you're using PySpark.\n",
    "BTW, it seems like folks usually import this module as `f` or `F`.\n",
    "In this example we're using the `col()` function, which allows us to refer to columns in our dataframe using string representations of the column names.\n",
    "\n",
    "You could also achieve the same result using the dot to reference the other columns, but this requires us to type the dataframe name over and over again, which makes it harder to reuse this code on different dataframes or in [dot chains](https://blog.mattbowers.dev/8020-pandas-tutorial#Chain-transformations-together-with-the-dot-chain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint, tip_percent: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "# add a new column using the dot to reference other columns (less recommended)\n",
    "pyspark_df.withColumn('tip_percent', pyspark_df.tip / pyspark_df.total_bill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to apply numerical transformations like exponents or logs, use the built-in functions in the `pyspark.sql.functions` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint, bill_squared: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "# log \n",
    "pyspark_df.withColumn('log_bill', f.log(f.col('total_bill')))\n",
    "\n",
    "# exponent\n",
    "pyspark_df.withColumn('bill_squared', f.pow(f.col('total_bill'), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can implement conditional assignment like SQL's `CASE WHEN` construct using the `when()` function and the `otherwise()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint, bill_size: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "# conditional assignment (like CASE WHEN)\n",
    "pyspark_df.withColumn('is_male', f.when(f.col('sex') == 'Male', True).otherwise(False))\n",
    "\n",
    "# using multiple when conditions and values\n",
    "pyspark_df.withColumn('bill_size', \n",
    "    f.when(f.col('total_bill') < 10, 'small')\n",
    "    .when(f.col('total_bill') < 20, 'medium')\n",
    "    .otherwise('large')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that since PySpark dataframes are immutable, calling `withColumns()` on a dataframe returns a new dataframe.\n",
    "If you want to persist the result, you'll need to make an assignment.\n",
    "\n",
    "```\n",
    "pyspark_df = pyspark_df.withColumns(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by and aggregate\n",
    "\n",
    "PySpark provides a `groupBy()` method similar to the pandas `groupby()`.\n",
    "Just like in pandas, we can call methods like `count()` and `mean()` on our grouped dataframe, and we also have a more flexible `agg()` method that allows us to specify column-aggregation mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  time|count|\n",
      "+------+-----+\n",
      "| Lunch|   68|\n",
      "|Dinner|  176|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# group by and count\n",
    "pyspark_df.groupBy('time').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+\n",
      "|  time|max(tip)|   avg(total_bill)|\n",
      "+------+--------+------------------+\n",
      "| Lunch|     6.7|17.168676470588235|\n",
      "|Dinner|    10.0| 20.79715909090909|\n",
      "+------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# group by and specify column-aggregation mappings with agg()\n",
    "pyspark_df.groupBy('time').agg({'total_bill': 'mean', 'tip': 'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get fancier with your aggregations, it might just be easier to express them using hive syntax. Read on to find out how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Hive SQL on dataframes\n",
    "\n",
    "One of the mind-blowing features of PySpark is that it \n",
    "allows you to write hive SQL queries on your dataframes.\n",
    "To take a PySpark dataframe into the SQL world, use the `createOrReplaceTempView()` method.\n",
    "This method takes one string argument which will be the dataframes name in the SQL world.\n",
    "Then you can use `spark.sql()` to run a query. \n",
    "The result is returned as a PySpark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# put pyspark dataframe in SQL world and query it\n",
    "pyspark_df.createOrReplaceTempView('tips')\n",
    "spark.sql('select * from tips').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is awesome for a couple of reasons. First, it allows us to easily express any transformations in hive syntax.\n",
    "If you're like me and you've already been using hive, this will dramatically reduce the PySpark learning curve, because when in doubt, you can always bump a dataframe into the SQL world and simply use hive to do what you need.\n",
    "Second, if you have a hive deployment, PySpark's SQL world also has access to all of your hive tables.\n",
    "This means you can write queries involving both hive tables and your PySpark dataframes.\n",
    "It also means you can run hive commands, like inserting into a table, directly from PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some aggregations that might be a little trickier to do using the PySpark built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run hive query and save result to dataframe\n",
    "tip_stats_by_time = spark.sql(\"\"\"\n",
    "    select\n",
    "        time\n",
    "        , count(*) as n \n",
    "        , avg(tip) as avg_tip\n",
    "        , percentile_approx(tip, 0.5) as med_tip\n",
    "        , avg(case when tip > 3 then 1 else 0 end) as pct_tip_gt_3\n",
    "    from \n",
    "        tips\n",
    "    group by 1\n",
    "\"\"\")\n",
    "\n",
    "tip_stats_by_time.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization with PySpark\n",
    "\n",
    "There aren't any tools for visualization included in PySpark.\n",
    "But that's no problem, because we can just use the `toPandas()` method on a PySpark dataframe to pull data back into pandas.\n",
    "Once we have a pandas dataframe, we can happily build visualizations as usual.\n",
    "Of course, if your PySpark dataframe is huge, you wouldn't want to use `toPandas()` directly, because PySpark will attempt to read the entire contents of its huge dataframe into memory.\n",
    "Instead, it's best to use PySpark to generate aggregations of your data for plotting or to pull only a sample of your full data into pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read aggregated pyspark dataframe into pandas for plotting\n",
    "plot_pdf = tip_stats_by_time.toPandas()\n",
    "plot_pdf.plot.bar(x='time', y=['avg_tip', 'med_tip']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "So that's a wrap on our crash course in working with PySpark.\n",
    "You now have a good idea of what pyspark is and how to get started manipulating dataframes with it. \n",
    "Stay tuned for a future post on PySpark's companion ML library MLlib.\n",
    "In the meantime, may no dataframe be too large for you ever again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
