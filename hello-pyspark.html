<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Hello PySpark! | Random Realizations</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Hello PySpark!" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Get up and running fast with a local pyspark installation, and learn the essentials of working with dataframes at scale." />
<meta property="og:description" content="Get up and running fast with a local pyspark installation, and learn the essentials of working with dataframes at scale." />
<link rel="canonical" href="https://blog.mattbowers.dev/hello-pyspark" />
<meta property="og:url" content="https://blog.mattbowers.dev/hello-pyspark" />
<meta property="og:site_name" content="Random Realizations" />
<meta property="og:image" content="https://blog.mattbowers.dev/images/guiones_wave.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-22T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://blog.mattbowers.dev/hello-pyspark","@type":"BlogPosting","headline":"Hello PySpark!","dateModified":"2021-06-22T00:00:00-05:00","datePublished":"2021-06-22T00:00:00-05:00","image":"https://blog.mattbowers.dev/images/guiones_wave.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.mattbowers.dev/hello-pyspark"},"description":"Get up and running fast with a local pyspark installation, and learn the essentials of working with dataframes at scale.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://blog.mattbowers.dev/feed.xml" title="Random Realizations" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','G-JKG69E687S','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper">
<a class="site-title" rel="author" href="/">Random Realizations</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
<a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a>
</div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Hello PySpark!</h1>
<p class="page-description">Get up and running fast with a local pyspark installation, and learn the essentials of working with dataframes at scale.</p>
<p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-06-22T00:00:00-05:00" itemprop="datePublished">
        Jun 22, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      15 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/mcb00/blog/tree/master/_notebooks/2021-06-22-hello-pyspark.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          
          <div class="px-2">
    <a href="https://colab.research.google.com/github/mcb00/blog/blob/master/_notebooks/2021-06-22-hello-pyspark.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab">
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-06-22-hello-pyspark.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/images/copied_from_nb/nb_images/guiones_wave.jpeg" alt="" title="A big day at Playa Guiones."></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Well, you guessed it: it's time for us to learn PySpark!</p>
<p>I know, I know, I can hear you screaming into your pillow. Indeed we just spent all that time converting from R and learning python and why the hell do we need yet another API for working with dataframes?</p>
<p>That's a totally fair question.</p>
<p>So what happens when we're working on something in the real world, where datasets get large in a hurry, and we suddenly have a dataframe that no longer fits into memory?
We need a way for our computations and datasets to scale across multiple nodes in a distributed system without having to get too fussy about all the distributed compute details.</p>
<p>Enter PySpark.</p>
<p>I think it's fair to think of PySpark as a python package for working with arbitrarily large dataframes, i.e., it's like pandas but scalable.
It's built on top of <a href="https://spark.apache.org/">Apache Spark</a>, a unified analytics engine for large-scale data processing. 
<a href="https://spark.apache.org/docs/latest/api/python/">PySpark</a>  is essentially a way to access the functionality of spark via python code. 
While there are other high-level interfaces to Spark (such as Java, Scala, and R), for data scientists who are already working extensively with python, PySpark will be the natural interface of choice.
PySpark also has great integration with <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">SQL</a>, and it has a companion machine learning library called <a href="https://spark.apache.org/mllib/">MLlib</a> that's more or less a scalable scikit-learn (maybe we can cover it in a future post).</p>
<p>So, here's the plan. 
First we're going to get set up to run PySpark locally in a jupyter notebook on our laptop.
This is my preferred environment for interactively playing with PySpark and learning the ropes.
Then we're going to get up and running in PySpark as quickly as possible by reviewing the most essential functionality for working with dataframes and comparing it to how we would do things in pandas.
Once we're comfortable running PySpark on the laptop, it's going to be much easier to jump onto a distributed cluster and run PySpark at scale.</p>
<p>Let's do this.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-Run-PySpark-in-a-Jupyter-Notebook-on-Your-Laptop">How to Run PySpark in a Jupyter Notebook on Your Laptop<a class="anchor-link" href="#How-to-Run-PySpark-in-a-Jupyter-Notebook-on-Your-Laptop"> </a>
</h2>
<p>Ok, I'm going to walk us through how to get things installed on a Mac or Linux machine where we're using homebrew and conda to manage virtual environments.
If you have a different setup, your favorite search engine will help you get PySpark set up locally.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Install-Spark">Install Spark<a class="anchor-link" href="#Install-Spark"> </a>
</h3>
<p>Most of the Spark sourcecode is written in Scala, so first we install Scala.</p>

<pre><code>$ brew install scala</code></pre>
<p>Install Spark.</p>

<pre><code>$ brew install apache-spark</code></pre>
<p>Check where Spark is installed.</p>

<pre><code>$ brew info apache-spark
apache-spark: stable 3.1.1, HEAD
Engine for large-scale data processing
https://spark.apache.org/
/usr/local/Cellar/apache-spark/3.1.2 (1,361 files, 242.6MB) *
...</code></pre>
<p>Set the Spark home environment variable to the path returned by <code>brew info</code> with <code>/libexec</code> appended to the end.
Don't forget to add the export to your <code>.zshrc</code> file too.</p>

<pre><code>$ export SPARK_HOME=/usr/local/Cellar/apache-spark/3.1.2/libexec</code></pre>
<p>Test the installation by starting the Spark shell.</p>

<pre><code>$ spark-shell
...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.1
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 14.0.1)
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt;</code></pre>
<p>If you get the <code>scala&gt;</code> prompt, then you've successfully installed Spark on your laptop!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Install-PySpark">Install PySpark<a class="anchor-link" href="#Install-PySpark"> </a>
</h3>
<p>Use conda to install the PySpark python package.
As usual, it's advisable to do this in a new virtual environment.</p>

<pre><code>$ conda install pyspark</code></pre>
<p>You should be able to launch an interactive PySpark REPL by saying pyspark.</p>

<pre><code>$ pyspark
...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.1.2
      /_/

Using Python version 3.8.3 (default, Jul  2 2020 11:26:31)
Spark context Web UI available at http://192.168.100.47:4041
Spark context available as 'sc' (master = local[*], app id = local-1624127229929).
SparkSession available as 'spark'.
&gt;&gt;&gt;</code></pre>
<p>This time we get a familiar python <code>&gt;&gt;&gt;</code> prompt.
This is an interactive shell where we can easily experiment with PySpark.
Feel free to run the example code in this post here in the PySpark shell, or, if you prefer a notebook, read on and we'll get set up to run PySpark in a jupyter notebook.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-Spark-Session-Object">The Spark Session Object<a class="anchor-link" href="#The-Spark-Session-Object"> </a>
</h3>
<p>You may have noticed that when we launched that PySpark interactive shell, it told us that something called <code>SparkSession</code> was available as <code>'spark'</code>.
So basically, what's happening here is that when we launch the pyspark shell, it instantiates an object called <code>spark</code> which is an instance of class <code>pyspark.sql.session.SparkSession</code>.
The spark session object is going to be our entry point for all kinds of PySpark functionality, i.e., we're going to be saying things like <code>spark.this()</code> and <code>spark.that()</code> to make stuff happen.</p>
<p>The PySpark interactive shell is kind enough to instantiate one of these spark session objects for us automatically.
However, when we're using another interface to PySpark (like say a jupyter notebook running a python kernal), we'll have to make a spark session object for ourselves.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Create-a-PySpark-Session-in-a-Jupyter-Notebook">Create a PySpark Session in a Jupyter Notebook<a class="anchor-link" href="#Create-a-PySpark-Session-in-a-Jupyter-Notebook"> </a>
</h3>
<p>There are a few ways to run PySpark in jupyter which you can read about <a href="https://www.datacamp.com/community/tutorials/apache-spark-python">here</a>.</p>
<p>For derping around with PySpark on your laptop, I think the best way is to instantiate a spark session from a  jupyter notebook running on a regular python kernel. 
The method we'll use involves running a standard jupyter notebook session with a python kernal and using the findspark package to initialize the spark session.
So, first install the findspark package.</p>

<pre><code>$ conda install findspark</code></pre>
<p>Launch jupyter as usual.</p>

<pre><code>$ jupyter notebook</code></pre>
<p>Go ahead and fire up a new notebook using a regular python 3 kernal.
Once you land inside the notebook, there are a couple things we need to do to get a spark session instantiated.
You can think of this as boilerplate code that we need to run in the first cell of a notebook where we're going to use PySpark.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark</span>
<span class="kn">import</span> <span class="nn">findspark</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s1">'My Spark App'</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we're running findspark's <code>init()</code> method to find our Spark installation. If you run into errors here, 
make sure you got the <code>SPARK_HOME</code> environment variable correctly set in the install instructions above.
Then we instantiate a spark session as <code>spark</code>.
Once you run this, you're ready to rock and roll with PySpark in your jupyter notebook.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>Spark provides a handy web UI that you can use for monitoring and debugging. Once you instantiate the spark session You can open the UI in your web browser at <a href="http://localhost:4040/jobs/">http://localhost:4040/jobs/</a>.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="PySpark-Concepts">PySpark Concepts<a class="anchor-link" href="#PySpark-Concepts"> </a>
</h2>
<p>PySpark provides two main abstractions for data: the RDD and the dataframe.
<strong>RDD</strong>'s are just a distributed list of objects; we won't go into details about them in this post.
For us, the key object in PySpark is the <strong>dataframe</strong>.</p>
<p>While PySpark dataframes expose much of the functionality you would expect from a library for tabular data manipulation, they behave a little differently from pandas dataframes, both syntactically and under-the-hood.
There are a couple of key concepts that will help explain these idiosyncracies.</p>
<p><strong>Immutability</strong> - Pyspark RDD's and dataframes are immutable. This means that if you change an object, e.g. by adding a column to a dataframe, PySpark returns a reference to a new dataframe; it does not modify the existing dataframe. This is kind of nice, because we don't have to worry about that whole <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy">view versus copy</a> nonsense that happens in pandas.</p>
<p><strong>Lazy Evaluation</strong> - Lazy evaluation means that when we start manipulating a dataframe, PySpark won't actually perform any of the computations until we explicitly ask for the result. This is nice because it potentially allows PySpark to do fancy optimizations before executing a sequence of operations. It's also confusing at first, because PySpark will seem to blaze through complex operations and then take forever to print a few rows of the dataframe.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="PySpark-Dataframe-Essentials">PySpark Dataframe Essentials<a class="anchor-link" href="#PySpark-Dataframe-Essentials"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Creating-a-PySpark-dataframe-with-createDataFrame()">Creating a PySpark dataframe with <code>createDataFrame()</code><a class="anchor-link" href="#Creating-a-PySpark-dataframe-with-createDataFrame()"> </a>
</h3>
<p>The first thing we'll need is a way to make dataframes.
<code>createDataFrame()</code> allows us to create PySpark dataframes from python objects like nested lists or pandas dataframes.
Notice that <code>createDataFrame()</code> is a method of the spark session class, so we'll call it from our spark session <code>spark</code>by saying <code>spark.createDataFrame()</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create pyspark dataframe from nested  lists</span>
<span class="n">my_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="p">[</span>
        <span class="p">[</span><span class="mi">2022</span><span class="p">,</span> <span class="s2">"tiger"</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2023</span><span class="p">,</span> <span class="s2">"rabbit"</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2024</span><span class="p">,</span> <span class="s2">"dragon"</span><span class="p">]</span>
    <span class="p">],</span>
    <span class="n">schema</span><span class="o">=</span><span class="p">[</span><span class="s1">'year'</span><span class="p">,</span> <span class="s1">'animal'</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's read the seaborn tips dataset into a pandas dataframe and then use it to create a PySpark dataframe.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># load tips dataset into a pandas dataframe</span>
<span class="n">pandas_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv'</span><span class="p">)</span>

<span class="c1"># create pyspark dataframe from a pandas dataframe</span>
<span class="n">pyspark_df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pandas_df</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>In real life when we’re running PySpark on a large-scale distributed system, we would not generally want to use python lists or pandas dataframes to load data into PySpark.  Ideally we would want to read data directly from where it is stored on HDFS, e.g. by reading <a href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html">parquet files</a>, or by querying directly from a hive database using <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">spark sql</a>.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Peeking-at-a-dataframe's-contents">Peeking at a dataframe's contents<a class="anchor-link" href="#Peeking-at-a-dataframe's-contents"> </a>
</h3>
<p>The default print method for the PySpark dataframe will just give you the schema.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pyspark_df</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we want to peek at some of the data, we'll need to use the <code>show()</code> method, which is analogous to the pandas <code>head()</code>.
Remember that <code>show()</code> will cause PySpark to execute any operations that it's been lazily waiting to evaluate, so sometimes it can take a while to run.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># show the first few rows of the dataframe</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>+----------+----+------+------+---+------+----+
|total_bill| tip|   sex|smoker|day|  time|size|
+----------+----+------+------+---+------+----+
|     16.99|1.01|Female|    No|Sun|Dinner|   2|
|     10.34|1.66|  Male|    No|Sun|Dinner|   3|
|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|
|     23.68|3.31|  Male|    No|Sun|Dinner|   2|
|     24.59|3.61|Female|    No|Sun|Dinner|   4|
+----------+----+------+------+---+------+----+
only showing top 5 rows

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We thus encounter our first rude awakening. 
PySpark's default representation of dataframes in the notebook isn't as pretty as that of pandas. 
But no one ever said it would be pretty, they just said it would be scalable.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can also use the <code>printSchema()</code>  method for a nice vertical representation of the schema.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># show the dataframe schema</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>root
 |-- total_bill: double (nullable = true)
 |-- tip: double (nullable = true)
 |-- sex: string (nullable = true)
 |-- smoker: string (nullable = true)
 |-- day: string (nullable = true)
 |-- time: string (nullable = true)
 |-- size: long (nullable = true)

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Select-columns-by-name">Select columns by name<a class="anchor-link" href="#Select-columns-by-name"> </a>
</h3>
<p>You can select specific columns from a dataframe using the <code>select()</code> method.
You can pass either a list of names, or pass names as arguments.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># select some of the columns</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">'total_bill'</span><span class="p">,</span> <span class="s1">'tip'</span><span class="p">)</span>

<span class="c1"># select columns in a list</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">select</span><span class="p">([</span><span class="s1">'day'</span><span class="p">,</span> <span class="s1">'time'</span><span class="p">,</span> <span class="s1">'total_bill'</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Filter-rows-based-on-column-values">Filter rows based on column values<a class="anchor-link" href="#Filter-rows-based-on-column-values"> </a>
</h3>
<p>Analogous to the <code>WHERE</code> clause in SQL, and the <code>query()</code> method in pandas, PySpark provides a <code>filter()</code> method which returns only the rows that meet the specified conditions.
Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using <code>and</code> and <code>or</code>, and you can even do a SQL-like <code>in</code> to check if the column value matches any items in a list.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">## compare a column to a value</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s1">'total_bill &gt; 20'</span><span class="p">)</span>

<span class="c1"># compare two columns with arithmetic</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s1">'tip &gt; 0.15 * total_bill'</span><span class="p">)</span>

<span class="c1"># check equality with a string value</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s1">'sex == "Male"'</span><span class="p">)</span>

<span class="c1"># check equality with any of several possible values</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s1">'day in ("Sat", "Sun")'</span><span class="p">)</span>

<span class="c1"># use "and" </span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s1">'day == "Fri" and time == "Lunch"'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you're into boolean indexing with the brackets, PySpark does support that too, but I encourage you to use <code>filter()</code> instead. 
Check out my rant about <a href="https://blog.mattbowers.dev/8020-pandas-tutorial#Select--rows-based-on-their-values-with-query(">why you shouldn't use boolean indexing</a>) for the details.
The TLDR is that <code>filter()</code> requires less typing, makes your code more readable and portable, and it allows you to chain method calls together using dot chains.</p>
<p>Here's the boolean indexing equivalent of the last example from above.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># using boolean indexing</span>
<span class="n">pyspark_df</span><span class="p">[(</span><span class="n">pyspark_df</span><span class="o">.</span><span class="n">day</span> <span class="o">==</span> <span class="s1">'Fri'</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">pyspark_df</span><span class="o">.</span><span class="n">time</span> <span class="o">==</span> <span class="s1">'Lunch'</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I know, it looks horrendous, but not as horrendous as the error message you'll get if you forget the parentheses. <img class="emoji" title=":smiley:" alt=":smiley:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f603.png" height="20" width="20"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Add-new-columns-to-a-dataframe">Add new columns to a dataframe<a class="anchor-link" href="#Add-new-columns-to-a-dataframe"> </a>
</h3>
<p>You can add new columns which are functions of the existing columns with the <code>withColumn()</code> method.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="nn">f</span>

<span class="c1"># add a new column using col() to reference other columns</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">'tip_percent'</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">'tip'</span><span class="p">)</span> <span class="o">/</span> <span class="n">f</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">'total_bill'</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that we've imported the <code>pyspark.sql.functions</code>) module. This module contains lots of useful functions that we'll be using all over the place, so it's probably a good idea to go ahead and import it whenever you're using PySpark.
BTW, it seems like folks usually import this module as <code>f</code> or <code>F</code>.
In this example we're using the <code>col()</code> function, which allows us to refer to columns in our dataframe using string representations of the column names.</p>
<p>You could also achieve the same result using the dot to reference the other columns, but this requires us to type the dataframe name over and over again, which makes it harder to reuse this code on different dataframes or in <a href="https://blog.mattbowers.dev/8020-pandas-tutorial#Chain-transformations-together-with-the-dot-chain">dot chains</a>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># add a new column using the dot to reference other columns (less recommended)</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">'tip_percent'</span><span class="p">,</span> <span class="n">pyspark_df</span><span class="o">.</span><span class="n">tip</span> <span class="o">/</span> <span class="n">pyspark_df</span><span class="o">.</span><span class="n">total_bill</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you want to apply numerical transformations like exponents or logs, use the built-in functions in the <code>pyspark.sql.functions</code> module.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># log </span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">'log_bill'</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">'total_bill'</span><span class="p">)))</span>

<span class="c1"># exponent</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">'bill_squared'</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">'total_bill'</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can implement conditional assignment like SQL's <code>CASE WHEN</code> construct using the <code>when()</code> function and the <code>otherwise()</code> method.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># conditional assignment (like CASE WHEN)</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">'is_male'</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">'sex'</span><span class="p">)</span> <span class="o">==</span> <span class="s1">'Male'</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="kc">False</span><span class="p">))</span>

<span class="c1"># using multiple when conditions and values</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">'bill_size'</span><span class="p">,</span> 
    <span class="n">f</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">'total_bill'</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">'small'</span><span class="p">)</span>
    <span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">'total_bill'</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">20</span><span class="p">,</span> <span class="s1">'medium'</span><span class="p">)</span>
    <span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="s1">'large'</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Remember that since PySpark dataframes are immutable, calling <code>withColumns()</code> on a dataframe returns a new dataframe.
If you want to persist the result, you'll need to make an assignment.</p>

<pre><code>pyspark_df = pyspark_df.withColumns(...)</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Group-by-and-aggregate">Group by and aggregate<a class="anchor-link" href="#Group-by-and-aggregate"> </a>
</h3>
<p>PySpark provides a <code>groupBy()</code> method similar to the pandas <code>groupby()</code>.
Just like in pandas, we can call methods like <code>count()</code> and <code>mean()</code> on our grouped dataframe, and we also have a more flexible <code>agg()</code> method that allows us to specify column-aggregation mappings.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># group by and count</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">'time'</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>+------+-----+
|  time|count|
+------+-----+
| Lunch|   68|
|Dinner|  176|
+------+-----+

</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># group by and specify column-aggregation mappings with agg()</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">'time'</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s1">'total_bill'</span><span class="p">:</span> <span class="s1">'mean'</span><span class="p">,</span> <span class="s1">'tip'</span><span class="p">:</span> <span class="s1">'max'</span><span class="p">})</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>+------+--------+------------------+
|  time|max(tip)|   avg(total_bill)|
+------+--------+------------------+
| Lunch|     6.7|17.168676470588235|
|Dinner|    10.0| 20.79715909090909|
+------+--------+------------------+

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you want to get fancier with your aggregations, it might just be easier to express them using hive syntax. Read on to find out how.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Run-Hive-SQL-on-dataframes">Run Hive SQL on dataframes<a class="anchor-link" href="#Run-Hive-SQL-on-dataframes"> </a>
</h3>
<p>One of the mind-blowing features of PySpark is that it 
allows you to write hive SQL queries on your dataframes.
To take a PySpark dataframe into the SQL world, use the <code>createOrReplaceTempView()</code> method.
This method takes one string argument which will be the dataframes name in the SQL world.
Then you can use <code>spark.sql()</code> to run a query. 
The result is returned as a PySpark dataframe.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># put pyspark dataframe in SQL world and query it</span>
<span class="n">pyspark_df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s1">'tips'</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s1">'select * from tips'</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is awesome for a couple of reasons. First, it allows us to easily express any transformations in hive syntax.
If you're like me and you've already been using hive, this will dramatically reduce the PySpark learning curve, because when in doubt, you can always bump a dataframe into the SQL world and simply use hive to do what you need.
Second, if you have a hive deployment, PySpark's SQL world also has access to all of your hive tables.
This means you can write queries involving both hive tables and your PySpark dataframes.
It also means you can run hive commands, like inserting into a table, directly from PySpark.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's do some aggregations that might be a little trickier to do using the PySpark built-in functions.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run hive query and save result to dataframe</span>
<span class="n">tip_stats_by_time</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">"""</span>
<span class="s2">    select</span>
<span class="s2">        time</span>
<span class="s2">        , count(*) as n </span>
<span class="s2">        , avg(tip) as avg_tip</span>
<span class="s2">        , percentile_approx(tip, 0.5) as med_tip</span>
<span class="s2">        , avg(case when tip &gt; 3 then 1 else 0 end) as pct_tip_gt_3</span>
<span class="s2">    from </span>
<span class="s2">        tips</span>
<span class="s2">    group by 1</span>
<span class="s2">"""</span><span class="p">)</span>

<span class="n">tip_stats_by_time</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Visualization-with-PySpark">Visualization with PySpark<a class="anchor-link" href="#Visualization-with-PySpark"> </a>
</h2>
<p>There aren't any tools for visualization included in PySpark.
But that's no problem, because we can just use the <code>toPandas()</code> method on a PySpark dataframe to pull data back into pandas.
Once we have a pandas dataframe, we can happily build visualizations as usual.
Of course, if your PySpark dataframe is huge, you wouldn't want to use <code>toPandas()</code> directly, because PySpark will attempt to read the entire contents of its huge dataframe into memory.
Instead, it's best to use PySpark to generate aggregations of your data for plotting or to pull only a sample of your full data into pandas.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># read aggregated pyspark dataframe into pandas for plotting</span>
<span class="n">plot_pdf</span> <span class="o">=</span> <span class="n">tip_stats_by_time</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
<span class="n">plot_pdf</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">'time'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="s1">'avg_tip'</span><span class="p">,</span> <span class="s1">'med_tip'</span><span class="p">]);</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Wrapping-Up">Wrapping Up<a class="anchor-link" href="#Wrapping-Up"> </a>
</h2>
<p>So that's a wrap on our crash course in working with PySpark.
You now have a good idea of what pyspark is and how to get started manipulating dataframes with it. 
Stay tuned for a future post on PySpark's companion ML library MLlib.
In the meantime, may no dataframe be too large for you ever again.</p>

</div>
</div>
</div>
</div>



  </div>
<!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js" repo="mcb00/blog" issue-term="title" label="blogpost-comment" theme="github-light" crossorigin="anonymous" async>
</script><a class="u-url" href="/hello-pyspark" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog about data science, statistics, machine learning, and the scientific method.</p>
      </div>
    </div>

    <div class="social-links">
<ul class="social-media-list">
<li><a rel="me" href="https://github.com/mcb00" title="mcb00"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li>
<li><a rel="me" href="https://twitter.com/mcbwrs" title="mcbwrs"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
