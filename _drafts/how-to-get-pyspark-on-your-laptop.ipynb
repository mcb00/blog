{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Get PySpark Running in Your Local Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Install Spark\n",
    "\n",
    "Install Scala.\n",
    "\n",
    "```\n",
    "$ brew install scala\n",
    "```\n",
    "\n",
    "Install Spark.\n",
    "\n",
    "```\n",
    "$ brew install apache-spark\n",
    "```\n",
    "\n",
    "Check where spark is installed.\n",
    "```\n",
    "$ brew info apache-spark\n",
    "apache-spark: stable 3.1.1, HEAD\n",
    "Engine for large-scale data processing\n",
    "https://spark.apache.org/\n",
    "/usr/local/Cellar/apache-spark/3.1.2 (1,361 files, 242.6MB) *\n",
    "...\n",
    "```\n",
    "\n",
    "Set the spark home environment variable to the path returned by `brew info` with `/libexec` appended to the end.\n",
    "Don't forget to add the export to your `.zshrc` file too.\n",
    "\n",
    "```\n",
    "$ export SPARK_HOME=/usr/local/Cellar/apache-spark/3.1.2/libexec\n",
    "```\n",
    "\n",
    "Test the installation by starting the spark shell.\n",
    "\n",
    "```\n",
    "$ spark-shell\n",
    "...\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n",
    "      /_/\n",
    "         \n",
    "Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 14.0.1)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala> \n",
    "```\n",
    "\n",
    "If you get the `scala>` prompt, then you've successfully installed spark on your laptop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Install PySpark\n",
    "\n",
    "Use conda to install the pyspark and findspark python packages.\n",
    "\n",
    "\n",
    "```\n",
    "$ conda install pyspark\n",
    "```\n",
    "\n",
    "You should be able to launch an interactive pyspark REPL by saying pyspark.\n",
    "\n",
    "```\n",
    "$ pyspark\n",
    "...\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.8.3 (default, Jul  2 2020 11:26:31)\n",
    "Spark context Web UI available at http://192.168.100.47:4041\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1624127229929).\n",
    "SparkSession available as 'spark'.\n",
    ">>> \n",
    "```\n",
    "\n",
    "This time we get a familiar `>>>` prompt.\n",
    "This is an interactive pyspark shell where we can easily experiment with pyspark.\n",
    "Feel free to run the example code in this post here in the pyspark shell, or, if you prefer a notebook, read on and we'll get set up to run pyspark in a jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Spark Session Object\n",
    "\n",
    "You may have noticed that when we launched that pyspark interactive shell, it told us that something called `SparkSession` was available as `'spark'`.\n",
    "So basically, what's happening here is that when we launch the pyspark shell, it instantiates an object called `spark` which is an instance of class `pyspark.sql.session.SparkSession`.\n",
    "The spark session object is going to be our entry point for all kinds of pyspark functionality, i.e., we're going to be saying things like `spark.this` and `spark.that()` to make stuff happen. \n",
    "\n",
    "The pyspark interactive shell is kind enough to instantiate one of these spark session objects for us automatically.\n",
    "However, when we're using another interface to pyspark (like say a jupyter notebook running a python kernal), we'll have to make a spark session object for ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PySpark in a Jupyter Notebook\n",
    "\n",
    "There are a few ways to run pyspark in jupyter which you can read about [here](https://www.datacamp.com/community/tutorials/apache-spark-python).\n",
    "\n",
    "For derping around with pyspark on your laptop, I think the best way is to instantiate a spark session from a  jupyter notebook running on a regular python kernal. \n",
    "The method we'll use involves running a standard jupyter notebook session with a python kernal and using the findspark package to initialize the spark session.\n",
    "Install the findspark package.\n",
    "\n",
    "```\n",
    "$ conda install findspark\n",
    "```\n",
    "\n",
    "Launch jupyter as usual.\n",
    "\n",
    "``` \n",
    "$ jupyter notebook\n",
    "```\n",
    "\n",
    "After firing up a fresh notebook, there are a couple things we need to do to create a new spark session.\n",
    "First, we import findspark and run its `init` method. This is going to find your spark install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get errors, re-read the spark installation instructions above to make sure you correctly set the `SPARK_HOME` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import pyspark and instantiate a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('My Spark App').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides a handy web UI that you can use for monitoring and debugging.\n",
    "You can open it in your web browser at [http://localhost:4040/jobs/](http://localhost:4040/jobs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This tutorial](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning#install) walks us through a simple ML application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
