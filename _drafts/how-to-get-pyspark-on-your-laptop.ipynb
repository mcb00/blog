{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Get PySpark Running in Your Local Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Install Spark\n",
    "\n",
    "Install Scala.\n",
    "\n",
    "```\n",
    "$ brew install scala\n",
    "```\n",
    "\n",
    "Install Spark.\n",
    "\n",
    "```\n",
    "$ brew install apache-spark\n",
    "```\n",
    "\n",
    "Check where spark is installed.\n",
    "```\n",
    "$ brew info apache-spark\n",
    "apache-spark: stable 3.1.1, HEAD\n",
    "Engine for large-scale data processing\n",
    "https://spark.apache.org/\n",
    "/usr/local/Cellar/apache-spark/3.1.1 (1,361 files, 242.6MB) *\n",
    "...\n",
    "```\n",
    "\n",
    "Set the spark home environment variable to the path returned by `brew info` with `/libexec` appended to the end.\n",
    "Don't forget to add the export to your `.zshrc` file too.\n",
    "\n",
    "```\n",
    "$ export SPARK_HOME=/usr/local/Cellar/apache-spark/3.1.1/libexec\n",
    "```\n",
    "\n",
    "Test the installation by starting the spark shell.\n",
    "\n",
    "```\n",
    "$ spark-shell\n",
    "...\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n",
    "      /_/\n",
    "         \n",
    "Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 14.0.1)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala> \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Install PySpark\n",
    "\n",
    "Use conda to install the pyspark and findspark python packages.\n",
    "\n",
    "```\n",
    "$ conda install pyspark\n",
    "$ conda install findspark\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PySpark in a Jupyter Notebook\n",
    "\n",
    "There are a few ways to run pyspark in jupyter which you can read about [here](https://www.datacamp.com/community/tutorials/apache-spark-python).\n",
    "The method we'll use involves running a standard jupyter notebook session with a python kernal and using the findspark package to initialize the spark session.\n",
    "\n",
    "Launch jupyter as usual.\n",
    "\n",
    "``` \n",
    "$ jupyter notebook\n",
    "```\n",
    "\n",
    "Once you land in the notebook, import findspark and run its `init` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import pyspark and create the spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open the spark UI in your web browser at [http://localhost:4040/jobs/](http://localhost:4040/jobs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This tutorial](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning#install) walks us through a simple ML application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('a', 1), ('b', 2), ('c', 3)])\n",
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
