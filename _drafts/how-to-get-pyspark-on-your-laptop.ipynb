{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Run PySpark in a Jupyter Notebook on Your Laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I'm not going to fuss around and try to sugar coat this for us, so I'll just say it: it's time to learn pyspark.\n",
    "Yes, yes, I know, I can hear you saying that we just spent all that time converting from R and learning python and pandas and why the hell do we need yet another API for working with dataframes?\n",
    "That's a totally fair question.\n",
    "\n",
    "Basically it comes down to scalability. \n",
    "If our pythonic prototypes are going to be of much use in the real world where datasets are humongous, we need a way for our computations and datasets to scale across multiple nodes in a distributed system.\n",
    "\n",
    "Enter spark.\n",
    "\n",
    "[Apache Spark](https://spark.apache.org/) is a unified analytics engine for large-scale data processing. \n",
    "[PySpark](https://spark.apache.org/docs/latest/api/python/)  is essentially a way to access the functionality of spark via python code. \n",
    "While there are other high-level interfaces to spark (such as Java, Scala, and R), for data scientists who are already working extensively with python, pyspark will be the natural interface of choice.\n",
    "\n",
    "So, here's the plan. \n",
    "First we're going to get set up to run pyspark locally in a jupyter notebook on our laptop.\n",
    "This is my preferred environment for interactively playing with pyspark and learning the ropes.\n",
    "Then we're going to get up and running in pyspark as quickly as possible by reviewing the most essential functionality and comparing it to how we would do things in pandas.\n",
    "Once we're comfortable running pyspark on the laptop, it's going to be much easier to jump onto a distributed cluster and run pyspark at scale.\n",
    "\n",
    "Great, let's do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Run PySpark in a Jupyter Notebook on Your Laptop\n",
    "\n",
    "Ok, I'm going to walk us through how to get things installed on a Mac or Linux machine where we're using homebrew and conda to manage virtual environments.\n",
    "If you have a different setup, your favorite search engine will help you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Spark\n",
    "\n",
    "Install Scala.\n",
    "\n",
    "```\n",
    "$ brew install scala\n",
    "```\n",
    "\n",
    "Install Spark.\n",
    "\n",
    "```\n",
    "$ brew install apache-spark\n",
    "```\n",
    "\n",
    "Check where spark is installed.\n",
    "```\n",
    "$ brew info apache-spark\n",
    "apache-spark: stable 3.1.1, HEAD\n",
    "Engine for large-scale data processing\n",
    "https://spark.apache.org/\n",
    "/usr/local/Cellar/apache-spark/3.1.2 (1,361 files, 242.6MB) *\n",
    "...\n",
    "```\n",
    "\n",
    "Set the spark home environment variable to the path returned by `brew info` with `/libexec` appended to the end.\n",
    "Don't forget to add the export to your `.zshrc` file too.\n",
    "\n",
    "```\n",
    "$ export SPARK_HOME=/usr/local/Cellar/apache-spark/3.1.2/libexec\n",
    "```\n",
    "\n",
    "Test the installation by starting the spark shell.\n",
    "\n",
    "```\n",
    "$ spark-shell\n",
    "...\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.1\n",
    "      /_/\n",
    "         \n",
    "Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 14.0.1)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala> \n",
    "```\n",
    "\n",
    "If you get the `scala>` prompt, then you've successfully installed spark on your laptop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install PySpark\n",
    "\n",
    "Use conda to install the pyspark python package.\n",
    "As usual, it's advisable to do this in a new virtual environment.\n",
    "\n",
    "\n",
    "```\n",
    "$ conda install pyspark\n",
    "```\n",
    "\n",
    "You should be able to launch an interactive pyspark REPL by saying pyspark.\n",
    "\n",
    "```\n",
    "$ pyspark\n",
    "...\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.8.3 (default, Jul  2 2020 11:26:31)\n",
    "Spark context Web UI available at http://192.168.100.47:4041\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1624127229929).\n",
    "SparkSession available as 'spark'.\n",
    ">>> \n",
    "```\n",
    "\n",
    "This time we get a familiar python `>>>` prompt.\n",
    "This is an interactive pyspark shell where we can easily experiment with pyspark.\n",
    "Feel free to run the example code in this post here in the pyspark shell, or, if you prefer a notebook, read on and we'll get set up to run pyspark in a jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spark Session Object\n",
    "\n",
    "You may have noticed that when we launched that pyspark interactive shell, it told us that something called `SparkSession` was available as `'spark'`.\n",
    "So basically, what's happening here is that when we launch the pyspark shell, it instantiates an object called `spark` which is an instance of class `pyspark.sql.session.SparkSession`.\n",
    "The spark session object is going to be our entry point for all kinds of pyspark functionality, i.e., we're going to be saying things like `spark.this` and `spark.that()` to make stuff happen. \n",
    "\n",
    "The pyspark interactive shell is kind enough to instantiate one of these spark session objects for us automatically.\n",
    "However, when we're using another interface to pyspark (like say a jupyter notebook running a python kernal), we'll have to make a spark session object for ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PySpark Session in a Jupyter Notebook\n",
    "\n",
    "There are a few ways to run pyspark in jupyter which you can read about [here](https://www.datacamp.com/community/tutorials/apache-spark-python).\n",
    "\n",
    "For derping around with pyspark on your laptop, I think the best way is to instantiate a spark session from a  jupyter notebook running on a regular python kernal. \n",
    "The method we'll use involves running a standard jupyter notebook session with a python kernal and using the findspark package to initialize the spark session.\n",
    "So, first install the findspark package.\n",
    "\n",
    "```\n",
    "$ conda install findspark\n",
    "```\n",
    "\n",
    "Launch jupyter as usual.\n",
    "\n",
    "``` \n",
    "$ jupyter notebook\n",
    "```\n",
    "\n",
    "\n",
    "Go ahead and fire up a new notebook using a regular python 3 kernal.\n",
    "Once you land inside the notebook, there are a couple things we need to do to get a spark session instantiated.\n",
    "You can thing of this as boilerplate code that we need to run in the first cell of a notebook where we're going to use pyspark.\n",
    "\n",
    "First import findspark and run its `init` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get errors, re-read the spark installation instructions above to make sure you correctly set the `SPARK_HOME` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import pyspark and instantiate a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('My Spark App').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you run this, you're ready to rock and roll with pyspark in your jupyter notebook.\n",
    "\n",
    "Note that Spark provides a handy web UI that you can use for monitoring and debugging.\n",
    "Once you instantiate the spark session You can open the UI in your web browser at [http://localhost:4040/jobs/](http://localhost:4040/jobs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This tutorial](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning#install) walks us through a simple ML application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
